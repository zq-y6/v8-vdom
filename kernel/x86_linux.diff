diff --git a/./.gitignore b/./.gitignore
deleted file mode 100644
index 8cdea1b95..000000000
--- a/./.gitignore
+++ /dev/null
@@ -1,3 +0,0 @@
-.vscode/
-.config
-*.diff
\ No newline at end of file
diff --git a/./arch/arm/Kconfig b/../../../../linux/arch/arm/Kconfig
index d70af8bab..4c97cb40e 100644
--- a/./arch/arm/Kconfig
+++ b/../../../../linux/arch/arm/Kconfig
@@ -1279,14 +1279,6 @@ config PAGE_OFFSET
 	default 0xB0000000 if VMSPLIT_3G_OPT
 	default 0xC0000000
 
-config HAS_VPK
-	bool "Virtual Memory Protection Keys"
-	default y
-	depends on MMU
-	depends on CPU_USE_DOMAINS
-	depends on CPU_CP15_MMU
-	select ARCH_HAS_PKEYS
-
 config KASAN_SHADOW_OFFSET
 	hex
 	depends on KASAN
diff --git a/./arch/arm/include/asm/mmu.h b/../../../../linux/arch/arm/include/asm/mmu.h
index 1095a8e0f..1592a4264 100644
--- a/./arch/arm/include/asm/mmu.h
+++ b/../../../../linux/arch/arm/include/asm/mmu.h
@@ -25,14 +25,8 @@ typedef struct {
 #define ASID_BITS	8
 #define ASID_MASK	((~0ULL) << ASID_BITS)
 #define ASID(mm)	((unsigned int)((mm)->context.id.counter & ~ASID_MASK))
-#ifdef CONFIG_HAS_VPK
-#define VKMASID(vkm)	((unsigned int)((vkm)->ctx_id.counter & ~ASID_MASK))
-#endif
 #else
 #define ASID(mm)	(0)
-#ifdef CONFIG_HAS_VPK
-#define VKMASID(vkm)	(0)
-#endif
 #endif
 
 #else
diff --git a/./arch/arm/include/asm/mmu_context.h b/../../../../linux/arch/arm/include/asm/mmu_context.h
index efa286f7a..84e58956f 100644
--- a/./arch/arm/include/asm/mmu_context.h
+++ b/../../../../linux/arch/arm/include/asm/mmu_context.h
@@ -21,21 +21,11 @@
 #include <asm/smp_plat.h>
 #include <asm-generic/mm_hooks.h>
 
-#ifdef CONFIG_HAS_VPK
-DECLARE_PER_CPU(struct vkey_map_struct *, loaded_vkm);
-#ifdef CONFIG_UACCESS_WITH_MEMCPY
-#error "CONFIG_HAS_VPK is not compatible with CONFIG_UACCESS_WITH_MEMCPY"
-#endif
-#endif
-
 void __check_vmalloc_seq(struct mm_struct *mm);
 
 #ifdef CONFIG_CPU_HAS_ASID
 
 void check_and_switch_context(struct mm_struct *mm, struct task_struct *tsk);
-#ifdef CONFIG_HAS_VPK
-void check_and_switch_context_fast(struct mm_struct *mm, struct task_struct *tsk);
-#endif
 
 #define init_new_context init_new_context
 static inline int
@@ -74,14 +64,8 @@ static inline void check_and_switch_context(struct mm_struct *mm,
 		 * finish_arch_post_lock_switch() call.
 		 */
 		mm->context.switch_pending = 1;
-	else {
-#ifdef CONFIG_HAS_VPK
-		if (tsk && tsk->vkm)
-			cpu_switch_vkm(tsk->vkm);
-		else
-#endif
+	else
 		cpu_switch_mm(mm->pgd, mm);
-	}
 }
 
 #ifndef MODULE
@@ -101,11 +85,6 @@ static inline void finish_arch_post_lock_switch(void)
 		preempt_disable();
 		if (mm->context.switch_pending) {
 			mm->context.switch_pending = 0;
-#ifdef CONFIG_HAS_VPK
-		if (current->vkm && current->vkm->pgd != vkm->pgd)
-			cpu_switch_vkm(current->vkm);
-		else
-#endif
 			cpu_switch_mm(mm->pgd, mm);
 		}
 		preempt_enable_no_resched();
@@ -119,27 +98,6 @@ static inline void finish_arch_post_lock_switch(void)
 
 #define activate_mm(prev,next)		switch_mm(prev, next, NULL)
 
-#ifdef CONFIG_HAS_VPK
-static inline void
-switch_mm_fast(struct mm_struct *mm, struct task_struct *tsk)
-{
-#ifdef CONFIG_MMU
-	unsigned int cpu = smp_processor_id();
-	struct vkey_map_struct *new_vkm;
-
-	if (cache_ops_need_broadcast() &&
-	    !cpumask_empty(mm_cpumask(mm)) &&
-	    !cpumask_test_cpu(cpu, mm_cpumask(mm)))
-		__flush_icache_all();
-
-	new_vkm = tsk->vkm;
-	cpumask_set_cpu(cpu, vkm_cpumask(new_vkm));
-
-	check_and_switch_context_fast(mm, tsk);
-#endif
-}
-#endif
-
 /*
  * This is the actual mm switch as far as the scheduler
  * is concerned.  No registers are touched.  We avoid
@@ -152,11 +110,6 @@ switch_mm(struct mm_struct *prev, struct mm_struct *next,
 {
 #ifdef CONFIG_MMU
 	unsigned int cpu = smp_processor_id();
-#ifdef CONFIG_HAS_VPK
-	struct vkey_map_struct *new_vkm;
-	struct vkey_map_struct *prev_vkm;
-	bool is_vkm_new;
-#endif
 
 	/*
 	 * __sync_icache_dcache doesn't broadcast the I-cache invalidation,
@@ -168,36 +121,11 @@ switch_mm(struct mm_struct *prev, struct mm_struct *next,
 	    !cpumask_test_cpu(cpu, mm_cpumask(next)))
 		__flush_icache_all();
 
-#ifdef CONFIG_HAS_VPK
-	prev_vkm = per_cpu(loaded_vkm, cpu);
-	if (tsk)
-		new_vkm = tsk->vkm;
-	else
-		new_vkm = NULL;
-	if (new_vkm)
-		is_vkm_new = !cpumask_test_and_set_cpu(cpu, vkm_cpumask(new_vkm));
-	else
-		is_vkm_new = false;
-	if (!cpumask_test_and_set_cpu(cpu, mm_cpumask(next)) ||	/* set the mm bitmap anyway */
-			is_vkm_new || prev != next || prev_vkm != new_vkm) {
-		check_and_switch_context(next, tsk);
-		if (cache_is_vivt()) {
-			if (prev->main_vkm) {	/* only clear the right vkm */
-				if (likely(prev_vkm))
-					cpumask_clear_cpu(cpu, vkm_cpumask(prev_vkm));
-				else
-					cpumask_clear_cpu(cpu, vkm_cpumask(prev->main_vkm));
-			} else
-				cpumask_clear_cpu(cpu, mm_cpumask(prev));
-		}
-	}
-#else	/* !CONFIG_HAS_VPK */
 	if (!cpumask_test_and_set_cpu(cpu, mm_cpumask(next)) || prev != next) {
 		check_and_switch_context(next, tsk);
 		if (cache_is_vivt())
 			cpumask_clear_cpu(cpu, mm_cpumask(prev));
 	}
-#endif	/* CONFIG_HAS_VPK */
 #endif
 }
 
diff --git a/./arch/arm/include/asm/pgalloc.h b/../../../../linux/arch/arm/include/asm/pgalloc.h
index aa1cb9662..a17f01235 100644
--- a/./arch/arm/include/asm/pgalloc.h
+++ b/../../../../linux/arch/arm/include/asm/pgalloc.h
@@ -17,7 +17,6 @@
 
 #ifdef CONFIG_MMU
 
-#define _PAGE_PROT_TABLE(dom)	(PMD_TYPE_TABLE | PMD_BIT4 | PMD_DOMAIN(dom))
 #define _PAGE_USER_TABLE	(PMD_TYPE_TABLE | PMD_BIT4 | PMD_DOMAIN(DOMAIN_USER))
 #define _PAGE_KERNEL_TABLE	(PMD_TYPE_TABLE | PMD_BIT4 | PMD_DOMAIN(DOMAIN_KERNEL))
 
@@ -145,20 +144,6 @@ pmd_populate(struct mm_struct *mm, pmd_t *pmdp, pgtable_t ptep)
 	__pmd_populate(pmdp, page_to_phys(ptep), prot);
 }
 
-static inline void
-pmd_populate_tag(struct mm_struct *mm, pmd_t *pmdp, pgtable_t ptep, int tag)
-{
-	extern pmdval_t user_pmd_table;
-	pmdval_t prot;
-
-	if (__LINUX_ARM_ARCH__ >= 6 && !IS_ENABLED(CONFIG_ARM_LPAE))
-		prot = ((user_pmd_table & (~PMD_DOMAIN(15))) | PMD_DOMAIN(tag));
-	else
-		prot = _PAGE_PROT_TABLE(tag);
-
-	__pmd_populate(pmdp, page_to_phys(ptep), prot);
-}
-
 #endif /* CONFIG_MMU */
 
 #endif
diff --git a/./arch/arm/include/asm/pkeys.h b/./arch/arm/include/asm/pkeys.h
deleted file mode 100644
index d24f791cd..000000000
--- a/./arch/arm/include/asm/pkeys.h
+++ /dev/null
@@ -1,53 +0,0 @@
-#ifndef _ASM_ARM_PKEYS_H
-#define _ASM_ARM_PKEYS_H
-
-#include <asm/domain.h>
-#define ARCH_DEFAULT_PKEY	DOMAIN_USER
-
-#ifdef CONFIG_HAS_VPK
-#define arch_max_pkey() (13)
-#define execute_only_pkey(mm) (4)   /* 0 kernel, 1 user, 2 io, 3 vector, 4 for xok, 4 - 15 others */
-#define get_execute_only_pkey(mm) (4)
-#define arch_override_mprotect_pkey(vma, prot, pkey) (((pkey) == -1) ? vma_pkey(vma) : (pkey))
-#define ARCH_VM_PKEY_FLAGS 0    /* never touch 32 bits vm_flags */
-
-static inline int vma_pkey(struct vm_area_struct *vma)
-{
-	unsigned long arm_domain_mask = 0xffff0000;
-    unsigned long arm_domain_shift = 16;
-    return (vma->vm_vkey & arm_domain_mask) >> arm_domain_shift;
-}
-
-/* This is never called from pure pkey mprotect, just for VPK */
-static inline bool mm_pkey_is_allocated(struct mm_struct *mm, int pkey)
-{
-	if (pkey <= execute_only_pkey(mm))
-		return false;
-	if (pkey >= arch_max_pkey())
-		return false;
-    return true;
-}
-
-static inline int mm_pkey_alloc(struct mm_struct *mm)
-{
-	return execute_only_pkey(mm);
-}
-
-static inline int mm_pkey_free(struct mm_struct *mm, int pkey)
-{
-	return 0;
-}
-
-static inline int arch_set_user_pkey_access(struct task_struct *tsk, int pkey,
-			unsigned long init_val)
-{
-	return 0;
-}
-
-static inline bool arch_pkeys_enabled(void)
-{
-	return true;
-}
-
-#endif  /* CONFIG_HAS_VPK */
-#endif
diff --git a/./arch/arm/include/asm/tlbflush.h b/../../../../linux/arch/arm/include/asm/tlbflush.h
index 2abc958e9..0ccc985b9 100644
--- a/./arch/arm/include/asm/tlbflush.h
+++ b/../../../../linux/arch/arm/include/asm/tlbflush.h
@@ -384,17 +384,8 @@ static inline void local_flush_tlb_mm(struct mm_struct *mm)
 	if (tlb_flag(TLB_WB))
 		dsb(nshst);
 
-#ifdef CONFIG_HAS_VPK
-	if (!list_empty(&mm->vkm_chain)) {
-		__local_flush_tlb_all();
-		tlb_op(TLB_V7_UIS_FULL, "c8, c7, 0", 0);
-	} else {
-#endif
 	__local_flush_tlb_mm(mm);
 	tlb_op(TLB_V7_UIS_ASID, "c8, c7, 2", asid);
-#ifdef CONFIG_HAS_VPK
-	}
-#endif
 
 	if (tlb_flag(TLB_BARRIER))
 		dsb(nsh);
@@ -407,21 +398,12 @@ static inline void __flush_tlb_mm(struct mm_struct *mm)
 	if (tlb_flag(TLB_WB))
 		dsb(ishst);
 
-#ifdef CONFIG_HAS_VPK
-	if(!list_empty(&mm->vkm_chain)) {
-		__local_flush_tlb_all();
-		tlb_op(TLB_V7_UIS_FULL, "c8, c3, 0", 0);
-	} else {
-#endif
 	__local_flush_tlb_mm(mm);
 #ifdef CONFIG_ARM_ERRATA_720789
 	tlb_op(TLB_V7_UIS_ASID, "c8, c3, 0", 0);
 #else
 	tlb_op(TLB_V7_UIS_ASID, "c8, c3, 2", ASID(mm));
 #endif
-#ifdef CONFIG_HAS_VPK
-	}
-#endif
 
 	if (tlb_flag(TLB_BARRIER))
 		dsb(ish);
@@ -459,17 +441,8 @@ local_flush_tlb_page(struct vm_area_struct *vma, unsigned long uaddr)
 	if (tlb_flag(TLB_WB))
 		dsb(nshst);
 
-#ifdef CONFIG_HAS_VPK
-	if (!list_empty(&vma->vm_mm->vkm_chain)) {
-		__local_flush_tlb_all();
-		tlb_op(TLB_V7_UIS_FULL, "c8, c7, 0", 0);
-	} else {
-#endif
 	__local_flush_tlb_page(vma, uaddr);
 	tlb_op(TLB_V7_UIS_PAGE, "c8, c7, 1", uaddr);
-#ifdef CONFIG_HAS_VPK
-	}
-#endif
 
 	if (tlb_flag(TLB_BARRIER))
 		dsb(nsh);
@@ -485,21 +458,12 @@ __flush_tlb_page(struct vm_area_struct *vma, unsigned long uaddr)
 	if (tlb_flag(TLB_WB))
 		dsb(ishst);
 
-#ifdef CONFIG_HAS_VPK
-	if (!list_empty(&vma->vm_mm->vkm_chain)) {
-		__local_flush_tlb_page(vma, uaddr);
-		tlb_op(TLB_V7_UIS_PAGE, "c8, c3, 3", uaddr & PAGE_MASK);
-	} else {
-#endif
 	__local_flush_tlb_page(vma, uaddr);
 #ifdef CONFIG_ARM_ERRATA_720789
 	tlb_op(TLB_V7_UIS_PAGE, "c8, c3, 3", uaddr & PAGE_MASK);
 #else
 	tlb_op(TLB_V7_UIS_PAGE, "c8, c3, 1", uaddr);
 #endif
-#ifdef CONFIG_HAS_VPK
-	}
-#endif
 
 	if (tlb_flag(TLB_BARRIER))
 		dsb(ish);
@@ -622,22 +586,6 @@ static inline void clean_pmd_entry(void *pmd)
 	tlb_l2_op(TLB_L2CLEAN_FR, "c15, c9, 1  @ L2 flush_pmd", pmd);
 }
 
-#ifdef CONFIG_HAS_VPK
-static inline bool get_tlb_flag(unsigned long f)
-{
-	const unsigned int __tlb_flag = __cpu_tlb_flags;
-	
-	return tlb_flag(f);
-}
-
-static inline void do_tlb_ops(unsigned long addr)
-{
-	const unsigned int __tlb_flag = __cpu_tlb_flags;
-	
-	tlb_op(TLB_V7_UIS_PAGE, "c8, c3, 3", (addr));
-}
-#endif
-
 #undef tlb_op
 #undef tlb_flag
 #undef always_tlb_flags
diff --git a/./arch/arm/include/asm/vkey_types.h b/./arch/arm/include/asm/vkey_types.h
deleted file mode 100644
index a906b798f..000000000
--- a/./arch/arm/include/asm/vkey_types.h
+++ /dev/null
@@ -1,12 +0,0 @@
-#ifndef _ASM_ARM_VKEY_TYPES_H
-#define _ASM_ARM_VKEY_TYPES_H
-
-#ifdef CONFIG_HAS_VPK
-
-/* 16 - DOM_KERN - DOM_IO, 1 is default, so 3 is xok */
-#define arch_max_pkey()		(13)
-typedef atomic64_t vkctx_t;
-
-#endif  /* CONFIG_HAS_VPK */
-
-#endif
\ No newline at end of file
diff --git a/./arch/arm/include/asm/vkeys.h b/./arch/arm/include/asm/vkeys.h
deleted file mode 100644
index a8355fa8d..000000000
--- a/./arch/arm/include/asm/vkeys.h
+++ /dev/null
@@ -1,156 +0,0 @@
-#ifndef _ASM_ARM_VKEYS_H
-#define _ASM_ARM_VKEYS_H
-
-#define VMA_VKEY_MASK	0x0000ffff	/* In 32 bit arch, the upper 16 bits are for pkeys */
-
-#define VKEY_AD 0x0		/* access  disable */
-#define VKEY_WD 0x1		/* write   disable */
-#define VKEY_PINNED 0x2
-#define VKEY_ND 0x3		/* nothing disable */
-#define VKEY_MASK 0x4
-
-#ifdef CONFIG_HAS_VPK
-#include <linux/mm_types.h>
-#include <linux/vkey_types.h>
-#include <linux/pkeys.h>
-#include <linux/cpumask.h>
-#include <asm/domain.h>
-#include <asm/pgalloc.h>
-
-/* arm32 no transparent huge page */
-#define set_pmd_at(mm, addr, dst_pmd, pmd)	do { } while(0);
-#define set_pud_at(mm, addr, dst_pud, pud)	do { } while(0);
-
-extern void flush_tlb_vkm_range(struct vm_area_struct *vma, struct vkey_map_struct *vkm);
-extern void flush_tlb_vkm_page(unsigned long addr, struct vkey_map_struct *vkm);
-
-static inline void mm_vkm_pkru_set_bits(int pkey, u32 perm) 
-{
-	unsigned int domain = get_domain();
-	domain &= ~domain_mask(pkey);
-	domain = domain | domain_val(pkey, perm);
-	set_domain(domain);
-    return;
-}
-
-static inline u32 mm_vkm_pkru_get_bits(int pkey)
-{
-	return ((get_domain() >> (pkey << 1)) & 0x3);
-}
-
-static inline u32 mm_vkm_pkru_get(void)
-{
-	return get_domain();
-}
-
-static inline void mm_vkm_pkru_reset(bool access)
-{
-	if (access)
-		set_domain(0x55555500 | DACR_INIT);
-	else
-		set_domain(DACR_INIT);
-}
-
-static inline int mm_vkm_idx_to_pkey(int i)
-{
-	return i + 4;   /* 1 for user default, just like 0, 4 is xok */
-}
-
-static inline pte_t mm_vkm_mkpte(pte_t org_pte, int pkey)
-{
-    return org_pte;
-}
-
-static inline pmd_t mm_vkm_mkpmd(pmd_t org_pmd, int pkey)
-{
-	return __pmd((pmd_val(org_pmd) & (~PMD_DOMAIN(15))) | PMD_DOMAIN(pkey));
-}
-
-static inline pud_t mm_vkm_mkpud(pud_t org_pud, int pkey)
-{
-	return org_pud;
-}
-
-static inline void arch_vkm_init(struct mm_struct *mm, 
-				struct vkey_map_struct *vkm, bool main)
-{
-	if (main) {
-		vkm->pgd = mm->pgd;
-		atomic64_set(&vkm->ctx_id, atomic64_read(&mm->context.id));
-	} else {
-		vkm->pgd = pgd_alloc(mm);
-		atomic64_set(&vkm->ctx_id, 0);
-	}
-}
-
-static __always_inline void
-arch_cpumask_clear_vkm(unsigned int cpu, struct cpumask *dstp, struct vkey_map_struct *vkm)
-{
-	/* First, make sure to clear all related ASID TLB entries, then clear the bit */
-	/* needs __flush_tlb_mm(vkm's fake mm) */
-	cpumask_clear_cpu(cpu, dstp);
-}
-
-static inline bool vkm_pmd_populate(struct mm_struct *mm, 
-					pmd_t *pmd, pgtable_t pte, int tag)
-{
-	struct vm_area_struct *vma;
-	struct vkey_map_struct *main_vkm;
-	int vkey;
-	unsigned long aligned_addr;
-	unsigned long offset = 20UL;
-	
-	if (tag == -1) {
-		/* Use the pmd offset to find vma, this is because pmd has 2 * u32 per entry */
-		aligned_addr = (((unsigned long)pmd - (unsigned long)(mm->pgd)) / sizeof(u32)) << offset;
-
-		vma = find_vma(mm, aligned_addr);
-		main_vkm = mm->main_vkm;
-		if (vma) {
-			vkey = vma->vm_vkey & VMA_VKEY_MASK;
-			if (vkey) {
-				if (main_vkm) {
-					int i;
-					/* Find the mapping in the main vkm */
-					tag = execute_only_pkey(mm);
-					spin_lock(&main_vkm->slock);
-					for (i = 0; i < arch_max_pkey() - 1; i++) {
-						if (main_vkm->pkey_vkey[i] == vkey) {
-							tag = mm_vkm_idx_to_pkey(i);
-							break;
-						}
-					}
-					spin_unlock(&main_vkm->slock);
-				} else
-					tag = execute_only_pkey(mm);
-			} else
-				tag = DOMAIN_USER;
-		} else
-			tag = DOMAIN_USER;
-	}
-
-	pmd_populate_tag(mm, pmd, pte, tag);
-	return true;
-}
-
-static inline int mm_vkm_pmd_get_pkey(pmd_t pmd)
-{
-	pmdval_t val = pmd_val(pmd);
-	return (val & PMD_DOMAIN_MASK) >> 5;
-}
-
-static inline bool vkm_mod_pmd_fast(struct mm_struct *mm, 
-			pmd_t *pmd, pgtable_t pte, int tag,
-			unsigned long start, unsigned long end)
-{
-	return vkm_pmd_populate(mm, pmd, pte, tag);
-}
-
-static inline bool mm_vkm_is_reserved_pk_fault(unsigned int flags)
-{
-	return false;
-}
-
-#endif
-
-#endif
diff --git a/./arch/arm/kernel/smp_tlb.c b/../../../../linux/arch/arm/kernel/smp_tlb.c
index bd9b4c426..d4908b373 100644
--- a/./arch/arm/kernel/smp_tlb.c
+++ b/../../../../linux/arch/arm/kernel/smp_tlb.c
@@ -21,16 +21,8 @@ struct tlb_args {
 	struct vm_area_struct *ta_vma;
 	unsigned long ta_start;
 	unsigned long ta_end;
-#ifdef CONFIG_HAS_VPK
-	struct vkey_map_struct *ta_vkm;
-#endif
 };
 
-#ifdef CONFIG_HAS_VPK
-static DEFINE_PER_CPU(struct vm_area_struct, swfl_vma);
-static DEFINE_PER_CPU(struct mm_struct, swfl_mm);
-#endif
-
 static inline void ipi_flush_tlb_all(void *ignored)
 {
 	local_flush_tlb_all();
@@ -65,24 +57,6 @@ static inline void ipi_flush_tlb_range(void *arg)
 	struct tlb_args *ta = (struct tlb_args *)arg;
 	unsigned int __ua_flags = uaccess_save_and_enable();
 
-#ifdef CONFIG_HAS_VPK
-	struct mm_struct *mm = ta->ta_vma->vm_mm;
-	struct list_head *pos;
-	struct vkey_map_struct *vkm;
-	unsigned int cpu = smp_processor_id();
-	list_for_each(pos, &mm->vkm_chain) {
-		vkm = list_entry(pos, struct vkey_map_struct, vkm_chain);
-		if (!ta->ta_vkm || vkm == ta->ta_vkm) {
-			per_cpu(swfl_vma, cpu).vm_mm = &per_cpu(swfl_mm, cpu);
-			atomic64_set(&per_cpu(swfl_mm, cpu).context.id, atomic64_read(&vkm->ctx_id));
-			local_flush_tlb_range(&per_cpu(swfl_vma, cpu), ta->ta_start, ta->ta_end);
-			if (vkm && vkm == ta->ta_vkm) {
-				uaccess_restore(__ua_flags);
-				return;
-			}
-		}
-	}
-#endif
 	local_flush_tlb_range(ta->ta_vma, ta->ta_start, ta->ta_end);
 
 	uaccess_restore(__ua_flags);
@@ -212,41 +186,19 @@ void flush_tlb_all(void)
 
 void flush_tlb_mm(struct mm_struct *mm)
 {
-	if (tlb_ops_need_broadcast()) {
-#ifdef CONFIG_HAS_VPK
-		struct list_head *pos;
-		struct vkey_map_struct *vkm;
-		list_for_each(pos, &mm->vkm_chain) {
-			vkm = list_entry(pos, struct vkey_map_struct, vkm_chain);
-			if (vkm == mm->main_vkm)
-				continue;
-			on_each_cpu_mask(vkm_cpumask(vkm), ipi_flush_tlb_mm, mm, 1);
-		}
-#endif
+	if (tlb_ops_need_broadcast())
 		on_each_cpu_mask(mm_cpumask(mm), ipi_flush_tlb_mm, mm, 1);
-	} else
+	else
 		__flush_tlb_mm(mm);
 	broadcast_tlb_mm_a15_erratum(mm);
 }
 
 void flush_tlb_page(struct vm_area_struct *vma, unsigned long uaddr)
 {
-#ifdef CONFIG_HAS_VPK
-	struct list_head *pos;
-	struct vkey_map_struct *vkm;
-#endif
 	if (tlb_ops_need_broadcast()) {
 		struct tlb_args ta;
 		ta.ta_vma = vma;
 		ta.ta_start = uaddr;
-#ifdef CONFIG_HAS_VPK
-		list_for_each(pos, &vma->vm_mm->vkm_chain) {
-			vkm = list_entry(pos, struct vkey_map_struct, vkm_chain);
-			if (vkm == vma->vm_mm->main_vkm)
-				continue;
-			on_each_cpu_mask(vkm_cpumask(vkm), ipi_flush_tlb_page, &ta, 1);
-		}
-#endif
 		on_each_cpu_mask(mm_cpumask(vma->vm_mm), ipi_flush_tlb_page,
 					&ta, 1);
 	} else
@@ -265,75 +217,18 @@ void flush_tlb_kernel_page(unsigned long kaddr)
 	broadcast_tlb_a15_erratum();
 }
 
-#ifdef CONFIG_HAS_VPK
-void flush_tlb_vkm_range(struct vm_area_struct *vma, struct vkey_map_struct *vkm)
-{
-	unsigned int cpu = smp_processor_id();
-
-	if (tlb_ops_need_broadcast()) {
-		struct tlb_args ta;
-		ta.ta_vma = vma;
-		ta.ta_start = vma->vm_start;
-		ta.ta_end = vma->vm_end;
-		ta.ta_vkm = vkm;
-		on_each_cpu_mask(vkm_cpumask(vkm), ipi_flush_tlb_range, &ta, 1);
-	} else {
-		atomic64_set(&per_cpu(swfl_mm, cpu).context.id, atomic64_read(&vkm->ctx_id));
-		if (vma->vm_end - vma->vm_start <= 64 * PAGE_SIZE) {
-			per_cpu(swfl_vma, cpu).vm_mm = &per_cpu(swfl_mm, cpu);
-			local_flush_tlb_range(&per_cpu(swfl_vma, cpu), vma->vm_start, vma->vm_end);
-		} else
-			local_flush_tlb_mm(&per_cpu(swfl_mm, cpu));
-	}
-	broadcast_tlb_mm_a15_erratum(vma->vm_mm);
-}
-
-void flush_tlb_vkm_page(unsigned long addr, struct vkey_map_struct *vkm)
-{
-	return;	/* ARM never use this function */
-}
-#endif
-
 void flush_tlb_range(struct vm_area_struct *vma,
                      unsigned long start, unsigned long end)
 {
-#ifdef CONFIG_HAS_VPK
-	struct mm_struct *mm = vma->vm_mm;
-	struct list_head *pos;
-	struct vkey_map_struct *vkm;
-#endif
 	if (tlb_ops_need_broadcast()) {
 		struct tlb_args ta;
 		ta.ta_vma = vma;
 		ta.ta_start = start;
 		ta.ta_end = end;
-#ifdef CONFIG_HAS_VPK
-		list_for_each(pos, &mm->vkm_chain) {
-			vkm = list_entry(pos, struct vkey_map_struct, vkm_chain);
-			if (vkm == vma->vm_mm->main_vkm)
-				continue;
-			ta.ta_vkm = vkm;
-			on_each_cpu_mask(vkm_cpumask(vkm), ipi_flush_tlb_range,
-					&ta, 1);
-		}
-		ta.ta_vkm = NULL;
-#endif
 		on_each_cpu_mask(mm_cpumask(vma->vm_mm), ipi_flush_tlb_range,
 					&ta, 1);
-	} else {
-#ifdef CONFIG_HAS_VPK
-		if (!list_empty(&mm->vkm_chain)) {
-			unsigned long uaddr;
-			if (get_tlb_flag(TLB_WB))
-				dsb(ishst);
-			for (uaddr = start; uaddr < end; uaddr += PAGE_SIZE)
-				do_tlb_ops(uaddr & PAGE_MASK);
-			if (get_tlb_flag(TLB_BARRIER))
-				dsb(nsh);
-		} else
-#endif
+	} else
 		local_flush_tlb_range(vma, start, end);
-	}
 	broadcast_tlb_mm_a15_erratum(vma->vm_mm);
 }
 
diff --git a/./arch/arm/kernel/suspend.c b/../../../../linux/arch/arm/kernel/suspend.c
index adbd77747..43f0a3ebf 100644
--- a/./arch/arm/kernel/suspend.c
+++ b/../../../../linux/arch/arm/kernel/suspend.c
@@ -44,11 +44,6 @@ int cpu_suspend(unsigned long arg, int (*fn)(unsigned long))
 	unpause_graph_tracing();
 
 	if (ret == 0) {
-#ifdef CONFIG_HAS_VPK
-		if (per_cpu(loaded_vkm, smp_processor_id()))
-			cpu_switch_vkm(per_cpu(loaded_vkm, smp_processor_id()));
-		else
-#endif
 		cpu_switch_mm(mm->pgd, mm);
 		local_flush_bp_all();
 		local_flush_tlb_all();
diff --git a/./arch/arm/mm/Kconfig b/../../../../linux/arch/arm/mm/Kconfig
index 0f466aed9..9724c16e9 100644
--- a/./arch/arm/mm/Kconfig
+++ b/../../../../linux/arch/arm/mm/Kconfig
@@ -480,7 +480,6 @@ config CPU_32v6K
 
 config CPU_32v7
 	bool
-	select CPU_USE_DOMAINS if MMU
 
 config CPU_32v7M
 	bool
diff --git a/./arch/arm/mm/context.c b/../../../../linux/arch/arm/mm/context.c
index 03d152660..48091870d 100644
--- a/./arch/arm/mm/context.c
+++ b/../../../../linux/arch/arm/mm/context.c
@@ -46,9 +46,6 @@ static DECLARE_BITMAP(asid_map, NUM_USER_ASIDS);
 static DEFINE_PER_CPU(atomic64_t, active_asids);
 static DEFINE_PER_CPU(u64, reserved_asids);
 static cpumask_t tlb_flush_pending;
-#ifdef CONFIG_HAS_VPK
-DEFINE_PER_CPU(struct vkey_map_struct *, loaded_vkm) = 0;
-#endif
 
 #ifdef CONFIG_ARM_ERRATA_798181
 void a15_erratum_get_cpumask(int this_cpu, struct mm_struct *mm,
@@ -59,7 +56,7 @@ void a15_erratum_get_cpumask(int this_cpu, struct mm_struct *mm,
 	u64 context_id, asid;
 
 	raw_spin_lock_irqsave(&cpu_asid_lock, flags);
-	context_id = mm->context.id.counter;	/* Not used in RPI */
+	context_id = mm->context.id.counter;
 	for_each_online_cpu(cpu) {
 		if (cpu == this_cpu)
 			continue;
@@ -189,65 +186,6 @@ static bool check_update_reserved_asid(u64 asid, u64 newasid)
 	return hit;
 }
 
-#ifdef CONFIG_HAS_VPK
-static u64 new_context(struct mm_struct *mm, unsigned int cpu, struct vkey_map_struct *vkm)
-{
-	static u32 cur_idx = 1;
-	u64 asid = atomic64_read(&mm->context.id);
-	u64 generation = atomic64_read(&asid_generation);
-
-	if (vkm && vkm->pgd != mm->pgd)
-		asid = atomic64_read(&vkm->ctx_id);
-
-	if (asid != 0) {
-		u64 newasid = generation | (asid & ~ASID_MASK);
-
-		/*
-		 * If our current ASID was active during a rollover, we
-		 * can continue to use it and this was just a false alarm.
-		 */
-		if (check_update_reserved_asid(asid, newasid))
-			return newasid;
-
-		/*
-		 * We had a valid ASID in a previous life, so try to re-use
-		 * it if possible.,
-		 */
-		asid &= ~ASID_MASK;
-		if (!__test_and_set_bit(asid, asid_map))
-			return newasid;
-	}
-
-	/*
-	 * Allocate a free ASID. If we can't find one, take a note of the
-	 * currently active ASIDs and mark the TLBs as requiring flushes.
-	 * We always count from ASID #1, as we reserve ASID #0 to switch
-	 * via TTBR0 and to avoid speculative page table walks from hitting
-	 * in any partial walk caches, which could be populated from
-	 * overlapping level-1 descriptors used to map both the module
-	 * area and the userspace stack.
-	 */
-	asid = find_next_zero_bit(asid_map, NUM_USER_ASIDS, cur_idx);
-	if (asid == NUM_USER_ASIDS) {
-		generation = atomic64_add_return(ASID_FIRST_VERSION,
-						 &asid_generation);
-		flush_context(cpu);
-		asid = find_next_zero_bit(asid_map, NUM_USER_ASIDS, 1);
-	}
-
-	__set_bit(asid, asid_map);
-	cur_idx = asid;
-
-	if (mm->main_vkm) {
-		if (vkm)
-			cpumask_clear(vkm_cpumask(vkm));
-		cpumask_clear(vkm_cpumask(mm->main_vkm));
-	}
-	cpumask_clear(mm_cpumask(mm));
-		
-	return asid | generation;
-}
-#else	/* !CONFIG_HAS_VPK */
 static u64 new_context(struct mm_struct *mm, unsigned int cpu)
 {
 	static u32 cur_idx = 1;
@@ -295,88 +233,15 @@ static u64 new_context(struct mm_struct *mm, unsigned int cpu)
 	cpumask_clear(mm_cpumask(mm));
 	return asid | generation;
 }
-#endif	/* CONFIG_HAS_VPK */
-
-void cpu_switch_vkm(struct vkey_map_struct *vkm)
-{
-	struct mm_struct tmp_mm;
-	tmp_mm.pgd = vkm->pgd;
-	atomic64_set(&tmp_mm.context.id, atomic64_read(&vkm->ctx_id));
-	cpu_switch_mm(vkm->pgd, &tmp_mm);
-}
-
-#ifdef CONFIG_HAS_VPK
-void check_and_switch_context_fast(struct mm_struct *mm, struct task_struct *tsk)
-{
-	unsigned long flags;
-	unsigned int cpu = smp_processor_id();
-	u64 asid;
-	struct vkey_map_struct *vkm = tsk->vkm;
-
-	/*
-	 * We cannot update the pgd and the ASID atomicly with classic
-	 * MMU, so switch exclusively to global mappings to avoid
-	 * speculative page table walking with the wrong TTBR.
-	 */
-	cpu_set_reserved_ttbr0();
-
-
-	if (vkm->pgd != mm->pgd)
-		asid = atomic64_read(&vkm->ctx_id);
-	else
-		asid = atomic64_read(&mm->context.id);
-	if (!((asid ^ atomic64_read(&asid_generation)) >> ASID_BITS)
-	    && atomic64_xchg(&per_cpu(active_asids, cpu), asid))
-		goto switch_mm_fastpath;
-
-	raw_spin_lock_irqsave(&cpu_asid_lock, flags);
-	if (vkm->pgd != mm->pgd)
-		asid = atomic64_read(&vkm->ctx_id);
-	else
-		asid = atomic64_read(&mm->context.id);
-
-	if ((asid ^ atomic64_read(&asid_generation)) >> ASID_BITS) {
-		asid = new_context(mm, cpu, vkm);
-
-		atomic64_set(&vkm->ctx_id, asid);
-		if (vkm == mm->main_vkm)	/* Keep the consistency of ctx_id */
-			atomic64_set(&mm->context.id, asid);
-	}
-
-	if (cpumask_test_and_clear_cpu(cpu, &tlb_flush_pending)) {
-		local_flush_bp_all();
-		local_flush_tlb_all();
-	}
-
-	atomic64_set(&per_cpu(active_asids, cpu), asid);
-	cpumask_set_cpu(cpu, mm_cpumask(mm));
-	raw_spin_unlock_irqrestore(&cpu_asid_lock, flags);
-	cpumask_set_cpu(cpu, vkm_cpumask(vkm));
-
-	printk(KERN_INFO "[%s] fast mm switch should go in fast path\n", __func__);
-
-switch_mm_fastpath:
-	per_cpu(loaded_vkm, cpu) = vkm;
-	cpu_switch_vkm(vkm);
-}
-#endif
 
 void check_and_switch_context(struct mm_struct *mm, struct task_struct *tsk)
 {
 	unsigned long flags;
 	unsigned int cpu = smp_processor_id();
 	u64 asid;
-#ifdef CONFIG_HAS_VPK
-	struct vkey_map_struct *vkm = tsk ? tsk->vkm : NULL;
-#endif
 
-	if (unlikely(mm->context.vmalloc_seq != init_mm.context.vmalloc_seq)) {
+	if (unlikely(mm->context.vmalloc_seq != init_mm.context.vmalloc_seq))
 		__check_vmalloc_seq(mm);
-#ifdef CONFIG_HAS_VPK
-		if (mm->main_vkm)
-			printk(KERN_INFO "[%s] meets different vmalloc seq\n", __func__);
-#endif
-	}
 
 	/*
 	 * We cannot update the pgd and the ASID atomicly with classic
@@ -385,39 +250,17 @@ void check_and_switch_context(struct mm_struct *mm, struct task_struct *tsk)
 	 */
 	cpu_set_reserved_ttbr0();
 
-#ifdef CONFIG_HAS_VPK
-	if (vkm && vkm->pgd != mm->pgd)
-		asid = atomic64_read(&vkm->ctx_id);
-	else
-#endif
 	asid = atomic64_read(&mm->context.id);
 	if (!((asid ^ atomic64_read(&asid_generation)) >> ASID_BITS)
 	    && atomic64_xchg(&per_cpu(active_asids, cpu), asid))
 		goto switch_mm_fastpath;
 
 	raw_spin_lock_irqsave(&cpu_asid_lock, flags);
-#ifdef CONFIG_HAS_VPK
-	if (vkm && vkm->pgd != mm->pgd)
-		asid = atomic64_read(&vkm->ctx_id);
-	else
-#endif
 	/* Check that our ASID belongs to the current generation. */
 	asid = atomic64_read(&mm->context.id);
 	if ((asid ^ atomic64_read(&asid_generation)) >> ASID_BITS) {
-#ifdef CONFIG_HAS_VPK
-		asid = new_context(mm, cpu, vkm);
-
-		if (vkm)
-			atomic64_set(&vkm->ctx_id, asid);
-		else if (mm->main_vkm)	/* Keep the consistency of ctx_id */
-			atomic64_set(&mm->main_vkm->ctx_id, asid);
-
-		if (!vkm || vkm == mm->main_vkm)	/* Keep the consistency of ctx_id */
-			atomic64_set(&mm->context.id, asid);
-#else
 		asid = new_context(mm, cpu);
 		atomic64_set(&mm->context.id, asid);
-#endif
 	}
 
 	if (cpumask_test_and_clear_cpu(cpu, &tlb_flush_pending)) {
@@ -428,22 +271,7 @@ void check_and_switch_context(struct mm_struct *mm, struct task_struct *tsk)
 	atomic64_set(&per_cpu(active_asids, cpu), asid);
 	cpumask_set_cpu(cpu, mm_cpumask(mm));
 	raw_spin_unlock_irqrestore(&cpu_asid_lock, flags);
-#ifdef CONFIG_HAS_VPK
-	if (vkm)
-		cpumask_set_cpu(cpu, vkm_cpumask(vkm));
-#endif
 
 switch_mm_fastpath:
-#ifdef CONFIG_HAS_VPK
-	if (vkm)
-		per_cpu(loaded_vkm, cpu) = vkm;
-	else
-		per_cpu(loaded_vkm, cpu) = NULL;
-#endif
-#ifdef CONFIG_HAS_VPK
-	if (vkm)
-		cpu_switch_vkm(vkm);
-	else
-#endif
 	cpu_switch_mm(mm->pgd, mm);
 }
diff --git a/./arch/arm/mm/fault.c b/../../../../linux/arch/arm/mm/fault.c
index f7162ac3e..a062e0751 100644
--- a/./arch/arm/mm/fault.c
+++ b/../../../../linux/arch/arm/mm/fault.c
@@ -18,7 +18,6 @@
 #include <linux/highmem.h>
 #include <linux/perf_event.h>
 #include <linux/kfence.h>
-#include <linux/vkeys.h>
 
 #include <asm/system_misc.h>
 #include <asm/system_info.h>
@@ -220,8 +219,9 @@ static inline bool is_permission_fault(unsigned int fsr)
 
 static vm_fault_t __kprobes
 __do_page_fault(struct mm_struct *mm, unsigned long addr, unsigned int flags,
-		unsigned long vma_flags, struct pt_regs *regs, struct vm_area_struct *vma)
+		unsigned long vma_flags, struct pt_regs *regs)
 {
+	struct vm_area_struct *vma = find_vma(mm, addr);
 	if (unlikely(!vma))
 		return VM_FAULT_BADMAP;
 
@@ -244,11 +244,6 @@ __do_page_fault(struct mm_struct *mm, unsigned long addr, unsigned int flags,
 	return handle_mm_fault(vma, addr & PAGE_MASK, flags, regs);
 }
 
-#ifdef CONFIG_HAS_VPK
-extern void vkey_print_error_message(unsigned long address, int vkey);
-extern vm_fault_t do_vkey_activate(int vkey);
-#endif
-
 static int __kprobes
 do_page_fault(unsigned long addr, unsigned int fsr, struct pt_regs *regs)
 {
@@ -257,7 +252,6 @@ do_page_fault(unsigned long addr, unsigned int fsr, struct pt_regs *regs)
 	vm_fault_t fault;
 	unsigned int flags = FAULT_FLAG_DEFAULT;
 	unsigned long vm_flags = VM_ACCESS_FLAGS;
-	struct vm_area_struct *vma;
 
 	if (kprobe_page_fault(regs, fsr))
 		return 0;
@@ -316,48 +310,7 @@ retry:
 #endif
 	}
 
-	vma = find_vma(mm, addr);
-#ifdef CONFIG_HAS_VPK
-	if (fsr_fs(fsr) == 11) {
-		int vma_vkey = mm_mprotect_vkey(vma, -1);
-		struct mapped_vkey_struct* mvk = current->mapped_vkeys;
-		bool access_error = true;
-		if (vma_vkey) {
-			if (current->vkrk) {
-				if (!mvk)
-					access_error = false;
-				else {
-					int i;
-					for (i = 0; i < MAX_ACTIVE_VKEYS; i++)
-						if (vma_vkey == mvk->map[i])
-							break;
-					if (i == MAX_ACTIVE_VKEYS)
-						access_error = false;
-				}
-			} else
-				printk(KERN_ERR "[%s] A thread with no VKRK\n", __func__);
-		}
-		if (access_error) {
-			if (get_execute_only_pkey() == /* This deals the conflict between 2MB PMD opt and shadow page table */
-				mm_vkm_pmd_get_pkey(*pmd_offset(pud_offset(
-					p4d_offset((current->vkm ? (current->vkm->pgd + pgd_index(addr)) :
-					pgd_offset_pgd(current->mm->pgd, (addr))), addr), addr), addr)))
-				goto handle_regular_fault;
-			vkey_print_error_message(addr, vma_vkey);
-			mmap_read_unlock(mm);
-			sig = SIGSEGV;
-			code = SEGV_ACCERR;
-			__do_user_fault(addr, fsr, sig, code, regs);
-			return 0;
-		}
-		fault = do_vkey_activate(mm_mprotect_vkey(vma, -1));
-	} else {
-handle_regular_fault:
-#endif
-	fault = __do_page_fault(mm, addr, flags, vm_flags, regs, vma);
-#ifdef CONFIG_HAS_VPK
-	}
-#endif
+	fault = __do_page_fault(mm, addr, flags, vm_flags, regs);
 
 	/* If we need to retry but a fatal signal is pending, handle the
 	 * signal first. We do not need to release the mmap_lock because
diff --git a/./arch/arm/mm/fsr-2level.c b/../../../../linux/arch/arm/mm/fsr-2level.c
index 8217d1476..f2be95197 100644
--- a/./arch/arm/mm/fsr-2level.c
+++ b/../../../../linux/arch/arm/mm/fsr-2level.c
@@ -15,11 +15,7 @@ static struct fsr_info fsr_info[] = {
 	{ do_bad,		SIGBUS,	 0,		"external abort on non-linefetch"  },
 	{ do_bad,		SIGSEGV, SEGV_ACCERR,	"section domain fault"		   },
 	{ do_bad,		SIGBUS,	 0,		"external abort on non-linefetch"  },
-#ifdef CONFIG_HAS_VPK
-	{ do_page_fault,	SIGSEGV, SEGV_MAPERR,	"page domain fault"		   },
-#else
 	{ do_bad,		SIGSEGV, SEGV_ACCERR,	"page domain fault"		   },
-#endif
 	{ do_bad,		SIGBUS,	 0,		"external abort on translation"	   },
 	{ do_sect_fault,	SIGSEGV, SEGV_ACCERR,	"section permission fault"	   },
 	{ do_bad,		SIGBUS,	 0,		"external abort on translation"	   },
@@ -59,11 +55,7 @@ static struct fsr_info ifsr_info[] = {
 	{ do_bad,		SIGBUS,	 0,		"external abort on non-linefetch"  },
 	{ do_bad,		SIGSEGV, SEGV_ACCERR,	"section domain fault"		   },
 	{ do_bad,		SIGBUS,  0,		"unknown 10"			   },
-#ifdef CONFIG_HAS_VPK
-	{ do_page_fault,	SIGSEGV, SEGV_MAPERR,	"page domain fault"		   },
-#else
 	{ do_bad,		SIGSEGV, SEGV_ACCERR,	"page domain fault"		   },
-#endif
 	{ do_bad,		SIGBUS,	 0,		"external abort on translation"	   },
 	{ do_sect_fault,	SIGSEGV, SEGV_ACCERR,	"section permission fault"	   },
 	{ do_bad,		SIGBUS,	 0,		"external abort on translation"	   },
diff --git a/./arch/arm/tools/syscall.tbl b/../../../../linux/arch/arm/tools/syscall.tbl
index 9526ba708..ac964612d 100644
--- a/./arch/arm/tools/syscall.tbl
+++ b/../../../../linux/arch/arm/tools/syscall.tbl
@@ -464,9 +464,3 @@
 448	common	process_mrelease		sys_process_mrelease
 449	common	futex_waitv			sys_futex_waitv
 450	common	set_mempolicy_home_node		sys_set_mempolicy_home_node
-# 451 is reserved
-452 common  vkey_alloc          sys_vkey_alloc
-453 common  vkey_free           sys_vkey_free
-454 common  vkey_mprotect       sys_vkey_mprotect
-455 common  vkey_reg_vkru       sys_vkey_reg_vkru
-456 common  vkey_wrvkrk         sys_vkey_wrvkrk
diff --git a/./arch/x86/Kconfig b/../../../../linux/arch/x86/Kconfig
index d337cb19c..9f5bd41bf 100644
--- a/./arch/x86/Kconfig
+++ b/../../../../linux/arch/x86/Kconfig
@@ -1877,14 +1877,6 @@ config X86_INTEL_MEMORY_PROTECTION_KEYS
 
 	  If unsure, say y.
 
-config HAS_VPK
-	prompt "Virtual Memory Protection Keys"
-	def_bool y
-	depends on X86_INTEL_MEMORY_PROTECTION_KEYS
-	select HAS_VPK_USER_VKRU
-	help
-	  MPK Virtualization via mapping and PCID feature.
-
 choice
 	prompt "TSX enable mode"
 	depends on CPU_SUP_INTEL
diff --git a/./arch/x86/entry/syscalls/syscall_64.tbl b/../../../../linux/arch/x86/entry/syscalls/syscall_64.tbl
index a41b7982d..c84d12608 100644
--- a/./arch/x86/entry/syscalls/syscall_64.tbl
+++ b/../../../../linux/arch/x86/entry/syscalls/syscall_64.tbl
@@ -343,12 +343,6 @@
 332	common	statx			sys_statx
 333	common	io_pgetevents		sys_io_pgetevents
 334	common	rseq			sys_rseq
-335	common	vkey_reg_lib	    sys_vkey_reg_lib
-336	common	vkey_alloc		sys_vkey_alloc
-337	common	vkey_free		sys_vkey_free
-338	common	vkey_mprotect		sys_vkey_mprotect
-339	common	vkey_reg_vkru	    sys_vkey_reg_vkru
-340	common	vkey_activate	    sys_vkey_activate
 # don't use numbers 387 through 423, add new calls after the last
 # 'common' entry
 424	common	pidfd_send_signal	sys_pidfd_send_signal
diff --git a/./arch/x86/include/asm/mmu_context.h b/../../../../linux/arch/x86/include/asm/mmu_context.h
index 1a67d4c88..275160461 100644
--- a/./arch/x86/include/asm/mmu_context.h
+++ b/../../../../linux/arch/x86/include/asm/mmu_context.h
@@ -6,7 +6,6 @@
 #include <linux/atomic.h>
 #include <linux/mm_types.h>
 #include <linux/pkeys.h>
-#include <linux/vkeys.h>
 
 #include <trace/events/tlb.h>
 
@@ -214,11 +213,6 @@ static inline bool arch_vma_access_permitted(struct vm_area_struct *vma,
 	/* allow access if the VMA is not one from this process */
 	if (foreign || vma_is_foreign(vma))
 		return true;
-#ifdef CONFIG_HAS_VPK
-	if (vma->vm_vkey)
-		return true;
-	else
-#endif
 	return __pkru_allows_pkey(vma_pkey(vma), write);
 }
 
diff --git a/./arch/x86/include/asm/pgtable_types.h b/../../../../linux/arch/x86/include/asm/pgtable_types.h
index 7b7703788..40497a902 100644
--- a/./arch/x86/include/asm/pgtable_types.h
+++ b/../../../../linux/arch/x86/include/asm/pgtable_types.h
@@ -59,9 +59,6 @@
 #define _PAGE_PKEY_BIT1	(_AT(pteval_t, 1) << _PAGE_BIT_PKEY_BIT1)
 #define _PAGE_PKEY_BIT2	(_AT(pteval_t, 1) << _PAGE_BIT_PKEY_BIT2)
 #define _PAGE_PKEY_BIT3	(_AT(pteval_t, 1) << _PAGE_BIT_PKEY_BIT3)
-#ifdef CONFIG_HAS_VPK
-#define _PAGE_RESERVED_MASK (_AT(pteval_t, 1) << 51)
-#endif
 #else
 #define _PAGE_PKEY_BIT0	(_AT(pteval_t, 0))
 #define _PAGE_PKEY_BIT1	(_AT(pteval_t, 0))
@@ -442,26 +439,15 @@ static inline pudval_t pud_flags(pud_t pud)
 
 static inline pmdval_t pmd_pfn_mask(pmd_t pmd)
 {
-#if defined(CONFIG_HAS_VPK) && defined(CONFIG_X86_64)
-	if (native_pmd_val(pmd) & _PAGE_PSE)
-		return PHYSICAL_PMD_PAGE_MASK & (~_PAGE_RESERVED_MASK);
-	else
-		return PTE_PFN_MASK & (~_PAGE_RESERVED_MASK);
-#else
 	if (native_pmd_val(pmd) & _PAGE_PSE)
 		return PHYSICAL_PMD_PAGE_MASK;
 	else
 		return PTE_PFN_MASK;
-#endif
 }
 
 static inline pmdval_t pmd_flags_mask(pmd_t pmd)
 {
-#if defined(CONFIG_HAS_VPK) && defined(CONFIG_X86_64)
-	return ~(pmd_pfn_mask(pmd) | _PAGE_RESERVED_MASK);
-#else
 	return ~pmd_pfn_mask(pmd);
-#endif
 }
 
 static inline pmdval_t pmd_flags(pmd_t pmd)
diff --git a/./arch/x86/include/asm/pkeys.h b/../../../../linux/arch/x86/include/asm/pkeys.h
index e9e4793cc..1d5f14aff 100644
--- a/./arch/x86/include/asm/pkeys.h
+++ b/../../../../linux/arch/x86/include/asm/pkeys.h
@@ -7,11 +7,7 @@
  * will be necessary to ensure that the types that store key
  * numbers and masks have sufficient capacity.
  */
-#ifndef arch_max_pkey
 #define arch_max_pkey() (cpu_feature_enabled(X86_FEATURE_OSPKE) ? 16 : 1)
-#endif
-
-#define ARCH_DEFAULT_PKEY	0
 
 extern int arch_set_user_pkey_access(struct task_struct *tsk, int pkey,
 		unsigned long init_val);
@@ -114,8 +110,7 @@ int mm_pkey_alloc(struct mm_struct *mm)
 static inline
 int mm_pkey_free(struct mm_struct *mm, int pkey)
 {
-	/* The default pkey should never be freed */
-	if (!mm_pkey_is_allocated(mm, pkey) || !pkey)
+	if (!mm_pkey_is_allocated(mm, pkey))
 		return -EINVAL;
 
 	mm_set_pkey_free(mm, pkey);
diff --git a/./arch/x86/include/asm/tlb.h b/../../../../linux/arch/x86/include/asm/tlb.h
index 6a60faba1..1bfe979bb 100644
--- a/./arch/x86/include/asm/tlb.h
+++ b/../../../../linux/arch/x86/include/asm/tlb.h
@@ -20,7 +20,7 @@ static inline void tlb_flush(struct mmu_gather *tlb)
 		end = tlb->end;
 	}
 
-	flush_tlb_mm_range(tlb->mm, start, end, stride_shift, tlb->freed_tables, NULL);
+	flush_tlb_mm_range(tlb->mm, start, end, stride_shift, tlb->freed_tables);
 }
 
 /*
diff --git a/./arch/x86/include/asm/tlbflush.h b/../../../../linux/arch/x86/include/asm/tlbflush.h
index 24361f40a..98fa0a114 100644
--- a/./arch/x86/include/asm/tlbflush.h
+++ b/../../../../linux/arch/x86/include/asm/tlbflush.h
@@ -4,7 +4,6 @@
 
 #include <linux/mm.h>
 #include <linux/sched.h>
-#include <linux/vkey_types.h>
 
 #include <asm/processor.h>
 #include <asm/cpufeature.h>
@@ -134,14 +133,6 @@ struct tlb_state {
 	 * context 0.
 	 */
 	struct tlb_context ctxs[TLB_NR_DYN_ASIDS];
-
-#ifdef CONFIG_HAS_VPK
-	/*
-	 * Unfortunately, this will use the 3rd cache line.
-	 * However, we may use less ASIDs per CPU.
-	 */
-	struct vkey_map_struct *loaded_vkm;
-#endif
 };
 DECLARE_PER_CPU_ALIGNED(struct tlb_state, cpu_tlbstate);
 
@@ -213,9 +204,6 @@ struct flush_tlb_info {
 	unsigned long		start;
 	unsigned long		end;
 	u64			new_tlb_gen;
-#ifdef CONFIG_HAS_VPK
-	u64					ctx_id;
-#endif
 	unsigned int		initiating_cpu;
 	u8			stride_shift;
 	u8			freed_tables;
@@ -232,23 +220,23 @@ void flush_tlb_multi(const struct cpumask *cpumask,
 #endif
 
 #define flush_tlb_mm(mm)						\
-		flush_tlb_mm_range(mm, 0UL, TLB_FLUSH_ALL, 0UL, true, NULL)
+		flush_tlb_mm_range(mm, 0UL, TLB_FLUSH_ALL, 0UL, true)
 
 #define flush_tlb_range(vma, start, end)				\
 	flush_tlb_mm_range((vma)->vm_mm, start, end,			\
 			   ((vma)->vm_flags & VM_HUGETLB)		\
 				? huge_page_shift(hstate_vma(vma))	\
-				: PAGE_SHIFT, false, NULL)
+				: PAGE_SHIFT, false)
 
 extern void flush_tlb_all(void);
 extern void flush_tlb_mm_range(struct mm_struct *mm, unsigned long start,
 				unsigned long end, unsigned int stride_shift,
-				bool freed_tables, struct vkey_map_struct *flush_vkm);
+				bool freed_tables);
 extern void flush_tlb_kernel_range(unsigned long start, unsigned long end);
 
 static inline void flush_tlb_page(struct vm_area_struct *vma, unsigned long a)
 {
-	flush_tlb_mm_range(vma->vm_mm, a, a + PAGE_SIZE, PAGE_SHIFT, false, NULL);
+	flush_tlb_mm_range(vma->vm_mm, a, a + PAGE_SIZE, PAGE_SHIFT, false);
 }
 
 static inline u64 inc_mm_tlb_gen(struct mm_struct *mm)
@@ -262,25 +250,9 @@ static inline u64 inc_mm_tlb_gen(struct mm_struct *mm)
 	return atomic64_inc_return(&mm->context.tlb_gen);
 }
 
-static inline u64 inc_vkm_tlb_gen(struct vkey_map_struct *vkm)
-{
-	return atomic64_inc_return(&vkm->tlb_gen);
-}
-
 static inline void arch_tlbbatch_add_mm(struct arch_tlbflush_unmap_batch *batch,
 					struct mm_struct *mm)
 {
-#ifdef CONFIG_HAS_VPK
-	struct list_head *pos;
-	struct vkey_map_struct *vkm;
-	list_for_each(pos, &mm->vkm_chain) {
-		vkm = list_entry(pos, struct vkey_map_struct, vkm_chain);
-		if (vkm->pgd != mm->pgd) {
-			inc_vkm_tlb_gen(vkm);
-			cpumask_or(&batch->cpumask, &batch->cpumask, vkm_cpumask(vkm));
-		}
-	}
-#endif
 	inc_mm_tlb_gen(mm);
 	cpumask_or(&batch->cpumask, &batch->cpumask, mm_cpumask(mm));
 }
diff --git a/./arch/x86/include/asm/vkey_types.h b/./arch/x86/include/asm/vkey_types.h
deleted file mode 100644
index 06e81be25..000000000
--- a/./arch/x86/include/asm/vkey_types.h
+++ /dev/null
@@ -1,9 +0,0 @@
-#ifndef _ASM_X86_VKEY_TYPES_H
-#define _ASM_X86_VKEY_TYPES_H
-
-#ifdef CONFIG_HAS_VPK
-#define arch_max_pkey()		(16)
-typedef u64 vkctx_t;
-#endif
-
-#endif
\ No newline at end of file
diff --git a/./arch/x86/include/asm/vkeys.h b/./arch/x86/include/asm/vkeys.h
deleted file mode 100644
index acf542228..000000000
--- a/./arch/x86/include/asm/vkeys.h
+++ /dev/null
@@ -1,165 +0,0 @@
-#ifndef _ASM_X86_VKEYS_H
-#define _ASM_X86_VKEYS_H
-
-#define VMA_VKEY_MASK	0xffffffff
-#define ARCH_VPK_WRITE_PTE
-
-#define VKEY_AD 0x1		/* access  disable */
-#define VKEY_WD 0x2		/* write   disable */
-#define VKEY_ND 0x0		/* nothing disable */
-#define VKEY_PINNED 0x3
-#define VKEY_MASK 0x5
-
-#ifdef CONFIG_HAS_VPK
-#include <linux/cpumask.h>
-#include <asm/pgalloc.h>
-#include <asm/paravirt.h>
-#include <asm/mmu_context.h>
-#include <asm/trap_pf.h>
-
-extern atomic64_t last_mm_ctx_id;
-
-#define get_execute_only_pkey(mm) (1)
-#define flush_tlb_vkm_range(vma, vkm)				\
-	flush_tlb_mm_range((vma)->vm_mm, (vma)->vm_start, (vma)->vm_end,	\
-				((vma)->vm_flags & VM_HUGETLB) ? 	\
-				huge_page_shift(hstate_vma(vma))	\
-				: PAGE_SHIFT, false, vkm)
-#define flush_tlb_vkm_page(addr, vkm)				\
-	flush_tlb_mm_range((vma)->vm_mm, (addr) & PAGE_MASK, ((addr) & PAGE_MASK) + PAGE_SIZE,	\
-				((vma)->vm_flags & VM_HUGETLB) ? 	\
-				huge_page_shift(hstate_vma(vma))	\
-				: PAGE_SHIFT, false, vkm)
-
-static inline unsigned long mm_vkm_pmd_pkey(pmd_t pmd, int pkey)
-{
-	pmdval_t val = pmd_val(pmd);
-	val &= (pmdval_t)(~_PAGE_PKEY_MASK);
-	val |= (pmdval_t)(
-		(pkey & 0x1 ? _PAGE_PKEY_BIT0 : 0) |	\
-		(pkey & 0x2 ? _PAGE_PKEY_BIT1 : 0) |	\
-		(pkey & 0x4 ? _PAGE_PKEY_BIT2 : 0) |	\
-		(pkey & 0x8 ? _PAGE_PKEY_BIT3 : 0)
-	);
-	return val;
-}
-
-static inline int mm_vkm_pte_get_pkey(pte_t pte)
-{
-	pteval_t val = pte_val(pte);
-	return (val & _PAGE_PKEY_MASK) >> _PAGE_BIT_PKEY_BIT0;
-}
-
-static inline bool vkm_pmd_populate(struct mm_struct *mm, 
-					pmd_t *pmd, pgtable_t pte, int tag)
-{
-	return false;
-}
-
-static inline bool get_vkm_pmd_taint(pmd_t *pmd)
-{
-	return (pmd_val(*pmd) & _PAGE_RESERVED_MASK);
-}
-
-static inline void vkm_pmd_taint(struct mm_struct *mm, 
-			unsigned long addr, pmd_t *pmd, bool taint)
-{
-	pmd_t reserved_pmd = *pmd;
-	pmdval_t val = pmd_val(reserved_pmd);
-	if (taint)
-		val |= (pmdval_t)(_PAGE_RESERVED_MASK);
-	else
-		val &= (pmdval_t)(~_PAGE_RESERVED_MASK);
-	reserved_pmd = __pmd(val);
-	set_pmd_at(mm, addr, pmd, reserved_pmd);
-}
-
-static inline bool vkm_mod_pmd_fast(struct mm_struct *mm, 
-			pmd_t *pmd, pgtable_t pte, int tag,
-			unsigned long start, unsigned long end)
-{
-	bool skip_ptes = false;
-
-	if ((start & PMD_MASK) == start && start + PMD_SIZE <= end) {	/* pmd aligned start and end */
-		if (tag == get_execute_only_pkey(mm)) {
-			vkm_pmd_taint(mm, start, pmd, true);
-			skip_ptes = true;
-		} else if (get_vkm_pmd_taint(pmd))
-			vkm_pmd_taint(mm, start, pmd, false);
-	}
-
-	return skip_ptes;
-}
-
-static inline void mm_vkm_pkru_set_bits(int pkey, u32 perm) 
-{
-	u32 pkru = rdpkru();
-	pkru &= (~(0x3 << (pkey << 1)));
-	pkru |= (perm << (pkey << 1));
-	wrpkru(pkru);
-	return;
-}
-
-static inline u32 mm_vkm_pkru_get_bits(int pkey)
-{
-	return ((rdpkru() >> (pkey << 1)) & 0x3);
-}
-
-static inline u32 mm_vkm_pkru_get(void)
-{
-	return rdpkru();
-}
-
-static inline void mm_vkm_pkru_reset(bool access)
-{
-	u32 pkru = rdpkru();
-	pkru &= 0x0000000c;
-	if (!access)
-		pkru |= 0x55555550;
-	wrpkru(pkru);
-}
-
-static inline int mm_vkm_idx_to_pkey(int i)
-{
-	return i + 1;
-}
-
-static inline pte_t mm_vkm_mkpte(pte_t org_pte, int pkey)
-{
-	return __pte(mm_vkm_pmd_pkey(__pmd(pte_val(org_pte)), pkey));
-}
-
-static inline pmd_t mm_vkm_mkpmd(pmd_t org_pmd, int pkey)
-{
-	return __pmd(mm_vkm_pmd_pkey(org_pmd, pkey));
-}
-
-static inline pud_t mm_vkm_mkpud(pud_t org_pud, int pkey)
-{
-	return __pud(mm_vkm_pmd_pkey(__pmd(pud_val(org_pud)), pkey));
-}
-
-static __always_inline void
-arch_cpumask_clear_vkm(unsigned int cpu, struct cpumask *dstp, struct vkey_map_struct *vkm) {}
-
-static inline void arch_vkm_init(struct mm_struct *mm, 
-				struct vkey_map_struct *vkm, bool main)
-{
-	if (main) {
-		vkm->pgd = mm->pgd;
-		vkm->ctx_id = mm->context.ctx_id;
-	} else {
-		vkm->pgd = pgd_alloc(mm);
-		vkm->ctx_id = atomic64_inc_return(&last_mm_ctx_id);
-	}
-	atomic64_set(&vkm->tlb_gen, 0);
-}
-
-static inline bool mm_vkm_is_reserved_pk_fault(unsigned int flags)
-{
-	return flags & X86_PF_RSVD;
-}
-
-#endif
-
-#endif
\ No newline at end of file
diff --git a/./arch/x86/kernel/alternative.c b/../../../../linux/arch/x86/kernel/alternative.c
index 437997015..b4470eabf 100644
--- a/./arch/x86/kernel/alternative.c
+++ b/../../../../linux/arch/x86/kernel/alternative.c
@@ -1047,7 +1047,7 @@ static void *__text_poke(void *addr, const void *opcode, size_t len)
 	 */
 	flush_tlb_mm_range(poking_mm, poking_addr, poking_addr +
 			   (cross_page_boundary ? 2 : 1) * PAGE_SIZE,
-			   PAGE_SHIFT, false, NULL);
+			   PAGE_SHIFT, false);
 
 	/*
 	 * If the text does not match what we just wrote then something is
diff --git a/./arch/x86/kernel/ldt.c b/../../../../linux/arch/x86/kernel/ldt.c
index e24454d52..525876e7b 100644
--- a/./arch/x86/kernel/ldt.c
+++ b/../../../../linux/arch/x86/kernel/ldt.c
@@ -372,7 +372,7 @@ static void unmap_ldt_struct(struct mm_struct *mm, struct ldt_struct *ldt)
 	}
 
 	va = (unsigned long)ldt_slot_va(ldt->slot);
-	flush_tlb_mm_range(mm, va, va + nr_pages * PAGE_SIZE, PAGE_SHIFT, false, NULL);
+	flush_tlb_mm_range(mm, va, va + nr_pages * PAGE_SIZE, PAGE_SHIFT, false);
 }
 
 #else /* !CONFIG_PAGE_TABLE_ISOLATION */
diff --git a/./arch/x86/mm/fault.c b/../../../../linux/arch/x86/mm/fault.c
index 517b71098..d0074c6ed 100644
--- a/./arch/x86/mm/fault.c
+++ b/../../../../linux/arch/x86/mm/fault.c
@@ -19,8 +19,6 @@
 #include <linux/uaccess.h>		/* faulthandler_disabled()	*/
 #include <linux/efi.h>			/* efi_crash_gracefully_on_page_fault()*/
 #include <linux/mm_types.h>
-#include <linux/vkeys.h>
-#include <linux/vkey_map.h>
 
 #include <asm/cpufeature.h>		/* boot_cpu_has, ...		*/
 #include <asm/traps.h>			/* dotraplinkage, ...		*/
@@ -39,10 +37,6 @@
 #define CREATE_TRACE_POINTS
 #include <asm/trace/exceptions.h>
 
-#ifdef CONFIG_HAS_VPK
-extern vm_fault_t do_vkey_activate(int vkey);
-#endif
-
 /*
  * Returns 0 if mmiotrace is disabled, or if the fault is not
  * handled by mmiotrace:
@@ -1078,9 +1072,6 @@ NOKPROBE_SYMBOL(spurious_kernel_fault);
 
 int show_unhandled_signals = 1;
 
-extern void vkey_print_error_message(unsigned long address, int vkey);
-extern spinlock_t vklock;
-
 static inline int
 access_error(unsigned long error_code, struct vm_area_struct *vma)
 {
@@ -1092,26 +1083,8 @@ access_error(unsigned long error_code, struct vm_area_struct *vma)
 	 * always an unconditional error and can never result in
 	 * a follow-up action to resolve the fault, like a COW.
 	 */
-	if (error_code & X86_PF_PK) {
-#ifdef CONFIG_HAS_VPK
-		int vma_vkey = mm_mprotect_vkey(vma, -1);
-		struct mapped_vkey_struct* mvk = current->mapped_vkeys;
-		int i;
-		/* If vma vkey, then the library must be loaded */
-		if (vma_vkey) {
-			/* Lookup VKRU */
-			if (current->vkru) {
-				if (!mvk)
-					return 0;
-				for (i = 0; i < MAX_ACTIVE_VKEYS; i++)
-					if (vma_vkey == mvk->map[i])
-						return -vma_vkey;
-				return 0;
-			}
-		}
-#endif
+	if (error_code & X86_PF_PK)
 		return 1;
-	}
 
 	/*
 	 * SGX hardware blocked the access.  This usually happens
@@ -1253,7 +1226,6 @@ void do_user_addr_fault(struct pt_regs *regs,
 	struct mm_struct *mm;
 	vm_fault_t fault;
 	unsigned int flags = FAULT_FLAG_DEFAULT;
-	int access_err_code;
 
 	tsk = current;
 	mm = tsk->mm;
@@ -1277,19 +1249,12 @@ void do_user_addr_fault(struct pt_regs *regs,
 	if (WARN_ON_ONCE(kprobe_page_fault(regs, X86_TRAP_PF)))
 		return;
 
-#ifndef CONFIG_HAS_VPK
 	/*
 	 * Reserved bits are never expected to be set on
 	 * entries in the user portion of the page tables.
 	 */
 	if (unlikely(error_code & X86_PF_RSVD))
 		pgtable_bad(regs, error_code, address);
-#else
-	if (error_code & X86_PF_RSVD) {
-		error_code &= (~X86_PF_RSVD);
-		error_code |= X86_PF_PK;
-	}
-#endif
 
 	/*
 	 * If SMAP is on, check for invalid kernel (supervisor) access to user
@@ -1411,22 +1376,7 @@ retry:
 	 * we can handle it..
 	 */
 good_area:
-	access_err_code = access_error(error_code, vma);
-	if (unlikely(access_err_code)) {
-#ifdef CONFIG_HAS_VPK
-		if (error_code & X86_PF_PK) {
-			if (access_err_code < 0)	/* currently only PTE fault can trigger */
-				if (get_execute_only_pkey() == /* This deals the conflict between 2MB PMD opt and shadow page table */
-					mm_vkm_pte_get_pkey(*pte_offset_map(pmd_offset(pud_offset(
-						p4d_offset((current->vkm ? (current->vkm->pgd + pgd_index(address)) : pgd_offset_pgd(current->mm->pgd, (address))), 
-						address), address), address), address))) {
-					flags |= X86_PF_RSVD;
-					goto handle_regular_fault;
-				}
-			if (!(regs->ip & 0xffff800000000000ULL))
-				vkey_print_error_message(address, mm_mprotect_vkey(vma, -1));
-		}
-#endif
+	if (unlikely(access_error(error_code, vma))) {
 		bad_area_access_error(regs, error_code, address, vma);
 		return;
 	}
@@ -1444,34 +1394,7 @@ good_area:
 	 * userland). The return to userland is identified whenever
 	 * FAULT_FLAG_USER|FAULT_FLAG_KILLABLE are both set in flags.
 	 */
-#ifdef CONFIG_HAS_VPK
-	/* 
-	 * Vkey causes fault and passes VKRU check, need vkey-pkey map, 
-	 * only change PTEs but not vm_flags or page_prot.
-	 * However, data race bugs exist if no mutex but we may need fine-grained
-	 * read-write lock for mm, vkm and ptes of each vkm.
-	 * 
-	 * In addition, all data vkey-related structures written here don't
-	 * race with elsewhere because other read and write functions are 
-	 * protected by the mmap_write_lock, mutex with mmap_read_lock.
-	 * 
-	 * Still, we may confront all the problems:
-	 * 1. May be threads always changes the vkey-pkey map if pkeys are few.
-	 * 2. Concurrent change of mm's vkm related and need in-mutex test in case
-	 *    the thread wins first has changed the mm's vkm structure.
-	 * 3. The changes on vkm itself, just use mutex for read and write for 
-	 *    that fine-grained per-vkm lock.
-	 */
-	if (error_code & X86_PF_PK) {
-		fault = do_vkey_activate(mm_mprotect_vkey(vma, -1));
-	} else {
-	/* Not vkey fault. */
-handle_regular_fault:
-#endif
 	fault = handle_mm_fault(vma, address, flags, regs);
-#ifdef CONFIG_HAS_VPK
-	}
-#endif
 
 	if (fault_signal_pending(fault, regs)) {
 		/*
diff --git a/./arch/x86/mm/init.c b/../../../../linux/arch/x86/mm/init.c
index 6c209504b..4ba024d5b 100644
--- a/./arch/x86/mm/init.c
+++ b/../../../../linux/arch/x86/mm/init.c
@@ -1033,9 +1033,6 @@ __visible DEFINE_PER_CPU_ALIGNED(struct tlb_state, cpu_tlbstate) = {
 	.loaded_mm = &init_mm,
 	.next_asid = 1,
 	.cr4 = ~0UL,	/* fail hard if we screw up cr4 shadow initialization */
-#ifdef CONFIG_HAS_VPK
-	.loaded_vkm = NULL,
-#endif
 };
 
 void update_cache_mode_entry(unsigned entry, enum page_cache_mode cache)
diff --git a/./arch/x86/mm/pat/set_memory.c b/../../../../linux/arch/x86/mm/pat/set_memory.c
index 50475866f..b4072115c 100644
--- a/./arch/x86/mm/pat/set_memory.c
+++ b/../../../../linux/arch/x86/mm/pat/set_memory.c
@@ -1103,7 +1103,7 @@ static bool try_to_free_pte_page(pte_t *pte)
 	return true;
 }
 
-bool try_to_free_pmd_page(pmd_t *pmd)
+static bool try_to_free_pmd_page(pmd_t *pmd)
 {
 	int i;
 
diff --git a/./arch/x86/mm/pgtable.c b/../../../../linux/arch/x86/mm/pgtable.c
index 7a7a3ff8a..3481b35cb 100644
--- a/./arch/x86/mm/pgtable.c
+++ b/../../../../linux/arch/x86/mm/pgtable.c
@@ -432,9 +432,6 @@ pgd_t *pgd_alloc(struct mm_struct *mm)
 	if (pgd == NULL)
 		goto out;
 
-#ifdef CONFIG_HAS_VPK
-	if (list_empty(&mm->vkm_chain))
-#endif
 	mm->pgd = pgd;
 
 	if (preallocate_pmds(mm, pmds, PREALLOCATED_PMDS) != 0)
diff --git a/./arch/x86/mm/pkeys.c b/../../../../linux/arch/x86/mm/pkeys.c
index e0aadcd50..e44e93888 100644
--- a/./arch/x86/mm/pkeys.c
+++ b/../../../../linux/arch/x86/mm/pkeys.c
@@ -42,12 +42,8 @@ int __execute_only_pkey(struct mm_struct *mm)
 	 * Set up PKRU so that it denies access for everything
 	 * other than execution.
 	 */
-#ifdef CONFIG_HAS_VPK
-	ret = 0;
-#else
 	ret = arch_set_user_pkey_access(current, execute_only_pkey,
 			PKEY_DISABLE_ACCESS);
-#endif
 	/*
 	 * If the PKRU-set operation failed somehow, just return
 	 * 0 and effectively disable execute-only support.
diff --git a/./arch/x86/mm/tlb.c b/../../../../linux/arch/x86/mm/tlb.c
index 46de28035..a6cf56a14 100644
--- a/./arch/x86/mm/tlb.c
+++ b/../../../../linux/arch/x86/mm/tlb.c
@@ -9,7 +9,6 @@
 #include <linux/cpu.h>
 #include <linux/debugfs.h>
 #include <linux/sched/smt.h>
-#include <linux/vkey_types.h>
 
 #include <asm/tlbflush.h>
 #include <asm/mmu_context.h>
@@ -211,52 +210,6 @@ static void clear_asid_other(void)
 atomic64_t last_mm_ctx_id = ATOMIC64_INIT(1);
 
 
-#ifdef CONFIG_HAS_VPK
-extern struct vkey_per_cpu_cl *vktramp;
-
-static void choose_new_asid(struct mm_struct *next, u64 next_tlb_gen,
-			    u16 *new_asid, bool *need_flush, struct vkey_map_struct *vkm)
-{
-	u16 asid;
-
-	if (!static_cpu_has(X86_FEATURE_PCID)) {
-		*new_asid = 0;
-		*need_flush = true;
-		return;
-	}
-
-	if (this_cpu_read(cpu_tlbstate.invalidate_other))
-		clear_asid_other();
-
-	for (asid = 0; asid < TLB_NR_DYN_ASIDS; asid++) {
-		if (vkm) {
-			if (this_cpu_read(cpu_tlbstate.ctxs[asid].ctx_id) !=
-			    vkm->ctx_id)
-				continue;
-		} else {
-			if (this_cpu_read(cpu_tlbstate.ctxs[asid].ctx_id) !=
-				next->context.ctx_id)
-				continue;
-		}
-
-		*new_asid = asid;
-		*need_flush = (this_cpu_read(cpu_tlbstate.ctxs[asid].tlb_gen) <
-			       next_tlb_gen);
-		return;
-	}
-
-	/*
-	 * We don't currently own an ASID slot on this CPU.
-	 * Allocate a slot.
-	 */
-	*new_asid = this_cpu_add_return(cpu_tlbstate.next_asid, 1) - 1;
-	if (*new_asid >= TLB_NR_DYN_ASIDS) {
-		*new_asid = 0;
-		this_cpu_write(cpu_tlbstate.next_asid, 1);
-	}
-	*need_flush = true;
-}
-#else
 static void choose_new_asid(struct mm_struct *next, u64 next_tlb_gen,
 			    u16 *new_asid, bool *need_flush)
 {
@@ -293,7 +246,6 @@ static void choose_new_asid(struct mm_struct *next, u64 next_tlb_gen,
 	}
 	*need_flush = true;
 }
-#endif
 
 /*
  * Given an ASID, flush the corresponding user ASID.  We can delay this
@@ -372,100 +324,6 @@ void switch_mm(struct mm_struct *prev, struct mm_struct *next,
 	local_irq_restore(flags);
 }
 
-#ifdef CONFIG_HAS_VPK
-void switch_mm_irqs_off_fast(struct mm_struct *mm, struct task_struct *tsk)
-{
-	bool was_lazy = this_cpu_read(cpu_tlbstate_shared.is_lazy);
-	unsigned cpu = smp_processor_id();
-	u64 next_tlb_gen;
-	bool need_flush;
-	u16 new_asid;
-	struct vkey_map_struct *new_vkm = tsk->vkm;
-	struct vkey_map_struct *prev_vkm = this_cpu_read(cpu_tlbstate.loaded_vkm);
-	bool prev_may_use_mm_bm;
-	bool next_may_use_mm_bm;
-
-	prev_may_use_mm_bm = prev_vkm ? prev_vkm->pgd == mm->pgd : true;
-	next_may_use_mm_bm = new_vkm->pgd == mm->pgd;
-
-	copy_vktramp_map_fast(cpu, new_vkm);
-
-	/*
-	 * NB: The scheduler will call us with prev == next when switching
-	 * from lazy TLB mode to normal mode if active_mm isn't changing.
-	 * When this happens, we don't assume that CR3 (and hence
-	 * cpu_tlbstate.loaded_mm) matches next.
-	 *
-	 * NB: leave_mm() calls us with prev == NULL and tsk == NULL.
-	 */
-
-	/* We don't want flush_tlb_func() to run concurrently with us. */
-
-	/*
-	 * Verify that CR3 is what we think it is.  This will catch
-	 * hypothetical buggy code that directly switches to swapper_pg_dir
-	 * without going through leave_mm() / switch_mm_irqs_off() or that
-	 * does something like write_cr3(read_cr3_pa()).
-	 *
-	 * Only do this check if CONFIG_DEBUG_VM=y because __read_cr3()
-	 * isn't free.
-	 */
-	if (was_lazy)
-		this_cpu_write(cpu_tlbstate_shared.is_lazy, false);
-	
-	if (prev_may_use_mm_bm) {
-		if (mm != &init_mm)
-			cpumask_clear_cpu(cpu, mm_cpumask(mm));
-	} else
-		cpumask_clear_cpu(cpu, vkm_cpumask(prev_vkm));
-
-	if (next_may_use_mm_bm) {
-		if (mm != &init_mm) {
-			next_tlb_gen = atomic64_read(&mm->context.tlb_gen);
-			cpumask_set_cpu(cpu, mm_cpumask(mm));
-		}
-	} else {
-		next_tlb_gen = atomic64_read(&new_vkm->tlb_gen);
-		cpumask_set_cpu(cpu, vkm_cpumask(new_vkm));
-	}
-
-	choose_new_asid(mm, next_tlb_gen, &new_asid, &need_flush, new_vkm);
-
-	this_cpu_write(cpu_tlbstate.loaded_mm, LOADED_MM_SWITCHING);
-	barrier();
-
-	if (need_flush) {
-		this_cpu_write(cpu_tlbstate.ctxs[new_asid].ctx_id, new_vkm->ctx_id);
-		this_cpu_write(cpu_tlbstate.ctxs[new_asid].tlb_gen, next_tlb_gen);
-
-		load_new_mm_cr3(new_vkm->pgd, new_asid, true);
-
-		trace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);
-	} else {
-		/* The new ASID is already up to date. */
-		load_new_mm_cr3(new_vkm->pgd, new_asid, false);
-
-		trace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, 0);
-	}
-
-	/* Make sure we write CR3 before loaded_mm. */
-	barrier();
-
-	this_cpu_write(cpu_tlbstate.loaded_mm, mm);
-	this_cpu_write(cpu_tlbstate.loaded_mm_asid, new_asid);
-	this_cpu_write(cpu_tlbstate.loaded_vkm, new_vkm);
-}
-
-void switch_mm_fast(struct mm_struct *mm, struct task_struct *tsk)
-{
-	unsigned long flags;
-
-	local_irq_save(flags);
-	switch_mm_irqs_off_fast(mm, tsk);
-	local_irq_restore(flags);
-}
-#endif
-
 /*
  * Invoked from return to user/guest by a task that opted-in to L1D
  * flushing but ended up running on an SMT enabled core due to wrong
@@ -637,30 +495,6 @@ void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,
 	u64 next_tlb_gen;
 	bool need_flush;
 	u16 new_asid;
-#ifdef CONFIG_HAS_VPK
-	struct vkey_map_struct *new_vkm = tsk ? tsk->vkm : NULL;
-	struct mapped_vkey_struct *mvk = tsk ? tsk->mapped_vkeys : NULL;
-	struct vkey_map_struct *prev_vkm = this_cpu_read(cpu_tlbstate.loaded_vkm);
-	bool prev_may_use_mm_bm;
-	bool next_may_use_mm_bm;
-	bool new_with_different_ctx;
-
-	if (likely(!prev_vkm))
-		prev_may_use_mm_bm = true;
-	else if (prev_vkm->pgd == real_prev->pgd)
-		prev_may_use_mm_bm = true;
-	else
-		prev_may_use_mm_bm = false;
-	if (likely(!new_vkm))
-		next_may_use_mm_bm = true;
-	else
-		next_may_use_mm_bm = (new_vkm->pgd == next->pgd);
-
-	vktramp[cpu].vkru = tsk ? tsk->vkru : 0;
-	if (prev_vkm || new_vkm)
-		copy_vktramp_map(cpu, new_vkm, mvk ? mvk->map : NULL, MAX_ACTIVE_VKEYS);
-
-#endif
 
 	/*
 	 * NB: The scheduler will call us with prev == next when switching
@@ -685,12 +519,6 @@ void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,
 	 * isn't free.
 	 */
 #ifdef CONFIG_DEBUG_VM
-#ifdef CONFIG_HAS_VPK
-	if (prev_vkm) {
-		if (WARN_ON_ONCE(__read_cr3() != build_cr3(prev_vkm->pgd, prev_asid)))
-			__flush_tlb_all();
-	} else
-#endif
 	if (WARN_ON_ONCE(__read_cr3() != build_cr3(real_prev->pgd, prev_asid))) {
 		/*
 		 * If we were to BUG here, we'd be very likely to kill
@@ -723,53 +551,6 @@ void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,
 	 * instruction.
 	 */
 	if (real_prev == next) {
-#ifdef CONFIG_HAS_VPK
-		/* The context id is the same with mm->context.ctx_id if the pgd is the same. */
-		u64 new_ctx_id = new_vkm ? new_vkm->ctx_id : next->context.ctx_id;
-		new_with_different_ctx = this_cpu_read(cpu_tlbstate.ctxs[prev_asid].ctx_id) != new_ctx_id;
-
-		/* If two threads are in the different vkm, still needs context switch. */
-		if (!was_lazy && !new_with_different_ctx)
-			return;
-
-		/*
-		 * Read the tlb_gen to check whether a flush is needed.
-		 * If the TLB is up to date, just use it.
-		 * The barrier synchronizes with the tlb_gen increment in
-		 * the TLB shootdown code.
-		 */
-		smp_mb();
-		if (!next_may_use_mm_bm)
-			next_tlb_gen = atomic64_read(&new_vkm->tlb_gen);
-		else
-			next_tlb_gen = atomic64_read(&next->context.tlb_gen);
-		
-		if (this_cpu_read(cpu_tlbstate.ctxs[prev_asid].tlb_gen) ==
-				next_tlb_gen && !new_with_different_ctx)
-			return;
-		
-		if (unlikely(new_with_different_ctx)) {
-			if (prev_may_use_mm_bm) {
-				if (real_prev != &init_mm)
-					cpumask_clear_cpu(cpu, mm_cpumask(real_prev));
-			} else
-				cpumask_clear_cpu(cpu, vkm_cpumask(prev_vkm));
-
-			if (next_may_use_mm_bm) {
-				if (next != &init_mm)
-					cpumask_set_cpu(cpu, mm_cpumask(next));
-			} else
-				cpumask_set_cpu(cpu, vkm_cpumask(new_vkm));
-
-			choose_new_asid(next, next_tlb_gen, &new_asid, &need_flush, new_vkm);
-
-			this_cpu_write(cpu_tlbstate.loaded_mm, LOADED_MM_SWITCHING);
-			barrier();
-		} else {
-			new_asid = prev_asid;
-			need_flush = true;
-		}
-#else
 		VM_WARN_ON(this_cpu_read(cpu_tlbstate.ctxs[prev_asid].ctx_id) !=
 			   next->context.ctx_id);
 
@@ -808,31 +589,7 @@ void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,
 		 */
 		new_asid = prev_asid;
 		need_flush = true;
-#endif
 	} else {
-#ifdef CONFIG_HAS_VPK
-		cond_mitigation(tsk);
-
-		if (likely(prev_may_use_mm_bm)) {
-			if (real_prev != &init_mm)
-				cpumask_clear_cpu(cpu, mm_cpumask(real_prev));
-		} else
-			cpumask_clear_cpu(cpu, vkm_cpumask(prev_vkm));
-
-		if (likely(next_may_use_mm_bm)) {
-			if (next != &init_mm)
-				cpumask_set_cpu(cpu, mm_cpumask(next));
-		} else
-			cpumask_set_cpu(cpu, vkm_cpumask(new_vkm));
-
-		/* Vkm with same pgd has same tlb_gen if correctly impl. */
-		if (!next_may_use_mm_bm)
-			next_tlb_gen = atomic64_read(&new_vkm->tlb_gen);
-		else
-			next_tlb_gen = atomic64_read(&next->context.tlb_gen);
-
-		choose_new_asid(next, next_tlb_gen, &new_asid, &need_flush, new_vkm);	
-#else
 		/*
 		 * Apply process to process speculation vulnerability
 		 * mitigations if applicable.
@@ -858,7 +615,6 @@ void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,
 		next_tlb_gen = atomic64_read(&next->context.tlb_gen);
 
 		choose_new_asid(next, next_tlb_gen, &new_asid, &need_flush);
-#endif
 
 		/* Let nmi_uaccess_okay() know that we're changing CR3. */
 		this_cpu_write(cpu_tlbstate.loaded_mm, LOADED_MM_SWITCHING);
@@ -866,28 +622,13 @@ void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,
 	}
 
 	if (need_flush) {
-#ifdef CONFIG_HAS_VPK
-		if (unlikely(new_vkm))
-			this_cpu_write(cpu_tlbstate.ctxs[new_asid].ctx_id, new_vkm->ctx_id);
-		else
-#endif
 		this_cpu_write(cpu_tlbstate.ctxs[new_asid].ctx_id, next->context.ctx_id);
 		this_cpu_write(cpu_tlbstate.ctxs[new_asid].tlb_gen, next_tlb_gen);
-#ifdef CONFIG_HAS_VPK
-		if (unlikely(new_vkm))
-			load_new_mm_cr3(new_vkm->pgd, new_asid, true);
-		else
-#endif
 		load_new_mm_cr3(next->pgd, new_asid, true);
 
 		trace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);
 	} else {
 		/* The new ASID is already up to date. */
-#ifdef CONFIG_HAS_VPK
-		if (unlikely(new_vkm))
-			load_new_mm_cr3(new_vkm->pgd, new_asid, false);
-		else
-#endif
 		load_new_mm_cr3(next->pgd, new_asid, false);
 
 		trace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, 0);
@@ -898,9 +639,6 @@ void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,
 
 	this_cpu_write(cpu_tlbstate.loaded_mm, next);
 	this_cpu_write(cpu_tlbstate.loaded_mm_asid, new_asid);
-#ifdef CONFIG_HAS_VPK
-	this_cpu_write(cpu_tlbstate.loaded_vkm, new_vkm);
-#endif
 
 	if (next != real_prev) {
 		cr4_update_pce_mm(next);
@@ -949,12 +687,6 @@ void initialize_tlbstate_and_flush(void)
 	u64 tlb_gen = atomic64_read(&init_mm.context.tlb_gen);
 	unsigned long cr3 = __read_cr3();
 
-#ifdef CONFIG_HAS_VPK
-	struct vkey_map_struct *vkm = this_cpu_read(cpu_tlbstate.loaded_vkm);
-	if (vkm)
-		WARN_ON((cr3 & CR3_ADDR_MASK) != __pa(vkm->pgd));
-	else
-#endif
 	/* Assert that CR3 already references the right mm. */
 	WARN_ON((cr3 & CR3_ADDR_MASK) != __pa(mm->pgd));
 
@@ -966,11 +698,6 @@ void initialize_tlbstate_and_flush(void)
 	WARN_ON(boot_cpu_has(X86_FEATURE_PCID) &&
 		!(cr4_read_shadow() & X86_CR4_PCIDE));
 
-#ifdef CONFIG_HAS_VPK
-	if (vkm)
-		write_cr3(build_cr3(vkm->pgd, 0));
-	else
-#endif
 	/* Force ASID 0 and force a TLB flush. */
 	write_cr3(build_cr3(mm->pgd, 0));
 
@@ -978,11 +705,6 @@ void initialize_tlbstate_and_flush(void)
 	this_cpu_write(cpu_tlbstate.last_user_mm_spec, LAST_USER_MM_INIT);
 	this_cpu_write(cpu_tlbstate.loaded_mm_asid, 0);
 	this_cpu_write(cpu_tlbstate.next_asid, 1);
-#ifdef CONFIG_HAS_VPK
-	if (vkm)
-		this_cpu_write(cpu_tlbstate.ctxs[0].ctx_id, vkm->ctx_id);
-	else
-#endif
 	this_cpu_write(cpu_tlbstate.ctxs[0].ctx_id, mm->context.ctx_id);
 	this_cpu_write(cpu_tlbstate.ctxs[0].tlb_gen, tlb_gen);
 
@@ -1015,14 +737,6 @@ static void flush_tlb_func(void *info)
 	u64 local_tlb_gen = this_cpu_read(cpu_tlbstate.ctxs[loaded_mm_asid].tlb_gen);
 	bool local = smp_processor_id() == f->initiating_cpu;
 	unsigned long nr_invalidate = 0;
-#ifdef CONFIG_HAS_VPK
-	u64 loaded_ctx_id = this_cpu_read(cpu_tlbstate.ctxs[loaded_mm_asid].ctx_id);
-	struct vkey_map_struct *loaded_vkm = this_cpu_read(cpu_tlbstate.loaded_vkm);
-	u64 vkm_tlb_gen = (loaded_vkm && 
-		loaded_vkm->pgd != loaded_mm->pgd) ? atomic64_read(&loaded_vkm->tlb_gen) : mm_tlb_gen;
-	bool part_flush_cond_mm = !loaded_vkm;
-	bool part_flush_cond_vkm = loaded_vkm;
-#endif
 
 	/* This code cannot presently handle being reentered. */
 	VM_WARN_ON(!irqs_disabled());
@@ -1032,25 +746,13 @@ static void flush_tlb_func(void *info)
 		count_vm_tlb_event(NR_TLB_REMOTE_FLUSH_RECEIVED);
 
 		/* Can only happen on remote CPUs */
-#ifdef CONFIG_HAS_VPK
-		/* In case context switch during IPI */
-		if (f->mm && f->ctx_id != loaded_ctx_id)
-			return;
-#else
 		if (f->mm && f->mm != loaded_mm)
 			return;
-#endif
 	}
 
 	if (unlikely(loaded_mm == &init_mm))
 		return;
 
-#ifdef CONFIG_HAS_VPK
-	if (loaded_vkm)
-		VM_WARN_ON(this_cpu_read(cpu_tlbstate.ctxs[loaded_mm_asid].ctx_id) !=
-		   loaded_vkm->ctx_id);
-	else
-#endif
 	VM_WARN_ON(this_cpu_read(cpu_tlbstate.ctxs[loaded_mm_asid].ctx_id) !=
 		   loaded_mm->context.ctx_id);
 
@@ -1068,12 +770,6 @@ static void flush_tlb_func(void *info)
 		return;
 	}
 
-#ifdef CONFIG_HAS_VPK
-	if (loaded_vkm) {
-		if (unlikely(local_tlb_gen == vkm_tlb_gen))
-			goto done;
-	} else
-#endif
 	if (unlikely(local_tlb_gen == mm_tlb_gen)) {
 		/*
 		 * There's nothing to do: we're already up to date.  This can
@@ -1084,17 +780,8 @@ static void flush_tlb_func(void *info)
 		goto done;
 	}
 
-#ifdef CONFIG_HAS_VPK
-	if (loaded_vkm) {
-		WARN_ON_ONCE(local_tlb_gen > vkm_tlb_gen);
-		WARN_ON_ONCE(f->new_tlb_gen > vkm_tlb_gen);
-	} else {
-#endif
 	WARN_ON_ONCE(local_tlb_gen > mm_tlb_gen);
 	WARN_ON_ONCE(f->new_tlb_gen > mm_tlb_gen);
-#ifdef CONFIG_HAS_VPK
-	}
-#endif
 
 	/*
 	 * If we get to this point, we know that our TLB is out of date.
@@ -1133,17 +820,9 @@ static void flush_tlb_func(void *info)
 	 *    local_tlb_gen all the way to mm_tlb_gen and we can probably
 	 *    avoid another flush in the very near future.
 	 */
-#ifdef CONFIG_HAS_VPK
-	part_flush_cond_mm &= (f->new_tlb_gen == local_tlb_gen + 1 && f->new_tlb_gen == mm_tlb_gen);
-	part_flush_cond_vkm &= (f->new_tlb_gen == local_tlb_gen + 1 && f->new_tlb_gen == vkm_tlb_gen);
-	if (f->end != TLB_FLUSH_ALL &&
-		(part_flush_cond_mm || part_flush_cond_vkm)
-		) {
-#else
 	if (f->end != TLB_FLUSH_ALL &&
 	    f->new_tlb_gen == local_tlb_gen + 1 &&
 	    f->new_tlb_gen == mm_tlb_gen) {
-#endif
 		/* Partial flush */
 		unsigned long addr = f->start;
 
@@ -1165,11 +844,6 @@ static void flush_tlb_func(void *info)
 	}
 
 	/* Both paths above update our state to mm_tlb_gen. */
-#ifdef CONFIG_HAS_VPK
-	if (loaded_vkm)
-		this_cpu_write(cpu_tlbstate.ctxs[loaded_mm_asid].tlb_gen, vkm_tlb_gen);
-	else
-#endif
 	this_cpu_write(cpu_tlbstate.ctxs[loaded_mm_asid].tlb_gen, mm_tlb_gen);
 
 	/* Tracing is done in a unified manner to reduce the code size */
@@ -1271,17 +945,10 @@ static DEFINE_PER_CPU_SHARED_ALIGNED(struct flush_tlb_info, flush_tlb_info);
 static DEFINE_PER_CPU(unsigned int, flush_tlb_info_idx);
 #endif
 
-#ifdef CONFIG_HAS_VPK
-static struct flush_tlb_info *get_flush_tlb_info(struct mm_struct *mm,
-			u64 ctx_id, unsigned long start, unsigned long end,
-			unsigned int stride_shift, bool freed_tables,
-			u64 new_tlb_gen)
-#else
 static struct flush_tlb_info *get_flush_tlb_info(struct mm_struct *mm,
 			unsigned long start, unsigned long end,
 			unsigned int stride_shift, bool freed_tables,
 			u64 new_tlb_gen)
-#endif
 {
 	struct flush_tlb_info *info = this_cpu_ptr(&flush_tlb_info);
 
@@ -1301,9 +968,6 @@ static struct flush_tlb_info *get_flush_tlb_info(struct mm_struct *mm,
 	info->freed_tables	= freed_tables;
 	info->new_tlb_gen	= new_tlb_gen;
 	info->initiating_cpu	= smp_processor_id();
-#ifdef CONFIG_HAS_VPK
-	info->ctx_id	= ctx_id;
-#endif
 
 	return info;
 }
@@ -1317,18 +981,13 @@ static void put_flush_tlb_info(void)
 #endif
 }
 
-/* The flush_vkm is the vkey map address space to flush. */
 void flush_tlb_mm_range(struct mm_struct *mm, unsigned long start,
 				unsigned long end, unsigned int stride_shift,
-				bool freed_tables, struct vkey_map_struct *flush_vkm)
+				bool freed_tables)
 {
 	struct flush_tlb_info *info;
 	u64 new_tlb_gen;
 	int cpu;
-#ifdef CONFIG_HAS_VPK
-	struct list_head *pos;
-	struct vkey_map_struct *vkm, *this_cpu_vkm;
-#endif
 
 	cpu = get_cpu();
 
@@ -1339,71 +998,20 @@ void flush_tlb_mm_range(struct mm_struct *mm, unsigned long start,
 		end = TLB_FLUSH_ALL;
 	}
 
-#ifdef CONFIG_HAS_VPK
-	this_cpu_vkm = this_cpu_read(cpu_tlbstate.loaded_vkm);
-	list_for_each(pos, &mm->vkm_chain) {
-		vkm = list_entry(pos, struct vkey_map_struct, vkm_chain);
-		if (flush_vkm) {
-			if (vkm == flush_vkm) {
-				if (vkm == mm->main_vkm)
-					break;
-				new_tlb_gen = inc_vkm_tlb_gen(vkm);
-				info = get_flush_tlb_info(mm, vkm->ctx_id, start, end, stride_shift, freed_tables,
-						new_tlb_gen);	/* This may be pretty slow, try only change TLB gen. */
-				if (cpumask_any_but(vkm_cpumask(vkm), cpu) < nr_cpu_ids)
-					flush_tlb_multi(vkm_cpumask(vkm), info);
-				else if (vkm == this_cpu_vkm) {
-					lockdep_assert_irqs_enabled();
-					local_irq_disable();
-					flush_tlb_func(info);
-					local_irq_enable();
-				}
-				put_flush_tlb_info();
-				put_cpu();
-				return;
-			}
-		} else {
-			if (vkm == mm->main_vkm)
-				continue;
-			new_tlb_gen = inc_vkm_tlb_gen(vkm);
-			info = get_flush_tlb_info(mm, vkm->ctx_id, start, end, stride_shift, freed_tables,
-					new_tlb_gen);	/* This may be pretty slow, try only change TLB gen. */
-			if (cpumask_any_but(vkm_cpumask(vkm), cpu) < nr_cpu_ids)
-				flush_tlb_multi(vkm_cpumask(vkm), info);
-			else if (vkm == this_cpu_vkm) {
-				lockdep_assert_irqs_enabled();
-				local_irq_disable();
-				flush_tlb_func(info);
-				local_irq_enable();
-			}
-			put_flush_tlb_info();
-		}
-	}
-#endif
 	/* This is also a barrier that synchronizes with switch_mm(). */
 	new_tlb_gen = inc_mm_tlb_gen(mm);
 
-#ifdef CONFIG_HAS_VPK
-	info = get_flush_tlb_info(mm, mm->context.ctx_id, start, end, stride_shift, freed_tables,
-				  new_tlb_gen);
-#else
 	info = get_flush_tlb_info(mm, start, end, stride_shift, freed_tables,
 				  new_tlb_gen);
-#endif
 
 	/*
 	 * flush_tlb_multi() is not optimized for the common case in which only
 	 * a local TLB flush is needed. Optimize this use-case by calling
 	 * flush_tlb_func_local() directly in this case.
 	 */
-	if (cpumask_any_but(mm_cpumask(mm), cpu) < nr_cpu_ids) {	/* Vkm with the same pgd included. */
+	if (cpumask_any_but(mm_cpumask(mm), cpu) < nr_cpu_ids) {
 		flush_tlb_multi(mm_cpumask(mm), info);
-#ifdef CONFIG_HAS_VPK
-	} else if (mm == this_cpu_read(cpu_tlbstate.loaded_mm) &&
-		(!this_cpu_vkm || this_cpu_vkm == mm->main_vkm)) {
-#else
 	} else if (mm == this_cpu_read(cpu_tlbstate.loaded_mm)) {
-#endif
 		lockdep_assert_irqs_enabled();
 		local_irq_disable();
 		flush_tlb_func(info);
@@ -1447,11 +1055,7 @@ void flush_tlb_kernel_range(unsigned long start, unsigned long end)
 		struct flush_tlb_info *info;
 
 		preempt_disable();
-#ifdef CONFIG_HAS_VPK
-		info = get_flush_tlb_info(NULL, 0, start, end, 0, false, 0);
-#else
 		info = get_flush_tlb_info(NULL, start, end, 0, false, 0);
-#endif
 
 		on_each_cpu(do_kernel_range_flush, info, 1);
 
@@ -1469,14 +1073,7 @@ void flush_tlb_kernel_range(unsigned long start, unsigned long end)
  */
 unsigned long __get_current_cr3_fast(void)
 {
-	unsigned long cr3;
-#ifdef CONFIG_HAS_VPK
-	if (this_cpu_read(cpu_tlbstate.loaded_vkm))
-		cr3 = build_cr3(this_cpu_read(cpu_tlbstate.loaded_vkm)->pgd,
-			this_cpu_read(cpu_tlbstate.loaded_mm_asid));
-	else
-#endif
-	cr3 = build_cr3(this_cpu_read(cpu_tlbstate.loaded_mm)->pgd,
+	unsigned long cr3 = build_cr3(this_cpu_read(cpu_tlbstate.loaded_mm)->pgd,
 		this_cpu_read(cpu_tlbstate.loaded_mm_asid));
 
 	/* For now, be very restrictive about when this can be called. */
@@ -1627,11 +1224,7 @@ void arch_tlbbatch_flush(struct arch_tlbflush_unmap_batch *batch)
 
 	int cpu = get_cpu();
 
-#ifdef CONFIG_HAS_VPK
-	info = get_flush_tlb_info(NULL, 0, 0, TLB_FLUSH_ALL, 0, false, 0);
-#else
 	info = get_flush_tlb_info(NULL, 0, TLB_FLUSH_ALL, 0, false, 0);
-#endif
 	/*
 	 * flush_tlb_multi() is not optimized for the common case in which only
 	 * a local TLB flush is needed. Optimize this use-case by calling
@@ -1679,11 +1272,6 @@ bool nmi_uaccess_okay(void)
 	if (loaded_mm != current_mm)
 		return false;
 
-#ifdef CONFIG_HAS_VPK
-	if (current->vkm)
-		VM_WARN_ON_ONCE(current->vkm->pgd != __va(read_cr3_pa()));
-	else
-#endif
 	VM_WARN_ON_ONCE(current_mm->pgd != __va(read_cr3_pa()));
 
 	return true;
diff --git a/./fs/exec.c b/../../../../linux/fs/exec.c
index 8250117e4..79f2c9483 100644
--- a/./fs/exec.c
+++ b/../../../../linux/fs/exec.c
@@ -66,7 +66,6 @@
 #include <linux/io_uring.h>
 #include <linux/syscall_user_dispatch.h>
 #include <linux/coredump.h>
-#include <linux/vkeys.h>
 
 #include <linux/uaccess.h>
 #include <asm/mmu_context.h>
@@ -754,11 +753,6 @@ int setup_arg_pages(struct linux_binprm *bprm,
 	unsigned long stack_size;
 	unsigned long stack_expand;
 	unsigned long rlim_stack;
-#ifdef CONFIG_HAS_VPK
-	unsigned long vm_vkey = mm_mprotect_vkey(vma, -1);
-#else
-	unsigned long vm_vkey = 0;
-#endif
 
 #ifdef CONFIG_STACK_GROWSUP
 	/* Limit stack size */
@@ -814,7 +808,7 @@ int setup_arg_pages(struct linux_binprm *bprm,
 	vm_flags |= VM_STACK_INCOMPLETE_SETUP;
 
 	ret = mprotect_fixup(vma, &prev, vma->vm_start, vma->vm_end,
-			vm_flags, vm_vkey);
+			vm_flags);
 	if (ret)
 		goto out_unlock;
 	BUG_ON(prev != vma);
diff --git a/./fs/userfaultfd.c b/../../../../linux/fs/userfaultfd.c
index 138eb9dc3..8e03b3d3f 100644
--- a/./fs/userfaultfd.c
+++ b/../../../../linux/fs/userfaultfd.c
@@ -29,7 +29,6 @@
 #include <linux/ioctl.h>
 #include <linux/security.h>
 #include <linux/hugetlb.h>
-#include <linux/vkeys.h>
 
 int sysctl_unprivileged_userfaultfd __read_mostly;
 
@@ -850,7 +849,6 @@ static int userfaultfd_release(struct inode *inode, struct file *file)
 	/* len == 0 means wake all */
 	struct userfaultfd_wake_range range = { .len = 0, };
 	unsigned long new_flags;
-	unsigned long vm_vkey;
 
 	WRITE_ONCE(ctx->released, true);
 
@@ -875,14 +873,9 @@ static int userfaultfd_release(struct inode *inode, struct file *file)
 			prev = vma;
 			continue;
 		}
-#ifdef CONFIG_HAS_VPK
-		vm_vkey = mm_mprotect_vkey(vma, -1);
-#else
-		vm_vkey = 0;
-#endif
 		new_flags = vma->vm_flags & ~__VM_UFFD_FLAGS;
 		prev = vma_merge(mm, prev, vma->vm_start, vma->vm_end,
-				 new_flags, vm_vkey, vma->anon_vma,
+				 new_flags, vma->anon_vma,
 				 vma->vm_file, vma->vm_pgoff,
 				 vma_policy(vma),
 				 NULL_VM_UFFD_CTX, anon_vma_name(vma));
@@ -1289,7 +1282,6 @@ static int userfaultfd_register(struct userfaultfd_ctx *ctx,
 	bool found;
 	bool basic_ioctls;
 	unsigned long start, end, vma_end;
-	unsigned long vm_vkey;
 
 	user_uffdio_register = (struct uffdio_register __user *) arg;
 
@@ -1441,13 +1433,8 @@ static int userfaultfd_register(struct userfaultfd_ctx *ctx,
 			start = vma->vm_start;
 		vma_end = min(end, vma->vm_end);
 
-#ifdef CONFIG_HAS_VPK
-		vm_vkey = mm_mprotect_vkey(vma, -1);
-#else
-		vm_vkey = 0;
-#endif
 		new_flags = (vma->vm_flags & ~__VM_UFFD_FLAGS) | vm_flags;
-		prev = vma_merge(mm, prev, start, vma_end, new_flags, vm_vkey,
+		prev = vma_merge(mm, prev, start, vma_end, new_flags,
 				 vma->anon_vma, vma->vm_file, vma->vm_pgoff,
 				 vma_policy(vma),
 				 ((struct vm_userfaultfd_ctx){ ctx }),
@@ -1524,7 +1511,7 @@ static int userfaultfd_unregister(struct userfaultfd_ctx *ctx,
 	struct uffdio_range uffdio_unregister;
 	unsigned long new_flags;
 	bool found;
-	unsigned long start, end, vma_end, vm_vkey;
+	unsigned long start, end, vma_end;
 	const void __user *buf = (void __user *)arg;
 
 	ret = -EFAULT;
@@ -1624,13 +1611,8 @@ static int userfaultfd_unregister(struct userfaultfd_ctx *ctx,
 			wake_userfault(vma->vm_userfaultfd_ctx.ctx, &range);
 		}
 
-#ifdef CONFIG_HAS_VPK
-		vm_vkey = mm_mprotect_vkey(vma, -1);
-#else
-		vm_vkey = 0;
-#endif
 		new_flags = vma->vm_flags & ~__VM_UFFD_FLAGS;
-		prev = vma_merge(mm, prev, start, vma_end, new_flags, vm_vkey,
+		prev = vma_merge(mm, prev, start, vma_end, new_flags,
 				 vma->anon_vma, vma->vm_file, vma->vm_pgoff,
 				 vma_policy(vma),
 				 NULL_VM_UFFD_CTX, anon_vma_name(vma));
diff --git a/./include/asm-generic/tlb.h b/../../../../linux/include/asm-generic/tlb.h
index 943c72daf..2c68a545f 100644
--- a/./include/asm-generic/tlb.h
+++ b/../../../../linux/include/asm-generic/tlb.h
@@ -14,7 +14,6 @@
 #include <linux/mmu_notifier.h>
 #include <linux/swap.h>
 #include <linux/hugetlb_inline.h>
-#include <linux/vkey_types.h>
 #include <asm/tlbflush.h>
 #include <asm/cacheflush.h>
 
@@ -253,10 +252,6 @@ extern bool __tlb_remove_page_size(struct mmu_gather *tlb, struct page *page,
 struct mmu_gather {
 	struct mm_struct	*mm;
 
-#ifdef CONFIG_HAS_VPK
-	struct vkey_map_struct *vkm;
-#endif
-
 #ifdef CONFIG_MMU_GATHER_TABLE_FREE
 	struct mmu_table_batch	*batch;
 #endif
diff --git a/./include/linux/mm.h b/../../../../linux/include/linux/mm.h
index d7936ed67..5744a3fc4 100644
--- a/./include/linux/mm.h
+++ b/../../../../linux/include/linux/mm.h
@@ -241,7 +241,7 @@ void setup_initial_init_mm(void *start_code, void *end_code,
  */
 
 struct vm_area_struct *vm_area_alloc(struct mm_struct *);
-struct vm_area_struct *vm_area_dup(struct vm_area_struct *, bool);
+struct vm_area_struct *vm_area_dup(struct vm_area_struct *);
 void vm_area_free(struct vm_area_struct *);
 
 #ifndef CONFIG_MMU
@@ -313,12 +313,6 @@ extern unsigned int kobjsize(const void *objp);
 #define VM_HIGH_ARCH_2	BIT(VM_HIGH_ARCH_BIT_2)
 #define VM_HIGH_ARCH_3	BIT(VM_HIGH_ARCH_BIT_3)
 #define VM_HIGH_ARCH_4	BIT(VM_HIGH_ARCH_BIT_4)
-#else
-#define VM_HIGH_ARCH_BIT_0	0
-#define VM_HIGH_ARCH_0	0
-#define VM_HIGH_ARCH_1	0
-#define VM_HIGH_ARCH_2	0
-#define VM_HIGH_ARCH_3	0
 #endif /* CONFIG_ARCH_USES_HIGH_VMA_FLAGS */
 
 #ifdef CONFIG_ARCH_HAS_PKEYS
@@ -334,19 +328,6 @@ extern unsigned int kobjsize(const void *objp);
 #endif
 #endif /* CONFIG_ARCH_HAS_PKEYS */
 
-#ifdef CONFIG_HAS_VPK
-# define VM_VKEY_BIT0	BIT(0)
-# define VM_VKEY_BIT1	BIT(1)
-# define VM_VKEY_BIT2	BIT(2)
-# define VM_VKEY_BIT3	BIT(3)
-# define VM_VKEY_BIT4	BIT(4)
-# define VM_VKEY_BIT5	BIT(5)
-# define VM_VKEY_BIT6	BIT(6)
-# define VM_VKEY_BIT7	BIT(7)
-# define VM_VKEY_BIT8	BIT(8)
-# define VM_VKEY_BIT9	BIT(9)
-#endif
-
 #if defined(CONFIG_X86)
 # define VM_PAT		VM_ARCH_1	/* PAT reserves whole VMA at once (x86) */
 #elif defined(CONFIG_PPC)
@@ -632,9 +613,6 @@ static inline void vma_init(struct vm_area_struct *vma, struct mm_struct *mm)
 	vma->vm_mm = mm;
 	vma->vm_ops = &dummy_vm_ops;
 	INIT_LIST_HEAD(&vma->anon_vma_chain);
-#ifdef CONFIG_HAS_VPK
-	INIT_LIST_HEAD(&vma->vkey_chain);
-#endif
 }
 
 static inline void vma_set_anonymous(struct vm_area_struct *vma)
@@ -1996,7 +1974,7 @@ extern unsigned long change_protection(struct vm_area_struct *vma, unsigned long
 			      unsigned long cp_flags);
 extern int mprotect_fixup(struct vm_area_struct *vma,
 			  struct vm_area_struct **pprev, unsigned long start,
-			  unsigned long end, unsigned long newflags, unsigned long newvkey);
+			  unsigned long end, unsigned long newflags);
 
 /*
  * doesn't attempt to fault and will return short.
@@ -2249,7 +2227,6 @@ static inline void mm_dec_nr_ptes(struct mm_struct *mm) {}
 #endif
 
 int __pte_alloc(struct mm_struct *mm, pmd_t *pmd);
-int __pte_alloc_ignore_vpk(struct mm_struct *mm, pmd_t *pmd);
 int __pte_alloc_kernel(pmd_t *pmd);
 
 #if defined(CONFIG_MMU)
@@ -2380,11 +2357,6 @@ static inline void pgtable_pte_page_dtor(struct page *page)
 #define pte_alloc_map(mm, pmd, address)			\
 	(pte_alloc(mm, pmd) ? NULL : pte_offset_map(pmd, address))
 
-#define pte_alloc_ignore_vpk(mm, pmd) (unlikely(pmd_none(*(pmd))) && __pte_alloc_ignore_vpk(mm, pmd))
-
-#define pte_alloc_map_ignore_vpk(mm, pmd, address)			\
-	(pte_alloc_ignore_vpk(mm, pmd) ? NULL : pte_offset_map(pmd, address))
-
 #define pte_alloc_map_lock(mm, pmd, address, ptlp)	\
 	(pte_alloc(mm, pmd) ?			\
 		 NULL : pte_offset_map_lock(mm, pmd, address, ptlp))
@@ -2653,7 +2625,7 @@ static inline int vma_adjust(struct vm_area_struct *vma, unsigned long start,
 }
 extern struct vm_area_struct *vma_merge(struct mm_struct *,
 	struct vm_area_struct *prev, unsigned long addr, unsigned long end,
-	unsigned long vm_flags, unsigned long vm_vkey, struct anon_vma *, struct file *, pgoff_t,
+	unsigned long vm_flags, struct anon_vma *, struct file *, pgoff_t,
 	struct mempolicy *, struct vm_userfaultfd_ctx, struct anon_vma_name *);
 extern struct anon_vma *find_mergeable_anon_vma(struct vm_area_struct *);
 extern int __split_vma(struct mm_struct *, struct vm_area_struct *,
diff --git a/./include/linux/mm_types.h b/../../../../linux/include/linux/mm_types.h
index 0e684b407..0f549870d 100644
--- a/./include/linux/mm_types.h
+++ b/../../../../linux/include/linux/mm_types.h
@@ -17,7 +17,6 @@
 #include <linux/page-flags-layout.h>
 #include <linux/workqueue.h>
 #include <linux/seqlock.h>
-#include <linux/vkey_types.h>
 
 #include <asm/mmu.h>
 
@@ -403,10 +402,6 @@ struct vm_area_struct {
 	 */
 	pgprot_t vm_page_prot;
 	unsigned long vm_flags;		/* Flags, see mm.h. */
-#ifdef CONFIG_HAS_VPK
-	unsigned long vm_vkey;
-	struct list_head vkey_chain;	/* VMAs with the same vkeys are chained for quick query */
-#endif
 
 	/*
 	 * For areas with an address space and backing store,
@@ -462,20 +457,6 @@ struct vm_area_struct {
 struct kioctx_table;
 struct mm_struct {
 	struct {
-#ifdef CONFIG_HAS_VPK
-		/* Concurrent RW to vkm_chain and main_vkm / vkrk will cause test and test and set for mmap_lock. */
-		struct vkey_struct vkey;	/* Per VM address space virtual protection keys */
-		struct list_head vkm_chain;	/* All vkey mappings in the address space */
-		struct vkey_map_struct *main_vkm;
-		rwlock_t vkey_lock;
-		atomic_t is_main_vkm_free;
-#ifdef CONFIG_HAS_VPK_USER_VKRU
-		unsigned long lvkru_kaddr;
-		unsigned long lvkru_uaddr;	/* This is the pointer to user library, locked once set. */
-		unsigned long vktramp_uaddr;
-		struct vm_area_struct *lvkey_code_vma;
-#endif
-#endif
 		struct vm_area_struct *mmap;		/* list of VMAs */
 		struct rb_root mm_rb;
 		u64 vmacache_seqnum;                   /* per-thread vmacache */
diff --git a/./include/linux/pkeys.h b/../../../../linux/include/linux/pkeys.h
index 6beb26b71..86be8bf27 100644
--- a/./include/linux/pkeys.h
+++ b/../../../../linux/include/linux/pkeys.h
@@ -4,6 +4,8 @@
 
 #include <linux/mm.h>
 
+#define ARCH_DEFAULT_PKEY	0
+
 #ifdef CONFIG_ARCH_HAS_PKEYS
 #include <asm/pkeys.h>
 #else /* ! CONFIG_ARCH_HAS_PKEYS */
diff --git a/./include/linux/sched.h b/../../../../linux/include/linux/sched.h
index 599bb34b5..75ba8aa60 100644
--- a/./include/linux/sched.h
+++ b/../../../../linux/include/linux/sched.h
@@ -34,7 +34,6 @@
 #include <linux/rseq.h>
 #include <linux/seqlock.h>
 #include <linux/kcsan.h>
-#include <linux/vkey_types.h>
 #include <asm/kmap_size.h>
 
 /* task_struct member predeclarations (sorted alphabetically): */
@@ -792,21 +791,6 @@ struct task_struct {
 	struct task_group		*sched_task_group;
 #endif
 
-#ifdef CONFIG_HAS_VPK
-	/* never copy these 2 variables, just nullify them when copy process */
-	struct vkey_map_struct *vkm;					/* pure pointer, never malloc from here */
-	struct mapped_vkey_struct *mapped_vkeys;		/* allocate when at least 1 vkey is activated and free when this is destroyed */
-	struct vkey_map_struct **vkm_arr;
-	struct mapped_vkey_struct **mvk_arr;
-	unsigned int vkm_nas;
-#ifdef CONFIG_HAS_VPK_USER_VKRU
-	unsigned long vkru;		/* userspace address of the vkru */
-	bool vkey_can_load_balance;
-#else
-	struct vkey_vkrk_struct *vkrk;
-#endif
-#endif
-
 #ifdef CONFIG_UCLAMP_TASK
 	/*
 	 * Clamp values requested for a scheduling entity.
diff --git a/./include/linux/syscalls.h b/../../../../linux/include/linux/syscalls.h
index 98a76780e..819c0cb00 100644
--- a/./include/linux/syscalls.h
+++ b/../../../../linux/include/linux/syscalls.h
@@ -1031,14 +1031,6 @@ asmlinkage long sys_pkey_mprotect(unsigned long start, size_t len,
 				  unsigned long prot, int pkey);
 asmlinkage long sys_pkey_alloc(unsigned long flags, unsigned long init_val);
 asmlinkage long sys_pkey_free(int pkey);
-asmlinkage long sys_vkey_reg_lib(unsigned long laddr, unsigned long taddr);
-asmlinkage long sys_vkey_reg_vkru(unsigned long addr, unsigned int nas);
-asmlinkage long sys_vkey_alloc(void);
-asmlinkage long sys_vkey_free(int vkey);
-asmlinkage long sys_vkey_mprotect(unsigned long start, size_t len,
-				  unsigned long prot, int vkey);
-asmlinkage long sys_vkey_wrvkrk(int vkey, int perm);
-asmlinkage long sys_vkey_activate(int vkey);
 asmlinkage long sys_statx(int dfd, const char __user *path, unsigned flags,
 			  unsigned mask, struct statx __user *buffer);
 asmlinkage long sys_rseq(struct rseq __user *rseq, uint32_t rseq_len,
diff --git a/./include/linux/vkey_map.h b/./include/linux/vkey_map.h
deleted file mode 100644
index af77a5943..000000000
--- a/./include/linux/vkey_map.h
+++ /dev/null
@@ -1,49 +0,0 @@
-#ifndef _LINUX_VKEY_MAP_H
-#define _LINUX_VKEY_MAP_H
-
-#include <linux/vkey_types.h>
-#include <linux/mm.h>
-
-#ifdef CONFIG_HAS_VPK
-
-/* These functions are not thread-free yet all mm-related, so need mmap-locks in the caller. */
-
-/* Allocate a vkm and initialize it, returns NULL if pgd or vkm failed allocation. */
-struct vkey_map_struct *mm_vkm_alloc_init(struct mm_struct *mm);
-
-/* Judge first. */
-bool mm_vkm_can_add(struct vkey_map_struct *vkm, vpmap_t *current_vkeys, int len);
-
-/* Add the one vkey to the vkey mapping. */
-vpmap_t mm_vkm_add_vkey(struct vkey_map_struct *vkm, int vkey, int evicted, int xok, int perm);
-
-/* Add the current vkeys to the vkey mapping. */
-void mm_vkm_add(struct vkey_map_struct *vkm, struct mapped_vkey_struct *mvk, int len, int xok, int *current_perm);
-
-/* Delete the vkeys in the vkey mapping and free the whole structure if all vkeys are freed */
-void mm_vkm_del(struct vkey_map_struct *vkm, struct mm_struct *mm, vpmap_t *current_vkeys, int len, int *current_perm, int evicted);
-
-void mm_vkm_free_page_table(struct mmu_gather *tlb, struct vm_area_struct *vma,
-				struct vkey_map_struct *vkm, unsigned long floor, unsigned long ceiling);
-
-void walk_vkey_map(struct mm_struct *mm);
-
-inline struct mapped_vkey_struct *tsk_mvk_alloc(void);
-inline struct mapped_vkey_struct **tsk_mvk_arr_alloc(void);
-inline struct vkey_map_struct **tsk_vkm_arr_alloc(void);
-inline struct vkey_vkrk_struct *tsk_vkrk_alloc(void);
-inline void tsk_mvk_free(struct mapped_vkey_struct *mvk);
-inline void tsk_vkrk_free(struct vkey_vkrk_struct *vkrk);
-inline void tsk_mvk_arr_free(struct mapped_vkey_struct **mvk);
-inline void tsk_vkm_arr_free(struct vkey_map_struct **vkm);
-
-int mm_vkm_mod_p4d_range(struct vm_area_struct *vma,
-	    pgd_t *dst_pgd, pgd_t *src_pgd, unsigned long addr,
-	    unsigned long end, int pkey, bool cp, bool one_pte);
-#endif
-
-extern void vkey_map_caches_init(void);
-void destroy_vkey_map(struct mm_struct *mm);
-void free_vkey_map(struct mm_struct *mm);
-
-#endif
diff --git a/./include/linux/vkey_types.h b/./include/linux/vkey_types.h
deleted file mode 100644
index ec9e9b5f9..000000000
--- a/./include/linux/vkey_types.h
+++ /dev/null
@@ -1,103 +0,0 @@
-#ifndef _LINUX_VKEY_TYPES_H
-#define _LINUX_VKEY_TYPES_H
-
-#include <linux/bitmap.h>
-#include <linux/list.h>
-#include <linux/types.h>
-#include <linux/spinlock.h>
-#include <asm/mmu.h>
-#ifdef CONFIG_HAS_VPK
-#include <asm/vkey_types.h>
-#else
-typedef int vkctx_t;
-#define arch_max_pkey()		(1)
-#endif
-
-/* 128 virt pkey per proc */
-#define arch_max_vkey()		(128)
-#define VKEY_KGD_ENTRIES	(8)
-#define VKEY_KTE_ENTRIES	(16)
-#define ARCH_DEFAULT_VKEY	(0)
-#define MAX_ACTIVE_VKEYS	(arch_max_pkey() - 2 < 0 ? 0 : arch_max_pkey() - 2)
-#define MAX_ADDR_SPACE_PER_THREAD	(6)
-#define VKEY_META_DATA_SZ	(8192)
-
-#if arch_max_vkey() > (1024 * 64)
-typedef unsigned long vpmap_t;
-#define VPMAP_LONGS		(15)
-#elif arch_max_vkey() > (256)
-typedef u16 vpmap_t;
-#define VPMAP_LONGS		(4)
-#else
-typedef u8 vpmap_t;
-#define VPMAP_LONGS		(2)
-#endif
-
-struct vkey_vkrk_struct {
-	unsigned long bm[arch_max_vkey() * 2 / sizeof(unsigned long)];
-};
-
-/*
- * This is the per-domain vkey mapping.
- * When crossing domain, both pgd and pkru register should be switched
- * vkey_dmap_struct bookkeeps:
- * - lru_array:			all pkeys and their vkeys and number of active thread
- * - nr_thread:			number of threads in this mapping
- * - pgd:				the root of the shared address space with different keys
- * Specifically, the vkm_pgd should be dropped when there are no threads in this domain.
- * However, if the vkm_pgd equals its mm->pgd, it shall not be dropped.
- * When vkm is created, if the mm->vkm_chain is empty, the vkm_pgd is copied rather than allocated.
- */
-struct vkey_map_struct {
-	spinlock_t slock;
-	pgd_t *pgd;
-	vkctx_t ctx_id;
-	atomic64_t tlb_gen;
-	int nr_thread;
-	struct {
-		int pkey_nr_thread[arch_max_pkey() - 1];
-		vpmap_t pkey_vkey[arch_max_pkey() - 1];	/* For stronger sec, this should be in another page */
-	};
-	struct list_head vkm_chain;	/* protected by mm->vkey_lock in PF, and mmap write semophore otherwise */
-	/* This varies with the number of CPUs. */
-	unsigned long cpu_bitmap[];
-};
-
-struct mapped_vkey_struct {
-	vpmap_t map[MAX_ACTIVE_VKEYS];
-	vpmap_t pmap[MAX_ACTIVE_VKEYS];
-	u32 pkru;
-	int ts[MAX_ACTIVE_VKEYS];
-};
-
-struct vkey_kgd_struct {
-	struct vkey_kte_struct *ktes[VKEY_KGD_ENTRIES];
-};
-
-struct vkey_kte_struct {
-	struct list_head vkey_vma_heads[VKEY_KTE_ENTRIES];
-};
-
-struct vkey_per_cpu_cl {
-	unsigned long vkru;
-	unsigned long map;		/* Note that this is insecure and let the kernel attacker know the pgd easily */
-	char fillings[L1_CACHE_BYTES - 2 * sizeof(unsigned long)];
-};
-
-/*
- * This is a per-proc structure and a field in mm_struct (as a ptr).
- * vkey_struct bookkeeps:
- * - vkey allocation bitmap (mutex)
- * - 
- */
-struct vkey_struct {
-	DECLARE_BITMAP(vkey_alloc_bm, arch_max_vkey());
-	struct vkey_kgd_struct *kgd;
-};
-
-static inline cpumask_t *vkm_cpumask(struct vkey_map_struct *vkm)
-{
-	return (struct cpumask *)&vkm->cpu_bitmap;
-}
-
-#endif
diff --git a/./include/linux/vkeys.h b/./include/linux/vkeys.h
deleted file mode 100644
index dd956165d..000000000
--- a/./include/linux/vkeys.h
+++ /dev/null
@@ -1,152 +0,0 @@
-#ifndef _LINUX_VKEYS_H
-#define _LINUX_VKEYS_H
-
-#include <linux/mm.h>
-#include <linux/vkey_types.h>
-#include <linux/types.h>
-#include <asm/vkeys.h>
-#include <asm/uaccess.h>
-
-#define vkey_kgd_offset(vkey)		((vkey >> const_ilog2(VKEY_KTE_ENTRIES)) & ((1U << const_ilog2(VKEY_KGD_ENTRIES)) - 1U))
-#define vkey_kte_offset(vkey)		(vkey & ((1U << const_ilog2(VKEY_KTE_ENTRIES)) - 1U))
-#define vkru_byte_offset(vkey)		(vkey >> 2U)
-
-#ifdef CONFIG_HAS_VPK
-
-#define calc_vm_vkey_bits_unmasked(key) (		\
-		((key) & 0x1   ? VM_VKEY_BIT0 : 0) |      \
-		((key) & 0x2   ? VM_VKEY_BIT1 : 0) |      \
-		((key) & 0x4   ? VM_VKEY_BIT2 : 0) |      \
-		((key) & 0x8   ? VM_VKEY_BIT3 : 0) |      \
-        ((key) & 0x10  ? VM_VKEY_BIT4 : 0) |      \
-		((key) & 0x20  ? VM_VKEY_BIT5 : 0) |      \
-		((key) & 0x40  ? VM_VKEY_BIT6 : 0) |      \
-		((key) & 0x80  ? VM_VKEY_BIT7 : 0) |      \
-        ((key) & 0x100 ? VM_VKEY_BIT8 : 0) |      \
-		((key) & 0x200 ? VM_VKEY_BIT9 : 0))
-
-static inline bool mm_vkey_is_allocated(struct mm_struct *mm, int vkey)
-{
-	if (vkey < 0 || vkey >= arch_max_vkey())
-		return false;
-	
-	return test_bit(vkey, mm->vkey.vkey_alloc_bm);
-}
-
-/* from a vkey to the vm_flag */
-static inline u64 calc_vm_vkey_bits(int vkey)
-{
-	u64 wide_vkey_vm_flag = calc_vm_vkey_bits_unmasked(vkey);
-    return wide_vkey_vm_flag & VMA_VKEY_MASK;
-}
-
-int mm_vkey_alloc(struct mm_struct *mm);
-int mm_vkey_free(struct mm_struct *mm, int vkey);
-int vktramp_mmap_lock(struct vm_area_struct *vma);
-int lvkru_mmap_lock(struct vm_area_struct *vma, unsigned long laddr_base);
-
-void walk_vkey_chain(struct mm_struct *mm, int vkey);
-
-static inline int mm_mprotect_vkey(struct vm_area_struct *vma, int vkey)
-{
-	if (vkey != -1)
-		return vkey;
-	return (vma->vm_vkey & VMA_VKEY_MASK);
-}
-
-#ifndef CONFIG_HAS_VPK_USER_VKRU
-static inline int vkey_get_vkrk_permission(struct vkey_vkrk_struct *vkrk, int vkey)
-{
-	int idx = 2 * vkey / sizeof(unsigned long);
-	int oft = vkey % sizeof(unsigned long);	/* Guaranteed to be not overflowed */
-	if (vkrk)
-		return ((vkrk->bm[idx]) >> (oft << 1U)) & 0x3;
-	else
-		return VKEY_AD;
-}
-
-static inline void vkey_set_vkrk_permission(int vkey, int perm)
-{
-	int idx = 2 * vkey / sizeof(unsigned long);
-	int oft = vkey % sizeof(unsigned long);	/* Guaranteed to be not overflowed */
-	struct vkey_vkrk_struct *vkrk = current->vkrk;
-	vkrk->bm[idx] &= (~(3UL << (oft << 1U)));
-	vkrk->bm[idx] |= ((unsigned long)perm << (oft << 1U));
-}
-#else
-static inline int vkey_get_vkru_permission(struct task_struct *tsk, int vkey)
-{
-	u8 vkey_oft = vkey & 0x3;
-	unsigned long vkru_oft = tsk->vkru - tsk->mm->lvkru_uaddr;
-	u8 *vkru_kaddr = (u8 *)(tsk->mm->lvkru_kaddr + vkru_oft + vkru_byte_offset(vkey));
-	return tsk->mm->lvkru_kaddr ? (((*vkru_kaddr) >> (vkey_oft << 1U)) & 0x3) : VKEY_AD;
-}
-#endif
-
-static inline void copy_vktramp_map_fast(int cpu, struct vkey_map_struct *vkm)
-{
-	extern struct vkey_per_cpu_cl *vktramp;
-	unsigned long *vktramp_map_base = (unsigned long *)&vktramp[cpu].map;
-	unsigned long *vkm_map_base = (unsigned long *)vkm->pkey_vkey;
-	*(vktramp_map_base) = *(vkm_map_base);
-	*(vktramp_map_base + 1) = *(vkm_map_base + 1);
-}
-
-static inline void copy_vktramp_map(int cpu, struct vkey_map_struct *vkm, vpmap_t *mvk_map, int len)
-{
-	extern struct vkey_per_cpu_cl *vktramp;
-	int i, j;
-	vpmap_t *vktramp_map_base = (vpmap_t *)&vktramp[cpu].map;
-	if (vkm && mvk_map) {
-		vpmap_t *vkm_map_base = vkm->pkey_vkey;
-		/* vkm lock should be outside this function */
-		/* create the mask for security, not readily synced map may cause */
-		/* A -> pkru ND pkey; B -> delete a pkey; C -> another vkey to that pkey; A -> can access C's vkey */
-		for (i = 0; i < arch_max_pkey() - 1; i++) {
-			*(vktramp_map_base + i) = 0;
-			if (*(vkm_map_base + i))
-				for (j = 0; j < len; j++)
-					if (mvk_map[j] == *(vkm_map_base + i)) {
-						*(vktramp_map_base + i) = *(vkm_map_base + i);
-						break;
-					}
-		}
-	} else {
-		for (i = 0; i < arch_max_pkey() - 1; i++)
-			*(vktramp_map_base + i) = 0;
-	}
-}
-
-#else
-
-static inline bool mm_vkey_is_allocated(struct mm_struct *mm, int vkey)
-{
-	return false;
-}
-
-static inline unsigned long calc_vm_vkey_bits(int vkey)
-{
-	return 0;
-}
-
-static inline int mm_vkey_alloc(struct mm_struct *mm)
-{
-	return -1;
-}
-
-static inline int mm_vkey_free(struct mm_struct *mm, int vkey)
-{
-	return -1;
-}
-
-static inline int mm_mprotect_vkey(struct vm_area_struct *vma, int vkey)
-{
-	return 0;
-}
-
-#endif
-
-void destroy_vkey(struct mm_struct *mm);
-extern void vkey_caches_init(void);
-
-#endif
diff --git a/./include/uapi/asm-generic/unistd.h b/../../../../linux/include/uapi/asm-generic/unistd.h
index 82da96764..1c48b0ae3 100644
--- a/./include/uapi/asm-generic/unistd.h
+++ b/../../../../linux/include/uapi/asm-generic/unistd.h
@@ -886,23 +886,8 @@ __SYSCALL(__NR_futex_waitv, sys_futex_waitv)
 #define __NR_set_mempolicy_home_node 450
 __SYSCALL(__NR_set_mempolicy_home_node, sys_set_mempolicy_home_node)
 
-#define __NR_vkey_reg_lib 451
-__SYSCALL(__NR_vkey_reg_lib, sys_vkey_reg_lib)
-#define __NR_vkey_alloc 452
-__SYSCALL(__NR_vkey_alloc, sys_vkey_alloc)
-#define __NR_vkey_free 453
-__SYSCALL(__NR_vkey_free, sys_vkey_free)
-#define __NR_vkey_mprotect 454
-__SYSCALL(__NR_vkey_mprotect, sys_vkey_mprotect)
-#define __NR_vkey_reg_vkru 455
-__SYSCALL(__NR_vkey_reg_vkru, sys_vkey_reg_vkru)
-#define __NR_vkey_wrvkrk 456
-__SYSCALL(__NR_vkey_wrvkrk, sys_vkey_wrvkrk)
-#define __NR_vkey_activate 457
-__SYSCALL(__NR_vkey_activate, sys_vkey_activate)
-
 #undef __NR_syscalls
-#define __NR_syscalls 458
+#define __NR_syscalls 451
 
 /*
  * 32 bit systems traditionally used different
diff --git a/./init/init_task.c b/../../../../linux/init/init_task.c
index 0e333cd2f..73cc8f035 100644
--- a/./init/init_task.c
+++ b/../../../../linux/init/init_task.c
@@ -101,19 +101,6 @@ struct task_struct init_task
 #endif
 #ifdef CONFIG_CGROUP_SCHED
 	.sched_task_group = &root_task_group,
-#endif
-#ifdef CONFIG_HAS_VPK
-	.vkm = NULL,
-	.mapped_vkeys = NULL,
-	.vkm_arr = NULL,
-	.mvk_arr = NULL,
-	.vkm_nas = 1,
-#ifdef CONFIG_HAS_VPK_USER_VKRU
-	.vkru = 0,
-	.vkey_can_load_balance = true,
-#else
-	.vkrk = NULL,
-#endif
 #endif
 	.ptraced	= LIST_HEAD_INIT(init_task.ptraced),
 	.ptrace_entry	= LIST_HEAD_INIT(init_task.ptrace_entry),
diff --git a/./init/main.c b/../../../../linux/init/main.c
index d6e2c9898..65fa2e41a 100644
--- a/./init/main.c
+++ b/../../../../linux/init/main.c
@@ -99,8 +99,6 @@
 #include <linux/kcsan.h>
 #include <linux/init_syscalls.h>
 #include <linux/stackdepot.h>
-#include <linux/vkeys.h>
-#include <linux/vkey_map.h>
 
 #include <asm/io.h>
 #include <asm/bugs.h>
@@ -1114,10 +1112,6 @@ asmlinkage __visible void __init __no_sanitize_address start_kernel(void)
 	cred_init();
 	fork_init();
 	proc_caches_init();
-#ifdef CONFIG_HAS_VPK
-	vkey_caches_init();
-	vkey_map_caches_init();
-#endif
 	uts_ns_init();
 	key_init();
 	security_init();
diff --git a/./kernel/Makefile b/../../../../linux/kernel/Makefile
index 2c8e3c350..56f4ee97f 100644
--- a/./kernel/Makefile
+++ b/../../../../linux/kernel/Makefile
@@ -15,7 +15,6 @@ obj-y     = fork.o exec_domain.o panic.o \
 obj-$(CONFIG_USERMODE_DRIVER) += usermode_driver.o
 obj-$(CONFIG_MODULES) += kmod.o
 obj-$(CONFIG_MULTIUSER) += groups.o
-obj-$(CONFIG_HAS_VPK)	+= vkeys.o vkey_map.o
 
 ifdef CONFIG_FUNCTION_TRACER
 # Do not trace internal ftrace files
diff --git a/./kernel/fork.c b/../../../../linux/kernel/fork.c
index 1ef21b963..f1e89007f 100644
--- a/./kernel/fork.c
+++ b/../../../../linux/kernel/fork.c
@@ -97,8 +97,6 @@
 #include <linux/scs.h>
 #include <linux/io_uring.h>
 #include <linux/bpf.h>
-#include <linux/vkeys.h>
-#include <linux/vkey_map.h>
 
 #include <asm/pgalloc.h>
 #include <linux/uaccess.h>
@@ -344,14 +342,6 @@ static struct kmem_cache *vm_area_cachep;
 /* SLAB cache for mm_struct structures (tsk->mm) */
 static struct kmem_cache *mm_cachep;
 
-/* VMA operations finally invokes vm_area_alloc, 
- * vm_area_dup, or vm_area_free.
- * Also, in our design and Linux's design,
- * only p/vkey_mprotect changes the flag (mmap does not).
- * Consequently, if we check the vm_flags[vkey] of all functions calling
- * these 3 functions along with vkey_mprotect, we can fully control the
- * vm_area_struct->vkey_chain.
- */
 struct vm_area_struct *vm_area_alloc(struct mm_struct *mm)
 {
 	struct vm_area_struct *vma;
@@ -362,7 +352,7 @@ struct vm_area_struct *vm_area_alloc(struct mm_struct *mm)
 	return vma;
 }
 
-struct vm_area_struct *vm_area_dup(struct vm_area_struct *orig, bool link_vkey_chain)
+struct vm_area_struct *vm_area_dup(struct vm_area_struct *orig)
 {
 	struct vm_area_struct *new = kmem_cache_alloc(vm_area_cachep, GFP_KERNEL);
 
@@ -375,11 +365,6 @@ struct vm_area_struct *vm_area_dup(struct vm_area_struct *orig, bool link_vkey_c
 		 */
 		*new = data_race(*orig);
 		INIT_LIST_HEAD(&new->anon_vma_chain);
-#ifdef CONFIG_HAS_VPK
-		INIT_LIST_HEAD(&new->vkey_chain);
-		if (link_vkey_chain && mm_mprotect_vkey(orig, -1))
-			list_add(&new->vkey_chain, &orig->vkey_chain);		/* same vkey by duplication */
-#endif
 		new->vm_next = new->vm_prev = NULL;
 		dup_anon_vma_name(orig, new);
 	}
@@ -389,9 +374,6 @@ struct vm_area_struct *vm_area_dup(struct vm_area_struct *orig, bool link_vkey_c
 void vm_area_free(struct vm_area_struct *vma)
 {
 	free_anon_vma_name(vma);
-#ifdef CONFIG_HAS_VPK
-	list_del(&vma->vkey_chain);
-#endif
 	kmem_cache_free(vm_area_cachep, vma);
 }
 
@@ -487,18 +469,6 @@ void free_task(struct task_struct *tsk)
 	arch_release_task_struct(tsk);
 	if (tsk->flags & PF_KTHREAD)
 		free_kthread_struct(tsk);
-#ifdef CONFIG_HAS_VPK
-	if (tsk->mapped_vkeys) {	/* task private no need to sync */
-		tsk_mvk_free(tsk->mapped_vkeys);
-		tsk_mvk_arr_free(tsk->mvk_arr);
-	}
-	if (tsk->vkm_arr)
-		tsk_vkm_arr_free(tsk->vkm_arr);
-#ifndef CONFIG_HAS_VPK_USER_VKRU
-	if (tsk->vkrk)
-		tsk_vkrk_free(tsk->vkrk);
-#endif
-#endif
 	free_task_struct(tsk);
 }
 EXPORT_SYMBOL(free_task);
@@ -581,7 +551,7 @@ static __latent_entropy int dup_mmap(struct mm_struct *mm,
 				goto fail_nomem;
 			charge = len;
 		}
-		tmp = vm_area_dup(mpnt, false);
+		tmp = vm_area_dup(mpnt);
 		if (!tmp)
 			goto fail_nomem;
 		retval = vma_dup_policy(mpnt, tmp);
@@ -701,7 +671,7 @@ static void check_mm(struct mm_struct *mm)
 	for (i = 0; i < NR_MM_COUNTERS; i++) {
 		long x = atomic_long_read(&mm->rss_stat.count[i]);
 
-		if (unlikely(x > 0))
+		if (unlikely(x))
 			pr_alert("BUG: Bad rss-counter state mm:%p type:%s val:%ld\n",
 				 mm, resident_page_types[i], x);
 	}
@@ -733,9 +703,6 @@ void __mmdrop(struct mm_struct *mm)
 	mmu_notifier_subscriptions_destroy(mm);
 	check_mm(mm);
 	put_user_ns(mm->user_ns);
-#ifdef CONFIG_HAS_VPK
-	free_vkey_map(mm);
-#endif
 	free_mm(mm);
 }
 EXPORT_SYMBOL_GPL(__mmdrop);
@@ -939,19 +906,6 @@ static struct task_struct *dup_task_struct(struct task_struct *orig, int node)
 #endif
 #ifdef CONFIG_THREAD_INFO_IN_TASK
 	refcount_set(&tsk->stack_refcount, 1);
-#endif
-#ifdef CONFIG_HAS_VPK
-	tsk->vkm = NULL;
-	tsk->mapped_vkeys = NULL;
-	tsk->vkm_arr = NULL;
-	tsk->mvk_arr = NULL;
-	tsk->vkm_nas = 1;
-#ifdef CONFIG_HAS_VPK_USER_VKRU
-	tsk->vkru = 0;
-	tsk->vkey_can_load_balance = true;
-#else
-	tsk->vkrk = NULL;
-#endif
 #endif
 
 	if (err)
@@ -1082,21 +1036,6 @@ static void mm_init_uprobes_state(struct mm_struct *mm)
 static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p,
 	struct user_namespace *user_ns)
 {
-#ifdef CONFIG_HAS_VPK
-	bitmap_zero(mm->vkey.vkey_alloc_bm, arch_max_vkey());
-	set_bit(ARCH_DEFAULT_VKEY, mm->vkey.vkey_alloc_bm);
-	mm->vkey.kgd = NULL;
-	mm->main_vkm = NULL;
-#ifdef CONFIG_HAS_VPK_USER_VKRU
-	mm->lvkru_kaddr = 0;
-	mm->lvkru_uaddr = 0;
-	mm->vktramp_uaddr = 0;
-	mm->lvkey_code_vma = NULL;
-#endif
-	INIT_LIST_HEAD(&mm->vkm_chain);
-	rwlock_init(&mm->vkey_lock);
-	atomic_set(&mm->is_main_vkm_free, 1);
-#endif
 	mm->mmap = NULL;
 	mm->mm_rb = RB_ROOT;
 	mm->vmacache_seqnum = 0;
@@ -1441,14 +1380,6 @@ static int wait_for_vfork_done(struct task_struct *child,
  */
 static void mm_release(struct task_struct *tsk, struct mm_struct *mm)
 {
-#ifdef CONFIG_HAS_VPK
-	if (tsk->vkm) {
-		mmap_write_lock(mm);
-		mm_vkm_del(tsk->vkm, mm, tsk->mapped_vkeys->map, MAX_ACTIVE_VKEYS, NULL, 0);
-		mmap_write_unlock(mm);
-	}
-#endif
-	
 	uprobe_free_utask(tsk);
 
 	/* Get rid of any cached register state */
diff --git a/./kernel/sched/core.c b/../../../../linux/kernel/sched/core.c
index 4e92a0a9a..9745613d5 100644
--- a/./kernel/sched/core.c
+++ b/../../../../linux/kernel/sched/core.c
@@ -6370,10 +6370,7 @@ static void sched_update_worker(struct task_struct *tsk)
 asmlinkage __visible void __sched schedule(void)
 {
 	struct task_struct *tsk = current;
-#if defined(CONFIG_HAS_VPK) && defined(CONFIG_HAS_VPK_USER_VKRU)
-	extern inline void vkey_thread_check_migrate(struct task_struct *tsk);
-	vkey_thread_check_migrate(tsk);
-#endif
+
 	sched_submit_work(tsk);
 	do {
 		preempt_disable();
diff --git a/./kernel/sched/fair.c b/../../../../linux/kernel/sched/fair.c
index c8091c16c..5146163bf 100644
--- a/./kernel/sched/fair.c
+++ b/../../../../linux/kernel/sched/fair.c
@@ -7746,13 +7746,7 @@ int can_migrate_task(struct task_struct *p, struct lb_env *env)
 	 * 2) cannot be migrated to this CPU due to cpus_ptr, or
 	 * 3) running (obviously), or
 	 * 4) are cache-hot on their current CPU.
-	 * 5) stuck in critical section of libvkeys
 	 */
-#if defined(CONFIG_HAS_VPK) && defined(CONFIG_HAS_VPK_USER_VKRU)
-	if (unlikely(!p->vkey_can_load_balance))
-		return 0;
-#endif
-
 	if (throttled_lb_pair(task_group(p), env->src_cpu, env->dst_cpu))
 		return 0;
 
diff --git a/./kernel/vkey_map.c b/./kernel/vkey_map.c
deleted file mode 100644
index d24b8b691..000000000
--- a/./kernel/vkey_map.c
+++ /dev/null
@@ -1,1037 +0,0 @@
-#include <linux/vkey_map.h>
-#include <linux/types.h>
-#include <linux/list.h>
-#include <linux/slab.h>
-#include <linux/mm.h>
-#include <linux/mm_inline.h>
-#include <linux/hugetlb.h>
-#include <linux/rmap.h>
-#include <linux/vkeys.h>
-#include <linux/cpumask.h>
-#include <asm/pgalloc.h>
-#include <asm/pgtable.h>
-#include <asm/tlb.h>
-#include <asm/vkeys.h>
-#include <linux/sched.h>
-
-#include "../mm/internal.h"
-
-#ifdef CONFIG_HAS_VPK
-
-static struct kmem_cache *vkey_vkm_cachep;
-static struct kmem_cache *vkey_mvk_cachep;
-static struct kmem_cache *vkey_vkm_arr_cachep;
-static struct kmem_cache *vkey_mvk_arr_cachep;
-static struct kmem_cache *vkey_vkrk_cachep;
-static void mm_vkm_unmap(struct vkey_map_struct *vkm, struct mm_struct *mm);
-void walk_vkey_thread(struct task_struct *tsk);
-
-extern atomic64_t last_mm_ctx_id;
-
-#define allocate_vkey_vkm()		(kmem_cache_alloc(vkey_vkm_cachep, GFP_KERNEL))
-#define free_vkey_vkm(vkm)		(kmem_cache_free(vkey_vkm_cachep, (vkm)))
-#define allocate_vkey_mvk()		(kmem_cache_alloc(vkey_mvk_cachep, GFP_KERNEL))
-#define free_vkey_mvk(mvk)		(kmem_cache_free(vkey_mvk_cachep, (mvk)))
-#define allocate_vkey_vkrk()	(kmem_cache_alloc(vkey_vkrk_cachep, GFP_KERNEL))
-#define free_vkey_vkrk(vkrk)	(kmem_cache_free(vkey_vkrk_cachep, (vkrk)))
-#define allocate_vkey_mvk_arr()		(kmem_cache_alloc(vkey_mvk_arr_cachep, GFP_KERNEL))
-#define free_vkey_mvk_arr(mvk)		(kmem_cache_free(vkey_mvk_arr_cachep, (mvk)))
-#define allocate_vkey_vkm_arr()		(kmem_cache_alloc(vkey_vkm_arr_cachep, GFP_KERNEL))
-#define free_vkey_vkm_arr(vkm)		(kmem_cache_free(vkey_vkm_arr_cachep, (vkm)))
-
-void vkey_print_error_message(unsigned long address, int vkey);
-
-static inline void vkm_init_cpumask(struct vkey_map_struct *vkm)
-{
-	unsigned long cpu_bitmap = (unsigned long)vkm;
-
-	cpu_bitmap += offsetof(struct vkey_map_struct, cpu_bitmap);
-	cpumask_clear((struct cpumask *)cpu_bitmap);
-}
-
-static inline void init_rss_vec(int *rss)
-{
-	memset(rss, 0, sizeof(int) * NR_MM_COUNTERS);
-}
-
-static inline void add_mm_rss_vec(struct mm_struct *mm, int *rss)
-{
-	int i;
-
-	if (current->mm == mm)
-		sync_mm_rss(mm);
-	for (i = 0; i < NR_MM_COUNTERS; i++)
-		if (rss[i])
-			add_mm_counter(mm, i, rss[i]);
-}
-
-static inline int mm_vkm_mod_pte_range(struct vm_area_struct *vma,
-	       pmd_t *dst_pmd, pmd_t *src_pmd, unsigned long addr,
-	       unsigned long end, bool one_pte, int pkey, bool cp)
-{
-	/* Copy the PTEs of the range from src_pmd to dst_pmd. */
-	struct mm_struct *mm = vma->vm_mm;
-	pte_t *src_pte, *dst_pte;
-	struct page *page;
-	int rss[NR_MM_COUNTERS];
-
-	dst_pte = pte_alloc_map_ignore_vpk(mm, dst_pmd, addr);
-	if (!dst_pte)
-		return -ENOMEM;
-
-	spin_lock(&mm->page_table_lock);
-	if (cp) {
-		src_pte = pte_offset_map(src_pmd, addr);
-		if (pkey != -1)
-			vkm_pmd_populate(mm, dst_pmd, pmd_pgtable(*dst_pmd), pkey);
-		do {
-			pte_t pte;
-			if (pte_none(*src_pte) || !pte_present(*src_pte))
-				continue;
-			pte = *src_pte;
-			if (pkey != -1)
-				pte = mm_vkm_mkpte(pte, pkey);
-			/* If concurrent fault, just skip the counter. */
-			if (!pte_same(pte, *dst_pte)) {
-				page = vm_normal_page(vma, addr, pte);
-				/* Update data page statistics */
-				if (page) {
-					init_rss_vec(rss);
-					get_page(page);
-					page_dup_rmap(page, false);
-					if (PageAnon(page)) {
-						if (!pte_present(*dst_pte) || pte_none(*dst_pte)) {
-							rss[MM_ANONPAGES]++;
-						}
-					} else if (PageSwapBacked(page))
-						rss[MM_SHMEMPAGES]++;
-					else
-						rss[MM_FILEPAGES]++;
-					add_mm_rss_vec(mm, rss);
-				}
-			}
-			set_pte_at(mm, addr, dst_pte, pte);
-		} while (dst_pte++, src_pte++, addr += PAGE_SIZE, addr != end && !one_pte);
-	} else {
-		bool skip_ptes = vkm_mod_pmd_fast(mm, dst_pmd, pmd_pgtable(*dst_pmd), pkey, addr, end);
-		bool same_pkey = false;
-		flush_tlb_batched_pending(mm);
-		arch_enter_lazy_mmu_mode();
-		/* Currently, no need to use mmu_notifier when only the vkey-pkey map changes. */
-#ifdef ARCH_VPK_WRITE_PTE
-		if (!skip_ptes) {
-			do {
-				if (pte_none(*dst_pte) || !pte_present(*dst_pte))
-					continue;
-				if (pkey == mm_vkm_pte_get_pkey(*dst_pte))
-					same_pkey = true;
-				break;
-			} while (dst_pte++, addr += PAGE_SIZE, addr != end);
-			if ((!same_pkey && addr != end) || one_pte)
-				do {
-					pte_t pte, oldpte;
-					if (pte_none(*dst_pte) || !pte_present(*dst_pte))
-						continue;
-					oldpte = pte = ptep_modify_prot_start(vma, addr, dst_pte);
-					pte = mm_vkm_mkpte(pte, pkey);
-					ptep_modify_prot_commit(vma, addr, dst_pte, oldpte, pte);
-				} while (dst_pte++, addr += PAGE_SIZE, addr != end && !one_pte);
-		}
-#endif
-		arch_leave_lazy_mmu_mode();
-	}
-	spin_unlock(&mm->page_table_lock);
-	return 0;
-}
-
-static inline int mm_vkm_mod_pmd_range(struct vm_area_struct *vma,
-	       pud_t *dst_pud, pud_t *src_pud, unsigned long addr,
-	       unsigned long end, bool one_pte, int pkey, bool cp)
-{
-	struct mm_struct *mm = vma->vm_mm;
-	pmd_t *src_pmd, *dst_pmd;
-	unsigned long next;
-
-	dst_pmd = pmd_alloc(mm, dst_pud, addr);
-	if (!dst_pmd)
-		return -ENOMEM;
-
-	if (cp) {
-		src_pmd = pmd_offset(src_pud, addr);
-		do {
-			next = pmd_addr_end(addr, end);
-			if (is_swap_pmd(*src_pmd) || pmd_trans_huge(*src_pmd)
-				|| pmd_devmap(*src_pmd)) {
-				pmd_t pmd;
-				VM_BUG_ON_VMA(next-addr != HPAGE_PMD_SIZE, vma);
-				spin_lock(&mm->page_table_lock);
-				pmd = *src_pmd;
-				if (pkey != -1)
-					pmd = mm_vkm_mkpmd(pmd, pkey);
-				set_pmd_at(mm, addr, dst_pmd, pmd);
-				spin_unlock(&mm->page_table_lock);
-				continue;
-			}
-			if (pmd_none_or_clear_bad(src_pmd))
-				continue;
-			if (mm_vkm_mod_pte_range(vma, dst_pmd, src_pmd, addr, next, one_pte, pkey, cp))
-				return -ENOMEM;
-		} while (dst_pmd++, src_pmd++, addr = next, addr != end && !one_pte);
-	} else {
-		do {
-			next = pmd_addr_end(addr, end);
-			/* See mm/mprotect.c::change_pmd_range. */
-			if (!is_swap_pmd(*dst_pmd) && !pmd_devmap(*dst_pmd) &&
-				pmd_none_or_clear_bad(dst_pmd))
-				continue;
-			if (is_swap_pmd(*dst_pmd) || pmd_trans_huge(*dst_pmd)
-				|| pmd_devmap(*dst_pmd)) {
-				pmd_t pmd;
-				VM_BUG_ON_VMA(next-addr != HPAGE_PMD_SIZE, vma);
-				spin_lock(&mm->page_table_lock);
-				pmd = pmdp_invalidate(vma, addr, dst_pmd);
-				pmd = mm_vkm_mkpmd(pmd, pkey);
-				set_pmd_at(mm, addr, dst_pmd, pmd);
-				spin_unlock(&mm->page_table_lock);
-				continue;
-			}
-			if (mm_vkm_mod_pte_range(vma, dst_pmd, src_pmd, addr, next, one_pte, pkey, cp))
-				return -ENOMEM;
-		} while (dst_pmd++, addr = next, addr != end && !one_pte);
-	}
-	return 0;
-}
-
-static inline int mm_vkm_mod_pud_range(struct vm_area_struct *vma,
-	       p4d_t *dst_p4d, p4d_t *src_p4d, unsigned long addr,
-	       unsigned long end, bool one_pte, int pkey, bool cp)
-{
-	struct mm_struct *mm = vma->vm_mm;
-	pud_t *src_pud, *dst_pud;
-	unsigned long next;
-
-	dst_pud = pud_alloc(mm, dst_p4d, addr);
-	if (!dst_pud)
-		return -ENOMEM;
-
-	if (cp) {
-		src_pud = pud_offset(src_p4d, addr);
-		do {
-			next = pud_addr_end(addr, end);
-			if (pud_trans_huge(*src_pud) || pud_devmap(*src_pud)) {
-				pud_t pud;
-				VM_BUG_ON_VMA(next-addr != HPAGE_PUD_SIZE, vma);
-				spin_lock(&mm->page_table_lock);
-				pud = *src_pud;
-				if (pkey != -1)
-					pud = mm_vkm_mkpud(pud, pkey);
-				set_pud_at(mm, addr, dst_pud, pud);
-				spin_unlock(&mm->page_table_lock);
-				continue;
-			}
-			if (pud_none_or_clear_bad(src_pud))
-				continue;
-			if (mm_vkm_mod_pmd_range(vma, dst_pud, src_pud, addr, next, one_pte, pkey, cp))
-				return -ENOMEM;
-		} while (dst_pud++, src_pud++, addr = next, addr != end && !one_pte);
-	} else {
-		do {
-			next = pud_addr_end(addr, end);
-			if (pud_none_or_clear_bad(dst_pud))
-				continue;
-			mm_vkm_mod_pmd_range(vma, dst_pud, NULL, addr, next, one_pte, pkey, cp);
-		} while (dst_pud++, addr = next, addr != end && !one_pte);		
-	}
-	return 0;	
-}
-
-int mm_vkm_mod_p4d_range(struct vm_area_struct *vma,
-	       pgd_t *dst_pgd, pgd_t *src_pgd, unsigned long addr,
-	       unsigned long end, int pkey, bool cp, bool one_pte)
-{
-	struct mm_struct *mm = vma->vm_mm;
-	p4d_t *src_p4d, *dst_p4d;
-	unsigned long next;
-
-	/* By default, p4d is shared by copied pgd range and not none. */
-	dst_p4d = p4d_alloc(mm, dst_pgd, addr);
-	if (!dst_p4d)
-		return -ENOMEM;
-
-	if (cp) {
-		src_p4d = p4d_offset(src_pgd, addr);
-		do {
-			next = p4d_addr_end(addr, end);
-			if (p4d_none_or_clear_bad(src_p4d))
-				continue;
-			if (mm_vkm_mod_pud_range(vma, dst_p4d, src_p4d,
-					addr, next, one_pte, pkey, cp))
-				return -ENOMEM;
-		} while (dst_p4d++, src_p4d++, addr = next, addr != end && !one_pte);
-	} else {
-		do {
-			next = p4d_addr_end(addr, end);
-			if (p4d_none_or_clear_bad(dst_p4d))
-				continue;
-			mm_vkm_mod_pud_range(vma, dst_p4d, NULL,
-					addr, next, one_pte, pkey, cp);
-		} while (dst_p4d++, addr = next, addr != end && !one_pte);
-	}
-	return 0;
-}
-
-/* This function refers copy_page_range(), which calls the following chain. */
-/* Copy the whole page table from the orig pgd, or change the mapped protection key when not -1. */
-static int
-mm_vkm_mod_page_range(struct vkey_map_struct *vkm, struct vm_area_struct *vma, int pkey, bool cp)
-{
-	pgd_t *src_pgd, *dst_pgd;
-	struct mm_struct *src_mm = vma->vm_mm;
-	unsigned long addr = vma->vm_start;
-	unsigned long end = vma->vm_end;
-	unsigned long next;
-
-	/* Skip special mapping and to-be-filled by page fault vm areas. */
-	if (!(vma->vm_flags & (VM_HUGETLB | VM_PFNMAP | VM_MIXEDMAP)) &&
-	    !vma->anon_vma && cp)
-		return 0;
-
-	/* Special copy of huge TLB feature, currently not supported. */
-	if (unlikely(is_vm_hugetlb_page(vma))) {
-		printk(KERN_ERR "[%s] vkey currently does not support huge TLB page...\n", __func__);
-		return -EINVAL;
-	}
-
-	/* Copy the PTEs one by one. */
-	dst_pgd = vkm->pgd + pgd_index(addr);
-	if (cp) {
-		src_pgd = pgd_offset(src_mm, addr);
-		do {
-			next = pgd_addr_end(addr, end);
-			if (pgd_none_or_clear_bad(src_pgd))
-				continue;
-			if (unlikely(mm_vkm_mod_p4d_range(vma, dst_pgd, src_pgd, addr, next, pkey, cp, false)))
-				return -ENOMEM;
-		} while (dst_pgd++, src_pgd++, addr = next, addr != end);
-	} else {
-		do {
-			next = pgd_addr_end(addr, end);
-			/* Leave it to page fault. */
-			if (pgd_none_or_clear_bad(dst_pgd))
-				continue;
-			mm_vkm_mod_p4d_range(vma, dst_pgd, NULL, addr, next, pkey, cp, false);
-		} while (dst_pgd++, addr = next, addr != end);
-	}
-
-	return 0;
-}
-
-static int mm_vkm_copy_page_table(struct vkey_map_struct *vkm, struct mm_struct *mm)
-{
-	struct vm_area_struct *src_vma;
-	int err;
-	
-	/* No need to copy if the vkm is the first one. */
-	if (mm->pgd == vkm->pgd)
-		return 0;
-
-	for (src_vma = mm->mmap; src_vma; src_vma = src_vma->vm_next) {
-		if (mm_mprotect_vkey(src_vma, -1)) {	/* Initialize the vkey protected vmas with execute only pkey */
-			if (unlikely(err = mm_vkm_mod_page_range(vkm, src_vma, execute_only_pkey(mm), true)))
- 				return err;
-		} else if (unlikely(err = mm_vkm_mod_page_range(vkm, src_vma, -1, true)))
-			return err;
-	}
-
-	return 0;
-}
-
-/* Change the pkey fields of present PTEs of vkeys. */
-static int mm_vkm_mprotect_present_ptes(struct vkey_map_struct *vkm, 
-			struct mm_struct *mm, int vkey, int pkey)
-{
-	struct list_head *pos;
-	struct vm_area_struct *vma;
-	struct vkey_kgd_struct *kgd = mm->vkey.kgd;
-	struct vkey_kte_struct *kte = NULL;
-
-	if (!kgd)
-		return 0;
-	else
-		kte = kgd->ktes[vkey_kgd_offset(vkey)];
-	if (!kte)
-		return 0;
-
-	list_for_each(pos, &(kte->vkey_vma_heads[vkey_kte_offset(vkey)])) {
-
-		/* Finally, we get the vmas indexed by the vkey from our vkey table. */
-		vma = list_entry(pos, struct vm_area_struct, vkey_chain);
-
-		/* 
-		 * 1. No change the vma's vm_flags.pkey field. Just leave this to modified page fault handler.
-		 * 2. No change to the vma's vm_page_prot. Again, handler has to look the map to handle page fault.
-		 * 3. Change the pkey bits in the vkm-local and present PTEs.
-		 * 4. Flush local TLB entries of a range.
-		 */
-		flush_cache_range(vma, vma->vm_start, vma->vm_end);
-		inc_tlb_flush_pending(mm);
-		if (unlikely(mm_vkm_mod_page_range(vkm, vma, pkey, false))) {
-			printk(KERN_ERR "[%s] fatal error detected for vkm, please kill the process\n", __func__);
-			flush_tlb_mm(vma->vm_mm);
-			dec_tlb_flush_pending(mm);
-			return -EINVAL;
-		}
-		flush_tlb_vkm_range(vma, vkm);
-		dec_tlb_flush_pending(mm);
-	}
-	return 0;
-}
-
-struct vkey_map_struct *mm_vkm_alloc_init(struct mm_struct *mm)
-{
-	struct vkey_map_struct *vkm;
-	int i;
-	int is_free = atomic_cmpxchg(&mm->is_main_vkm_free, 1, 0);
-
-	vkm = allocate_vkey_vkm();
-	if (vkm) {
-		vkm->nr_thread = 0;
-		for (i = 0; i < arch_max_pkey() - 1; i++) {
-			vkm->pkey_nr_thread[i] = 0;
-			vkm->pkey_vkey[i] = ARCH_DEFAULT_VKEY;
-		}
-		arch_vkm_init(mm, vkm, is_free);
-		vkm_init_cpumask(vkm);	/* The main vkm's cpu_bitmap will never be in use. */
-		/* This caller is called by vkey_activate when mmap is locked. */
-		/* So we can use the vma area to duplicate all present PTEs */
-		if (vkm->pgd && !mm_vkm_copy_page_table(vkm, mm)) {
-			/* This is used when the vkm is not the same with main address space. */
-			/* The main space uses page_table_lock in mm_struct. */
-			spin_lock_init(&vkm->slock);
-		} else
-			free_vkey_vkm(vkm);
-	}
-
-	return vkm;
-}
-
-bool mm_vkm_can_add(struct vkey_map_struct *vkm, vpmap_t *current_vkeys, int len)
-{
-	int nr_vkm_vkey;
-	int nr_extra_vkey;
-	int i, j;
-
-	nr_extra_vkey = nr_vkm_vkey = 0;
-
-	for (i = 0; i < arch_max_pkey() - 1; i++)
-		if (vkm->pkey_vkey[i])
-			nr_vkm_vkey++;
-	
-	for (i = 0; i < MAX_ACTIVE_VKEYS; i++) {
-		if (!current_vkeys[i])
-			continue;
-		for (j = 0; j < arch_max_pkey() - 1; j++)
-			if (current_vkeys[i] == vkm->pkey_vkey[j])
-				break;
-		if (j == arch_max_pkey() - 1)
-			nr_extra_vkey++;
-	}
-
-	return (nr_vkm_vkey + nr_extra_vkey <= MAX_ACTIVE_VKEYS);
-}
-
-bool mm_vkm_is_in(struct vkey_map_struct *vkm, vpmap_t *current_vkeys, int len)
-{
-	int i, j;
-	for (i = 0; i < len; i++) {
-		if (!current_vkeys[i])
-			continue;
-		for (j = 0; j < arch_max_pkey() - 1; j++)
-			if (current_vkeys[i] == vkm->pkey_vkey[j])
-				break;
-		if (j == arch_max_pkey() - 1)
-			return false;
-	}
-	return true;
-}
-
-vpmap_t mm_vkm_add_vkey(struct vkey_map_struct *vkm, int vkey, int evicted, int xok, int perm)
-{
-	int i;
-	int first_evicted;
-	int orig_vkey;
-	vpmap_t ret;
-
-	first_evicted = 0;
-	for (i = (arch_max_pkey() - 1) - 1; i >= 0; i--) {
-		if (mm_vkm_idx_to_pkey(i) == xok)
-			continue;
-		if (vkm->pkey_vkey[i] == vkey) {
-			vkm->pkey_nr_thread[i]++;
-			mm_vkm_mprotect_present_ptes(vkm, current->mm, vkey, mm_vkm_idx_to_pkey(i));
-			mm_vkm_pkru_set_bits(mm_vkm_idx_to_pkey(i), (u32)perm);
-			ret = mm_vkm_idx_to_pkey(i);
-			goto out;
-		}
-		if (vkm->pkey_vkey[i] == evicted)
-			first_evicted = i;
-	}
-	orig_vkey = vkm->pkey_vkey[first_evicted];
-	vkm->pkey_vkey[first_evicted] = vkey;
-
-	/* Update the PTEs of the vkm. */
-	/* If the evicted vkey is originally 0, just update the present PTE, non-present are handled by later PFs. */
-	/* If the evicted vkey is not zero, set the live PTEs to xok, and update present PTEs. */
-	if (orig_vkey)
-		mm_vkm_mprotect_present_ptes(vkm, current->mm, orig_vkey, xok);
-	else
-		vkm->pkey_nr_thread[first_evicted] = 1;
-	mm_vkm_mprotect_present_ptes(vkm, current->mm, vkey, mm_vkm_idx_to_pkey(first_evicted));
-	mm_vkm_pkru_set_bits(mm_vkm_idx_to_pkey(first_evicted), (u32)perm);
-	ret = mm_vkm_idx_to_pkey(first_evicted);
-
-out:
-	return ret;
-}
-
-void mm_vkm_add(struct vkey_map_struct *vkm, struct mapped_vkey_struct *mvk, int len, int xok, int *current_perm)
-{
-	int i, j;
-	int new_vk;
-	vpmap_t *current_vkeys = mvk->map;
-	vpmap_t *current_pkeys = mvk->pmap;
-	
-	vkm->nr_thread++;
-
-	mm_vkm_pkru_reset(false);
-	for (i = 0; i < len; i++) {
-		new_vk = current_vkeys[i];
-		if (!new_vk)
-			continue;
-
-		/* The first pass tries if the new vkey is in the original mapping. */
-		for (j = 0; j < arch_max_pkey() - 1; j++)
-			if (vkm->pkey_vkey[j] == new_vk) {
-				vkm->pkey_nr_thread[j]++;
-				mm_vkm_mprotect_present_ptes(vkm, current->mm, new_vk, mm_vkm_idx_to_pkey(j));
-				break;
-			}
-
-		/* The new vkey has not been mapped. */
-		if (j == arch_max_pkey() - 1)
-			for (j = 0; j < arch_max_pkey() - 1; j++) {
-				if (xok == mm_vkm_idx_to_pkey(j))   /* eXecute-Only pkey should never be mapped */
-					continue;
-				if (!vkm->pkey_vkey[j]) {
-					vkm->pkey_vkey[j] = new_vk;
-					vkm->pkey_nr_thread[j] = 1;
-					mm_vkm_mprotect_present_ptes(vkm, current->mm, new_vk, mm_vkm_idx_to_pkey(j));
-					break;
-				}
-			}
-
-		/* Set the PKRU register. */
-		mm_vkm_pkru_set_bits(mm_vkm_idx_to_pkey(j), (u32)current_perm[i]);
-		current_pkeys[i] = mm_vkm_idx_to_pkey(j);
-
-		if (j == arch_max_pkey() - 1)
-			break;
-	}
-}
-
-void mm_vkm_del(struct vkey_map_struct *vkm, struct mm_struct *mm, vpmap_t *current_vkeys, int len, int *current_perm, int evicted)
-{
-	int new_vk;
-	int i, j;
-	vpmap_t *orig_map;
-	int *orig_nr_thread;
-	int original_perm;
-
-	orig_map = vkm->pkey_vkey;
-	orig_nr_thread = vkm->pkey_nr_thread;
-	vkm->nr_thread--;
-	
-	if (!current_perm)
-		mm_vkm_pkru_reset(true);
-
-	if (!vkm->nr_thread) {
-		for (j = 0; j < arch_max_pkey() - 1; j++) {
-			orig_nr_thread[j] = 0;
-			orig_map[j] = ARCH_DEFAULT_VKEY;
-		}
-	} else for (i = 0; i <= len; i++) {
-		if (i == len)
-			new_vk = evicted;
-		else
-			new_vk = current_vkeys[i];
-		if (!new_vk)
-			continue;
-		
-		/* Find all pkeys which maps to new_vk, del the pkey_nr_thread, if 0 -> map 0. */
-		for (j = 0; j < arch_max_pkey() - 1; j++)
-			if (orig_map[j] == new_vk) {
-				/* Here, memorize the perm in current_perm, and then disable */
-				/* No need to override */
-				original_perm = mm_vkm_pkru_get_bits(mm_vkm_idx_to_pkey(j));
-				if (current_perm && i < len) {
-					current_perm[i] = original_perm;
-				}
-				orig_nr_thread[j]--;
-				if (!orig_nr_thread[j]) {
-					orig_map[j] = ARCH_DEFAULT_VKEY;
-					/* No need to lock up if our pkru value is always right */
-					/*mm_vkm_mprotect_present_ptes(vkm, mm, new_vk, get_execute_only_pkey(mm));*/
-				}
-				break;
-			}
-	}
-}
-
-/*
- * Note: this doesn't free the actual pages themselves. That
- * has been handled earlier when unmapping all the memory regions.
- */
-static void mm_vkm_free_pte_range(struct mmu_gather *tlb, pmd_t *pmd,
-			   unsigned long addr)
-{
-	pgtable_t token = pmd_pgtable(*pmd);
-	pmd_clear(pmd);
-	pte_free_tlb(tlb, token, addr);
-	mm_dec_nr_ptes(tlb->mm);
-}
-
-static inline void mm_vkm_free_pmd_range(struct mmu_gather *tlb, pud_t *pud,
-				unsigned long addr, unsigned long end,
-				unsigned long floor, unsigned long ceiling)
-{
-	pmd_t *pmd;
-	unsigned long next;
-	unsigned long start;
-
-	start = addr;
-	pmd = pmd_offset(pud, addr);
-	do {
-		next = pmd_addr_end(addr, end);
-		if (pmd_none_or_clear_bad(pmd))
-			continue;
-		mm_vkm_free_pte_range(tlb, pmd, addr);
-	} while (pmd++, addr = next, addr != end);
-
-	start &= PUD_MASK;
-	if (start < floor)
-		return;
-	if (ceiling) {
-		ceiling &= PUD_MASK;
-		if (!ceiling)
-			return;
-	}
-	if (end - 1 > ceiling - 1)
-		return;
-
-	pmd = pmd_offset(pud, start);
-	pud_clear(pud);
-	pmd_free_tlb(tlb, pmd, start);
-	mm_dec_nr_pmds(tlb->mm);
-}
-
-static inline void mm_vkm_free_pud_range(struct mmu_gather *tlb, p4d_t *p4d,
-				unsigned long addr, unsigned long end,
-				unsigned long floor, unsigned long ceiling)
-{
-	pud_t *pud;
-	unsigned long next;
-	unsigned long start;
-
-	start = addr;
-	pud = pud_offset(p4d, addr);
-	do {
-		next = pud_addr_end(addr, end);
-		if (pud_none_or_clear_bad(pud))
-			continue;
-		mm_vkm_free_pmd_range(tlb, pud, addr, next, floor, ceiling);
-	} while (pud++, addr = next, addr != end);
-
-	start &= P4D_MASK;
-	if (start < floor)
-		return;
-	if (ceiling) {
-		ceiling &= P4D_MASK;
-		if (!ceiling)
-			return;
-	}
-	if (end - 1 > ceiling - 1)
-		return;
-
-	pud = pud_offset(p4d, start);
-	p4d_clear(p4d);
-	pud_free_tlb(tlb, pud, start);
-	mm_dec_nr_puds(tlb->mm);
-}
-
-static inline void mm_vkm_free_p4d_range(struct mmu_gather *tlb, pgd_t *pgd,
-				unsigned long addr, unsigned long end,
-				unsigned long floor, unsigned long ceiling)
-{
-	p4d_t *p4d;
-	unsigned long next;
-	unsigned long start;
-
-	start = addr;
-	p4d = p4d_offset(pgd, addr);
-	do {
-		next = p4d_addr_end(addr, end);
-		if (p4d_none_or_clear_bad(p4d))
-			continue;
-		mm_vkm_free_pud_range(tlb, p4d, addr, next, floor, ceiling);
-	} while (p4d++, addr = next, addr != end);
-
-	start &= PGDIR_MASK;
-	if (start < floor)
-		return;
-	if (ceiling) {
-		ceiling &= PGDIR_MASK;
-		if (!ceiling)
-			return;
-	}
-	if (end - 1 > ceiling - 1)
-		return;
-
-	p4d = p4d_offset(pgd, start);
-	pgd_clear(pgd);
-	p4d_free_tlb(tlb, p4d, start);
-}
-
-static void mm_vkm_free_pgd_range(struct mmu_gather *tlb, struct vkey_map_struct *vkm,
-			unsigned long addr, unsigned long end, unsigned long floor, unsigned long ceiling)
-{
-	pgd_t *pgd;
-	unsigned long next;
-
-	addr &= PMD_MASK;
-	if (addr < floor) {
-		addr += PMD_SIZE;
-		if (!addr)
-			return;
-	}
-	if (ceiling) {
-		ceiling &= PMD_MASK;
-		if (!ceiling)
-			return;
-	}
-	if (end - 1 > ceiling - 1)
-		end -= PMD_SIZE;
-	if (addr > end - 1)
-		return;
-
-	tlb_change_page_size(tlb, PAGE_SIZE);
-	pgd = vkm->pgd + pgd_index(addr);
-	do {
-		next = pgd_addr_end(addr, end);
-		if (pgd_none_or_clear_bad(pgd))
-			continue;
-		mm_vkm_free_p4d_range(tlb, pgd, addr, next, floor, ceiling);
-	} while (pgd++, addr = next, addr != end);
-}
-
-void mm_vkm_free_page_table(struct mmu_gather *tlb, struct vm_area_struct *vma,
-				struct vkey_map_struct *vkm, unsigned long floor, unsigned long ceiling)
-{
-	while (vma) {
-		struct vm_area_struct *next = vma->vm_next;
-		unsigned long addr = vma->vm_start;
-		if (is_vm_hugetlb_page(vma))
-			printk(KERN_ERR "[%s] vkey currently does not support huge TLB page...\n", __func__);
-		else
-			mm_vkm_free_pgd_range(tlb, vkm, addr, vma->vm_end, floor, next ? next->vm_start : ceiling);
-		vma = next;
-    }
-}
-
-static void mm_vkm_unmap(struct vkey_map_struct *vkm, struct mm_struct *mm)
-{
-	struct mmu_gather tlb;
-	struct vm_area_struct *vma = mm->mmap;
-	
-	/* If the address space is the same, then leave the delete of pgtable to exit_mmap(). */
-	if (vkm->pgd != mm->pgd) {
-		/* Free pgd and all related puds, pmds and ptes. */
-		tlb_gather_mmu(&tlb, mm);
-		tlb.vkm = vkm;
-		unmap_vmas(&tlb, vma, 0, -1);
-		mm_vkm_free_page_table(&tlb, vma, vkm, FIRST_USER_ADDRESS, USER_PGTABLES_CEILING);
-		tlb_finish_mmu(&tlb);
-		pgd_free(mm, vkm->pgd);
-	}
-}
-
-void __init vkey_map_caches_init(void)
-{	
-	vkey_vkm_cachep = kmem_cache_create("vkey_vkm_cache",
-			sizeof(struct vkey_map_struct) + cpumask_size(), 0,
-			SLAB_HWCACHE_ALIGN|SLAB_NOLEAKTRACE,
-			NULL);
-	if (!vkey_vkm_cachep)
-		printk(KERN_ERR "[%s] vkey vkm slab initialization failed...\n", __func__);
-	else
-		printk(KERN_INFO "[%s] vkey vkm slab initialized\n", __func__);
-
-	vkey_mvk_cachep = kmem_cache_create("vkey_mvk_cache",
-			sizeof(struct mapped_vkey_struct), 0,
-			SLAB_HWCACHE_ALIGN|SLAB_NOLEAKTRACE,
-			NULL);
-	if (!vkey_mvk_cachep)
-		printk(KERN_ERR "[%s] vkey mapped vk slab initialization failed...\n", __func__);
-	else
-		printk(KERN_INFO "[%s] vkey mapped vk slab initialized\n", __func__);
-
-	vkey_vkm_arr_cachep = kmem_cache_create("vkey_vkm_arr_cache",
-			sizeof(struct vkey_map_struct*) * MAX_ADDR_SPACE_PER_THREAD, 0,
-			SLAB_HWCACHE_ALIGN|SLAB_NOLEAKTRACE,
-			NULL);
-	if (!vkey_vkm_arr_cachep)
-		printk(KERN_ERR "[%s] vkey vkm arr slab initialization failed...\n", __func__);
-	else
-		printk(KERN_INFO "[%s] vkey vkm arr slab initialized\n", __func__);
-
-	vkey_mvk_arr_cachep = kmem_cache_create("vkey_mvk_arr_cache",
-			sizeof(struct mapped_vkey_struct*) * MAX_ADDR_SPACE_PER_THREAD, 0,
-			SLAB_HWCACHE_ALIGN|SLAB_NOLEAKTRACE,
-			NULL);
-	if (!vkey_mvk_arr_cachep)
-		printk(KERN_ERR "[%s] vkey mvk arr slab initialization failed...\n", __func__);
-	else
-		printk(KERN_INFO "[%s] vkey mvk arr slab initialized\n", __func__);
-
-#ifndef CONFIG_HAS_VPK_USER_VKRU
-	vkey_vkrk_cachep = kmem_cache_create("vkey_vkrk_cache",
-			sizeof(struct vkey_vkrk_struct), 0,
-			SLAB_HWCACHE_ALIGN|SLAB_NOLEAKTRACE,
-			NULL);
-	if (!vkey_vkrk_cachep)
-		printk(KERN_ERR "[%s] vkey vkrk slab initialization failed...\n", __func__);
-	else
-		printk(KERN_INFO "[%s] vkey vkrk slab initialized\n", __func__);
-#endif
-}
-
-void destroy_vkey_map(struct mm_struct *mm)
-{
-	struct list_head *pos;
-	struct list_head *n;
-	struct vkey_map_struct *vkm;
-
-	list_for_each_safe(pos, n, &mm->vkm_chain) {
-		vkm = list_entry(pos, struct vkey_map_struct, vkm_chain);
-		mm_vkm_unmap(vkm, mm);
-	}
-}
-
-void free_vkey_map(struct mm_struct *mm)
-{
-	struct list_head *pos;
-	struct list_head *n;
-	struct vkey_map_struct *vkm;
-	bool pr = !list_empty(&mm->vkm_chain);
-	
-	if (pr)
-		printk(KERN_INFO "Start free vkms from tsk %lx\n", (unsigned long)current);
-
-	list_for_each_safe(pos, n, &mm->vkm_chain) {
-		vkm = list_entry(pos, struct vkey_map_struct, vkm_chain);
-		list_del(&vkm->vkm_chain);
-		free_vkey_vkm(vkm);
-	}
-
-	if (pr)
-		printk(KERN_INFO "End free vkms from %lx\n", (unsigned long)current);
-}
-
-void walk_vkey_map(struct mm_struct *mm)
-{
-	struct list_head *pos;
-	struct vkey_map_struct *vkm;
-	int i;
-
-	list_for_each(pos, &mm->vkm_chain) {
-		vkm = list_entry(pos, struct vkey_map_struct, vkm_chain);
-		printk(KERN_INFO "[%s] vkey vkm %lx pgd %lx dumped with %d threads: ", __func__, (unsigned long)vkm, (unsigned long)vkm->pgd, vkm->nr_thread);
-		for (i = 0; i < arch_max_pkey() - 1; i++)
-			printk("v(%d)->p(%d)->t(%d)  ", vkm->pkey_vkey[i], mm_vkm_idx_to_pkey(i), vkm->pkey_nr_thread[i]);
-		printk("\n");
-	}
-}
-
-void walk_vkey_thread(struct task_struct *tsk)
-{
-#ifdef CONFIG_HAS_VPK_USER_VKRU
-	printk(KERN_INFO "[%s] thread %lx(%d) in is vkm %lx, vkru %lx:\n", __func__, (unsigned long)tsk, tsk->pid, (unsigned long)(tsk->vkm), (unsigned long)(tsk->vkru));
-	if (tsk->mapped_vkeys && tsk->vkru)
-	printk("v(%d)[%d] "
-	       "v(%d)[%d] "
-		   "v(%d)[%d] "
-		   "v(%d)[%d] "
-		   "v(%d)[%d] "
-		   "v(%d)[%d] "
-		   "v(%d)[%d] "
-		   "v(%d)[%d] "
-		   "v(%d)[%d] "
-		   "v(%d)[%d] "
-		   "v(%d)[%d] "
-		   "v(%d)[%d] "
-		   "v(%d)[%d] "
-		   "v(%d)[%d]",
-			tsk->mapped_vkeys->map[0],  vkey_get_vkru_permission(tsk, tsk->mapped_vkeys->map[0]),
-			tsk->mapped_vkeys->map[1],  vkey_get_vkru_permission(tsk, tsk->mapped_vkeys->map[1]),
-			tsk->mapped_vkeys->map[2],  vkey_get_vkru_permission(tsk, tsk->mapped_vkeys->map[2]),
-			tsk->mapped_vkeys->map[3],  vkey_get_vkru_permission(tsk, tsk->mapped_vkeys->map[3]),
-			tsk->mapped_vkeys->map[4],  vkey_get_vkru_permission(tsk, tsk->mapped_vkeys->map[4]),
-			tsk->mapped_vkeys->map[5],  vkey_get_vkru_permission(tsk, tsk->mapped_vkeys->map[5]),
-			tsk->mapped_vkeys->map[6],  vkey_get_vkru_permission(tsk, tsk->mapped_vkeys->map[6]),
-			tsk->mapped_vkeys->map[7],  vkey_get_vkru_permission(tsk, tsk->mapped_vkeys->map[7]),
-			tsk->mapped_vkeys->map[8],  vkey_get_vkru_permission(tsk, tsk->mapped_vkeys->map[8]),
-			tsk->mapped_vkeys->map[9],  vkey_get_vkru_permission(tsk, tsk->mapped_vkeys->map[9]),
-			tsk->mapped_vkeys->map[10], vkey_get_vkru_permission(tsk, tsk->mapped_vkeys->map[10]),
-			tsk->mapped_vkeys->map[11], vkey_get_vkru_permission(tsk, tsk->mapped_vkeys->map[11]),
-			tsk->mapped_vkeys->map[12], vkey_get_vkru_permission(tsk, tsk->mapped_vkeys->map[12]),
-			tsk->mapped_vkeys->map[13], vkey_get_vkru_permission(tsk, tsk->mapped_vkeys->map[13]));
-	printk("pkru register %x\n", rdpkru());
-#else
-	printk(KERN_INFO "[%s] thread %lx(%d) in is vkm %lx, vkrk %lx:\n", __func__, (unsigned long)tsk, tsk->pid, (unsigned long)(tsk->vkm), (unsigned long)(tsk->vkrk));
-	if (tsk->mapped_vkeys && tsk->vkrk)
-	printk("v(%d)[%d] "
-	       "v(%d)[%d] "
-		   "v(%d)[%d] "
-		   "v(%d)[%d] "
-		   "v(%d)[%d] "
-		   "v(%d)[%d] "
-		   "v(%d)[%d] "
-		   "v(%d)[%d] "
-		   "v(%d)[%d] "
-		   "v(%d)[%d] "
-		   "v(%d)[%d]",
-			tsk->mapped_vkeys->map[0],  vkey_get_vkrk_permission(tsk->vkrk, tsk->mapped_vkeys->map[0]),
-			tsk->mapped_vkeys->map[1],  vkey_get_vkrk_permission(tsk->vkrk, tsk->mapped_vkeys->map[1]),
-			tsk->mapped_vkeys->map[2],  vkey_get_vkrk_permission(tsk->vkrk, tsk->mapped_vkeys->map[2]),
-			tsk->mapped_vkeys->map[3],  vkey_get_vkrk_permission(tsk->vkrk, tsk->mapped_vkeys->map[3]),
-			tsk->mapped_vkeys->map[4],  vkey_get_vkrk_permission(tsk->vkrk, tsk->mapped_vkeys->map[4]),
-			tsk->mapped_vkeys->map[5],  vkey_get_vkrk_permission(tsk->vkrk, tsk->mapped_vkeys->map[5]),
-			tsk->mapped_vkeys->map[6],  vkey_get_vkrk_permission(tsk->vkrk, tsk->mapped_vkeys->map[6]),
-			tsk->mapped_vkeys->map[7],  vkey_get_vkrk_permission(tsk->vkrk, tsk->mapped_vkeys->map[7]),
-			tsk->mapped_vkeys->map[8],  vkey_get_vkrk_permission(tsk->vkrk, tsk->mapped_vkeys->map[8]),
-			tsk->mapped_vkeys->map[9],  vkey_get_vkrk_permission(tsk->vkrk, tsk->mapped_vkeys->map[9]),
-			tsk->mapped_vkeys->map[10], vkey_get_vkrk_permission(tsk->vkrk, tsk->mapped_vkeys->map[10]));
-	printk("dacr register %x\n", get_domain());
-#endif
-}
-
-void vkey_print_error_message(unsigned long address, int vkey)
-{
-	walk_vkey_thread(current);
-
-	/* print the vkey map */
-	walk_vkey_map(current->mm);
-
-#ifdef CONFIG_64BIT
-	if (address)
-		printk("[%s] The fault PTE of vkey %d of %lx is %lx in the vkm\n", __func__, vkey, address, pte_val(*pte_offset_map(pmd_offset(pud_offset(
-				p4d_offset((current->vkm ? (current->vkm->pgd + pgd_index(address)) : pgd_offset_pgd(current->mm->pgd, (address))), address), address), address), address)));
-#else
-	if (address)
-		printk("[%s] The fault PTE of vkey %d of %lx is %x in the vkm\n", __func__, vkey, address, pte_val(*pte_offset_map(pmd_offset(pud_offset(
-				p4d_offset((current->vkm ? (current->vkm->pgd + pgd_index(address)) : pgd_offset_pgd(current->mm->pgd, (address))), address), address), address), address)));
-#endif
-}
-
-inline struct mapped_vkey_struct *tsk_mvk_alloc(void)
-{
-	struct mapped_vkey_struct *mvk;
-	int i;
-	mvk = allocate_vkey_mvk();
-	if (mvk) {
-		for (i = 0; i < MAX_ACTIVE_VKEYS; i++) {
-			mvk->map[i] = ARCH_DEFAULT_VKEY;
-			mvk->pmap[i] = 0;
-			mvk->ts[i] = i;
-		}
-	}
-	return mvk;
-}
-
-inline struct vkey_vkrk_struct *tsk_vkrk_alloc(void)
-{
-#ifdef CONFIG_HAS_VPK_USER_VKRU
-	return NULL;
-#else
-	int i;
-	struct vkey_vkrk_struct *vkrk = allocate_vkey_vkrk();
-	if (!vkrk)
-		return NULL;
-	for (i = 0; i < arch_max_vkey(); i++) {
-		int vkey = i + 1;
-		int idx = 2 * vkey / sizeof(unsigned long);
-		int oft = vkey % sizeof(unsigned long);
-		vkrk->bm[idx] &= (~(3UL << (oft << 1U)));
-		vkrk->bm[idx] |= ((unsigned long)VKEY_ND << (oft << 1U));
-	}
-	return vkrk;
-#endif
-}
-
-inline void tsk_mvk_free(struct mapped_vkey_struct *mvk)
-{
-	free_vkey_mvk(mvk);
-}
-
-inline void tsk_vkrk_free(struct vkey_vkrk_struct *vkrk)
-{
-	free_vkey_vkrk(vkrk);
-}
-
-inline struct mapped_vkey_struct **tsk_mvk_arr_alloc(void)
-{
-	struct mapped_vkey_struct **arr = allocate_vkey_mvk_arr();
-	int i;
-	if (arr)
-		for (i = 0; i < MAX_ADDR_SPACE_PER_THREAD; i++)
-			arr[i] = NULL;
-	return arr;
-}
-
-inline struct vkey_map_struct **tsk_vkm_arr_alloc(void)
-{
-	struct vkey_map_struct **arr = allocate_vkey_vkm_arr();
-	int i;
-	if (arr)
-		for (i = 0; i < MAX_ADDR_SPACE_PER_THREAD; i++)
-			arr[i] = NULL;
-	return arr;
-}
-
-inline void tsk_mvk_arr_free(struct mapped_vkey_struct **mvk)
-{
-	free_vkey_mvk_arr(mvk);
-}
-
-inline void tsk_vkm_arr_free(struct vkey_map_struct **vkm)
-{
-	free_vkey_vkm_arr(vkm);
-}
-
-#else
-
-void vkey_map_caches_init(void)
-{
-
-}
-
-void destroy_vkey_map(struct mm_struct *mm)
-{
-
-}
-
-void free_vkey_map(struct mm_struct *mm)
-{
-
-}
-
-#endif
diff --git a/./kernel/vkeys.c b/./kernel/vkeys.c
deleted file mode 100644
index 73ae99e5e..000000000
--- a/./kernel/vkeys.c
+++ /dev/null
@@ -1,224 +0,0 @@
-#include <linux/slab.h>
-#include <linux/vkeys.h>
-#include <linux/log2.h>
-#include <linux/vmalloc.h>
-#include <linux/mmu_notifier.h>
-#include <asm/tlb.h>
-#include <asm/ptrace.h>
-
-#ifdef CONFIG_HAS_VPK
-
-static struct kmem_cache *vkey_kgd_cachep;
-static struct kmem_cache *vkey_kte_cachep;
-struct vkey_per_cpu_cl *vktramp;
-spinlock_t vklock;
-
-#define allocate_vkey_kgd()		(kmem_cache_alloc(vkey_kgd_cachep, GFP_KERNEL))
-#define free_vkey_kgd(kgd)		(kmem_cache_free(vkey_kgd_cachep, (kgd)))
-#define allocate_vkey_kte()		(kmem_cache_alloc(vkey_kte_cachep, GFP_KERNEL))
-#define free_vkey_kte(kte)		(kmem_cache_free(vkey_kte_cachep, (kte)))
-#define task_stack_page(task)	((void *)(task)->stack)
-
-static inline void init_valid_kgd(struct vkey_kgd_struct *kgd)
-{
-	int i;
-	for (i = 0; i < VKEY_KGD_ENTRIES; i++)
-		kgd->ktes[i] = NULL;
-}
-
-static inline void init_valid_kte(struct vkey_kte_struct *kte)
-{
-	int i;
-	for (i = 0; i < VKEY_KTE_ENTRIES; i++)
-		INIT_LIST_HEAD(&kte->vkey_vma_heads[i]);
-}
-
-int mm_vkey_alloc(struct mm_struct *mm)
-{
-	int ret;
-	int kgd_oft;
-
-	ret = find_first_zero_bit(mm->vkey.vkey_alloc_bm, arch_max_vkey());
-	if (ret == arch_max_vkey())
-		return -1;
-
-	if (unlikely(!mm->vkey.kgd)) {
-		mm->vkey.kgd = allocate_vkey_kgd();
-		if (!mm->vkey.kgd)
-			return -ENOMEM;
-		init_valid_kgd(mm->vkey.kgd);
-	}
-	
-	kgd_oft = vkey_kgd_offset(ret);
-	if (!(mm->vkey.kgd->ktes[kgd_oft])) {
-		mm->vkey.kgd->ktes[kgd_oft] = allocate_vkey_kte();
-		if (!(mm->vkey.kgd->ktes[kgd_oft]))
-			return -ENOMEM;
-		init_valid_kte(mm->vkey.kgd->ktes[kgd_oft]);
-	}
-
-	set_bit(ret, mm->vkey.vkey_alloc_bm);
-	return ret;
-}
-
-int mm_vkey_free(struct mm_struct *mm, int vkey)
-{
-	/* vkey deafult should never be freed */
-	if (!mm_vkey_is_allocated(mm, vkey) || vkey == ARCH_DEFAULT_VKEY)
-		return -EINVAL;
-
-	/* Here, should we go through every vm_area_struct(s) assigned the vkey
-	 * to be freed in the current process (mm argument), and set their
-	 * vkey to be -1?
-	 */
-	clear_bit(vkey, mm->vkey.vkey_alloc_bm);
-
-	return 0;
-}
-
-void destroy_vkey(struct mm_struct *mm)
-{
-	struct vkey_kte_struct *kte;
-	int i, j;
-
-	/* free all kgd and kte structure, after the broken vma chain, hopefully no UAF */
-	if (mm->vkey.kgd) {
-		for (i = 0; i < VKEY_KGD_ENTRIES; i++) {
-			kte = mm->vkey.kgd->ktes[i];
-			if (kte) {
-				for (j = 0; j < VKEY_KTE_ENTRIES; j++)
-					list_del_init(&kte->vkey_vma_heads[j]);
-				free_vkey_kte(kte);
-			}
-		}
-		free_vkey_kgd(mm->vkey.kgd);
-	}
-}
-
-void __init vkey_caches_init(void)
-{	
-	vkey_kgd_cachep = kmem_cache_create("vkey_kgd_cache",
-			sizeof(struct vkey_kgd_struct), 0,
-			SLAB_HWCACHE_ALIGN|SLAB_NOLEAKTRACE,
-			NULL);
-	if (!vkey_kgd_cachep)
-		printk(KERN_ERR "[%s] vkey kgd slab initialization failed...\n", __func__);
-	else
-		printk(KERN_INFO "[%s] vkey kgd slab initialized\n", __func__);
-
-	vkey_kte_cachep = kmem_cache_create("vkey_kte_cache",
-			sizeof(struct vkey_kte_struct), 0,
-			SLAB_HWCACHE_ALIGN|SLAB_NOLEAKTRACE,
-			NULL);
-	if (!vkey_kte_cachep)
-		printk(KERN_ERR "[%s] vkey kte slab initialization failed...\n", __func__);
-	else
-		printk(KERN_INFO "[%s] vkey kte slab initialized\n", __func__);
-
-	vktramp = kzalloc((cpumask_size() * sizeof(struct vkey_per_cpu_cl) + 
-						PAGE_SIZE - 1) / PAGE_SIZE * PAGE_SIZE, GFP_KERNEL);
-	if (!vktramp)
-		printk(KERN_ERR "[%s] vkey trampoline initialization failed...\n", __func__);
-	else
-		printk(KERN_INFO "[%s] vkey trampoline initialized\n", __func__);
-}
-
-void walk_vkey_chain(struct mm_struct *mm, int vkey)
-{
-	struct list_head *pos;
-	struct vm_area_struct *vma;
-	list_for_each(pos, &(mm->vkey.kgd->ktes[vkey_kgd_offset(vkey)]->vkey_vma_heads[vkey_kte_offset(vkey)])) {
-		vma = list_entry(pos, struct vm_area_struct, vkey_chain);
-		printk(KERN_INFO "[%s] [start, end) = [%lx, %lx)\n", __func__, vma->vm_start, vma->vm_end);
-	}
-}
-
-#ifdef CONFIG_HAS_VPK_USER_VKRU
-int lvkru_mmap_lock(struct vm_area_struct *vma, unsigned long laddr_base)
-{
-	void *lvkru_kbase;
-	struct page *pages[VKEY_META_DATA_SZ / PAGE_SIZE];
-
-	if (vma->vm_end - vma->vm_start != VKEY_META_DATA_SZ) {
-		printk(KERN_ERR "[%s] the allocated anonymous memory is not proper...\n", __func__);
-		return -EINVAL;
-	}
-
-	if (get_user_pages(laddr_base, VKEY_META_DATA_SZ / PAGE_SIZE, 0,
-			pages, NULL) != VKEY_META_DATA_SZ / PAGE_SIZE) {
-		printk(KERN_ERR "[%s] failed to get user pages...\n", __func__);
-		return -EINVAL;
-	}
-
-	lvkru_kbase = vmap(pages, VKEY_META_DATA_SZ / PAGE_SIZE, VM_MAP, PAGE_KERNEL);
-	vma->vm_mm->lvkru_kaddr = lvkru_kbase;
-
-	if (lvkru_kbase)
-		return 0;
-	printk(KERN_ERR "[%s] failed to vmap the user pages...\n", __func__);
-	return -EINVAL;
-}
-
-/* Map this to user as read only, then lock against mprotect, mremap, etc. */
-int vktramp_mmap_lock(struct vm_area_struct *vma)
-{
-	struct page *page;
-	struct mmu_notifier_range range;
-	struct mmu_gather tlb;
-
-	if (vma->vm_end - vma->vm_start != (cpumask_size() * sizeof(struct vkey_per_cpu_cl) + 
-						PAGE_SIZE - 1) / PAGE_SIZE * PAGE_SIZE) {
-		printk(KERN_ERR "[%s] the allocated anonymous memory is not proper...\n", __func__);
-		return -EINVAL;
-	}
-
-	page = virt_to_page((unsigned long)vktramp);
-	if (page) {
-		int ret;
-		lru_add_drain();
-		mmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, vma, vma->vm_mm,
-				vma->vm_start, vma->vm_end);
-		tlb_gather_mmu(&tlb, vma->vm_mm);
-		update_hiwater_rss(vma->vm_mm);
-		mmu_notifier_invalidate_range_start(&range);
-		ret = remap_pfn_range(vma, vma->vm_start, page_to_pfn(page), vma->vm_end - vma->vm_start, vma->vm_page_prot);
-		mmu_notifier_invalidate_range_end(&range);
-		tlb_finish_mmu(&tlb);
-		return ret;
-	}
-	return -EINVAL;
-}
-
-inline void vkey_thread_check_migrate(struct task_struct *tsk)
-{
-	// FIXME: [VDom] assign in_user_critical_section!!! And this func should be called as early as possible!!!
-	// may be as soon as "->__state, ", what about migrate_disable(); and migrate_enable();?
-	const unsigned long ulib_start = 0x0, ulib_end = 0x1000;
-	if (tsk->vkru) {
-		struct pt_regs *regs = task_pt_regs(tsk);
-		struct vm_area_struct *vma = tsk->mm ? tsk->mm->lvkey_code_vma : NULL;
-		if (vma) {
-			bool in_user_critical_section = (regs->ip >= vma->vm_start + ulib_start && regs->ip < vma->vm_start + ulib_end);
-			if (unlikely(in_user_critical_section)) {
-				if (tsk->vkey_can_load_balance)
-					tsk->vkey_can_load_balance = false;
-			} else if (unlikely(!tsk->vkey_can_load_balance))
-				tsk->vkey_can_load_balance = true;
-		}
-	}
-}
-#endif
-
-#else
-
-void destroy_vkey(struct mm_struct *mm)
-{
-
-}
-
-void __init vkey_caches_init(void)
-{
-
-}
-
-#endif
diff --git a/./mm/Kconfig b/../../../../linux/mm/Kconfig
index 54a8aa89e..3326ee390 100644
--- a/./mm/Kconfig
+++ b/../../../../linux/mm/Kconfig
@@ -804,8 +804,6 @@ config ARCH_USES_HIGH_VMA_FLAGS
 	bool
 config ARCH_HAS_PKEYS
 	bool
-config HAS_VPK_USER_VKRU
-	bool
 
 config PERCPU_STATS
 	bool "Collect percpu memory statistics"
diff --git a/./mm/debug_vm_pgtable.c b/../../../../linux/mm/debug_vm_pgtable.c
index eb37d588e..db2abd9e4 100644
--- a/./mm/debug_vm_pgtable.c
+++ b/../../../../linux/mm/debug_vm_pgtable.c
@@ -29,7 +29,6 @@
 #include <linux/start_kernel.h>
 #include <linux/sched/mm.h>
 #include <linux/io.h>
-#include <linux/vkeys.h>
 
 #include <asm/cacheflush.h>
 #include <asm/pgalloc.h>
@@ -681,9 +680,6 @@ static void __init pmd_populate_tests(struct pgtable_debug_args *args)
 	 * This entry points to next level page table page.
 	 * Hence this must not qualify as pmd_bad().
 	 */
-#ifdef CONFIG_HAS_VPK
-	if (!vkm_pmd_populate(args->mm, args->pmdp, args->start_ptep, -1))
-#endif
 	pmd_populate(args->mm, args->pmdp, args->start_ptep);
 	pmd = READ_ONCE(*args->pmdp);
 	WARN_ON(pmd_bad(pmd));
diff --git a/./mm/huge_memory.c b/../../../../linux/mm/huge_memory.c
index 32c200a51..406a3c28c 100644
--- a/./mm/huge_memory.c
+++ b/../../../../linux/mm/huge_memory.c
@@ -34,8 +34,6 @@
 #include <linux/oom.h>
 #include <linux/numa.h>
 #include <linux/page_owner.h>
-#include <linux/vkey_map.h>
-#include <linux/vkeys.h>
 
 #include <asm/tlb.h>
 #include <asm/pgalloc.h>
@@ -1942,9 +1940,6 @@ static void __split_huge_zero_page_pmd(struct vm_area_struct *vma,
 	pmdp_huge_clear_flush(vma, haddr, pmd);
 
 	pgtable = pgtable_trans_huge_withdraw(mm, pmd);
-#ifdef CONFIG_HAS_VPK
-	if (!vkm_pmd_populate(mm, &_pmd, pgtable, -1))
-#endif
 	pmd_populate(mm, &_pmd, pgtable);
 
 	for (i = 0; i < HPAGE_PMD_NR; i++, haddr += PAGE_SIZE) {
@@ -1957,9 +1952,6 @@ static void __split_huge_zero_page_pmd(struct vm_area_struct *vma,
 		pte_unmap(pte);
 	}
 	smp_wmb(); /* make pte visible before pmd */
-#ifdef CONFIG_HAS_VPK
-	if (!vkm_pmd_populate(mm, pmd, pgtable, -1))
-#endif
 	pmd_populate(mm, pmd, pgtable);
 }
 
@@ -2072,9 +2064,6 @@ static void __split_huge_pmd_locked(struct vm_area_struct *vma, pmd_t *pmd,
 	 * This's critical for some architectures (Power).
 	 */
 	pgtable = pgtable_trans_huge_withdraw(mm, pmd);
-#ifdef CONFIG_HAS_VPK
-	if (!vkm_pmd_populate(mm, &_pmd, pgtable, -1))
-#endif
 	pmd_populate(mm, &_pmd, pgtable);
 
 	for (i = 0, addr = haddr; i < HPAGE_PMD_NR; i++, addr += PAGE_SIZE) {
@@ -2143,9 +2132,6 @@ static void __split_huge_pmd_locked(struct vm_area_struct *vma, pmd_t *pmd,
 	}
 
 	smp_wmb(); /* make pte visible before pmd */
-#ifdef CONFIG_HAS_VPK
-	if (!vkm_pmd_populate(mm, pmd, pgtable, -1))
-#endif
 	pmd_populate(mm, pmd, pgtable);
 
 	if (freeze) {
diff --git a/./mm/init-mm.c b/../../../../linux/mm/init-mm.c
index 3fecfbfb8..b4a6f38fb 100644
--- a/./mm/init-mm.c
+++ b/../../../../linux/mm/init-mm.c
@@ -36,18 +36,6 @@ struct mm_struct init_mm = {
 	.page_table_lock =  __SPIN_LOCK_UNLOCKED(init_mm.page_table_lock),
 	.arg_lock	=  __SPIN_LOCK_UNLOCKED(init_mm.arg_lock),
 	.mmlist		= LIST_HEAD_INIT(init_mm.mmlist),
-#ifdef CONFIG_HAS_VPK
-	.vkm_chain	= LIST_HEAD_INIT(init_mm.vkm_chain),
-	.main_vkm	= NULL,
-#ifdef CONFIG_HAS_VPK_USER_VKRU
-	.lvkru_kaddr	= 0,
-	.lvkru_uaddr	= 0,
-	.vktramp_uaddr	= 0,
-	.lvkey_code_vma	= NULL,
-#endif
-	.vkey_lock	= __RW_LOCK_UNLOCKED(init_mm.vkey_lock),
-	.is_main_vkm_free = ATOMIC_INIT(1),
-#endif
 	.user_ns	= &init_user_ns,
 	.cpu_bitmap	= CPU_BITS_NONE,
 	INIT_MM_CONTEXT(init_mm)
diff --git a/./mm/internal.h b/../../../../linux/mm/internal.h
index 96f4bc30a..d80300392 100644
--- a/./mm/internal.h
+++ b/../../../../linux/mm/internal.h
@@ -70,7 +70,6 @@ bool __folio_end_writeback(struct folio *folio);
 void free_pgtables(struct mmu_gather *tlb, struct vm_area_struct *start_vma,
 		unsigned long floor, unsigned long ceiling);
 void pmd_install(struct mm_struct *mm, pmd_t *pmd, pgtable_t *pte);
-void pmd_install_ignore_vpk(struct mm_struct *mm, pmd_t *pmd, pgtable_t *pte);
 
 static inline bool can_madv_lru_vma(struct vm_area_struct *vma)
 {
diff --git a/./mm/khugepaged.c b/../../../../linux/mm/khugepaged.c
index a3aee0a8f..131492fd1 100644
--- a/./mm/khugepaged.c
+++ b/../../../../linux/mm/khugepaged.c
@@ -19,7 +19,6 @@
 #include <linux/page_table_check.h>
 #include <linux/swapops.h>
 #include <linux/shmem_fs.h>
-#include <linux/vkeys.h>
 
 #include <asm/tlb.h>
 #include <asm/pgalloc.h>
@@ -1165,9 +1164,6 @@ static void collapse_huge_page(struct mm_struct *mm,
 		 * hugepmds and never for establishing regular pmds that
 		 * points to regular pagetables. Use pmd_populate for that
 		 */
-#ifdef CONFIG_HAS_VPK
-		if (!vkm_pmd_populate(mm, pmd, pmd_pgtable(_pmd), -1))
-#endif
 		pmd_populate(mm, pmd, pmd_pgtable(_pmd));
 		spin_unlock(pmd_ptl);
 		anon_vma_unlock_write(vma->anon_vma);
diff --git a/./mm/madvise.c b/../../../../linux/mm/madvise.c
index 242601284..38d0f515d 100644
--- a/./mm/madvise.c
+++ b/../../../../linux/mm/madvise.c
@@ -31,7 +31,6 @@
 #include <linux/swapops.h>
 #include <linux/shmem_fs.h>
 #include <linux/mmu_notifier.h>
-#include <linux/vkeys.h>
 
 #include <asm/tlb.h>
 
@@ -143,13 +142,6 @@ static int madvise_update_vma(struct vm_area_struct *vma,
 	struct mm_struct *mm = vma->vm_mm;
 	int error;
 	pgoff_t pgoff;
-	unsigned long vm_vkey;
-
-#ifdef CONFIG_HAS_VPK
-	vm_vkey = mm_mprotect_vkey(vma, -1);
-#else
-	vm_vkey = 0;
-#endif
 
 	if (new_flags == vma->vm_flags && anon_vma_name_eq(anon_vma_name(vma), anon_name)) {
 		*prev = vma;
@@ -157,8 +149,7 @@ static int madvise_update_vma(struct vm_area_struct *vma,
 	}
 
 	pgoff = vma->vm_pgoff + ((start - vma->vm_start) >> PAGE_SHIFT);
-	*prev = vma_merge(mm, *prev, start, end, new_flags,
-			  vm_vkey, vma->anon_vma,
+	*prev = vma_merge(mm, *prev, start, end, new_flags, vma->anon_vma,
 			  vma->vm_file, pgoff, vma_policy(vma),
 			  vma->vm_userfaultfd_ctx, anon_name);
 	if (*prev) {
diff --git a/./mm/memory.c b/../../../../linux/mm/memory.c
index 7b22ebaf1..c125c4969 100644
--- a/./mm/memory.c
+++ b/../../../../linux/mm/memory.c
@@ -74,8 +74,6 @@
 #include <linux/perf_event.h>
 #include <linux/ptrace.h>
 #include <linux/vmalloc.h>
-#include <linux/vkey_map.h>
-#include <linux/vkeys.h>
 
 #include <trace/events/kmem.h>
 
@@ -440,35 +438,6 @@ void pmd_install(struct mm_struct *mm, pmd_t *pmd, pgtable_t *pte)
 {
 	spinlock_t *ptl = pmd_lock(mm, pmd);
 
-	if (likely(pmd_none(*pmd))) {	/* Has another populated it ? */
-		mm_inc_nr_ptes(mm);
-		/*
-		 * Ensure all pte setup (eg. pte page lock and page clearing) are
-		 * visible before the pte is made visible to other CPUs by being
-		 * put into page tables.
-		 *
-		 * The other side of the story is the pointer chasing in the page
-		 * table walking code (when walking the page table without locking;
-		 * ie. most of the time). Fortunately, these data accesses consist
-		 * of a chain of data-dependent loads, meaning most CPUs (alpha
-		 * being the notable exception) will already guarantee loads are
-		 * seen in-order. See the alpha page table accessors for the
-		 * smp_rmb() barriers in page table walking code.
-		 */
-		smp_wmb(); /* Could be smp_wmb__xxx(before|after)_spin_lock */
-#ifdef CONFIG_HAS_VPK
-		if (!vkm_pmd_populate(mm, pmd, *pte, -1))
-#endif
-		pmd_populate(mm, pmd, *pte);
-		*pte = NULL;
-	}
-	spin_unlock(ptl);
-}
-
-void pmd_install_ignore_vpk(struct mm_struct *mm, pmd_t *pmd, pgtable_t *pte)
-{
-	spinlock_t *ptl = pmd_lock(mm, pmd);
-
 	if (likely(pmd_none(*pmd))) {	/* Has another populated it ? */
 		mm_inc_nr_ptes(mm);
 		/*
@@ -503,18 +472,6 @@ int __pte_alloc(struct mm_struct *mm, pmd_t *pmd)
 	return 0;
 }
 
-int __pte_alloc_ignore_vpk(struct mm_struct *mm, pmd_t *pmd)
-{
-	pgtable_t new = pte_alloc_one(mm);
-	if (!new)
-		return -ENOMEM;
-
-	pmd_install_ignore_vpk(mm, pmd, &new);
-	if (new)
-		pte_free(mm, new);
-	return 0;
-}
-
 int __pte_alloc_kernel(pmd_t *pmd)
 {
 	pte_t *new = pte_alloc_one_kernel(&init_mm);
@@ -1440,6 +1397,7 @@ again:
 				continue;
 			pte_clear_not_present_full(mm, addr, pte, tlb->fullmm);
 			rss[mm_counter(page)]--;
+
 			if (is_device_private_entry(entry))
 				page_remove_rmap(page, false);
 
@@ -1468,9 +1426,6 @@ again:
 	arch_leave_lazy_mmu_mode();
 
 	/* Do the actual TLB flush before dropping ptl */
-#ifdef CONFIG_HAS_VPK
-	if (!tlb->vkm)
-#endif
 	if (force_flush)
 		tlb_flush_mmu_tlbonly(tlb);
 	pte_unmap_unlock(start_pte, ptl);
@@ -1569,7 +1524,7 @@ next:
 	return addr;
 }
 
-unsigned long zap_p4d_range(struct mmu_gather *tlb,
+static inline unsigned long zap_p4d_range(struct mmu_gather *tlb,
 				struct vm_area_struct *vma, pgd_t *pgd,
 				unsigned long addr, unsigned long end,
 				struct zap_details *details)
@@ -1598,11 +1553,6 @@ void unmap_page_range(struct mmu_gather *tlb,
 
 	BUG_ON(addr >= end);
 	tlb_start_vma(tlb, vma);
-#ifdef CONFIG_HAS_VPK
-	if (tlb->vkm)
-		pgd = tlb->vkm->pgd + pgd_index(addr);
-	else
-#endif
 	pgd = pgd_offset(vma->vm_mm, addr);
 	do {
 		next = pgd_addr_end(addr, end);
@@ -1709,24 +1659,8 @@ void zap_page_range(struct vm_area_struct *vma, unsigned long start,
 	tlb_gather_mmu(&tlb, vma->vm_mm);
 	update_hiwater_rss(vma->vm_mm);
 	mmu_notifier_invalidate_range_start(&range);
-	for ( ; vma && vma->vm_start < range.end; vma = vma->vm_next) {
-#ifdef CONFIG_HAS_VPK
-		struct mm_struct *mm = vma->vm_mm;
-		struct list_head *pos;
-		struct vkey_map_struct *vkm;
-		if (!list_empty(&mm->vkm_chain)) {
-			list_for_each(pos, &mm->vkm_chain) {
-				vkm = list_entry(pos, struct vkey_map_struct, vkm_chain);
-				if (vkm->pgd == mm->pgd)
-					continue;
-				tlb.vkm = vkm;
-				unmap_single_vma(&tlb, vma, start, range.end, NULL);
-			}
-		}
-		tlb.vkm = NULL;
-#endif
+	for ( ; vma && vma->vm_start < range.end; vma = vma->vm_next)
 		unmap_single_vma(&tlb, vma, start, range.end, NULL);
-	}
 	mmu_notifier_invalidate_range_end(&range);
 	tlb_finish_mmu(&tlb);
 }
@@ -1745,11 +1679,6 @@ static void zap_page_range_single(struct vm_area_struct *vma, unsigned long addr
 {
 	struct mmu_notifier_range range;
 	struct mmu_gather tlb;
-#ifdef CONFIG_HAS_VPK
-	struct mm_struct *mm = vma->vm_mm;
-	struct list_head *pos;
-	struct vkey_map_struct *vkm;
-#endif
 
 	lru_add_drain();
 	mmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, vma, vma->vm_mm,
@@ -1757,18 +1686,6 @@ static void zap_page_range_single(struct vm_area_struct *vma, unsigned long addr
 	tlb_gather_mmu(&tlb, vma->vm_mm);
 	update_hiwater_rss(vma->vm_mm);
 	mmu_notifier_invalidate_range_start(&range);
-#ifdef CONFIG_HAS_VPK
-	if (!list_empty(&mm->vkm_chain)) {
-		list_for_each(pos, &mm->vkm_chain) {
-			vkm = list_entry(pos, struct vkey_map_struct, vkm_chain);
-			if (vkm->pgd == mm->pgd)
-				continue;
-			tlb.vkm = vkm;
-			unmap_single_vma(&tlb, vma, address, range.end, details);
-		}
-	}
-	tlb.vkm = NULL;
-#endif
 	unmap_single_vma(&tlb, vma, address, range.end, details);
 	mmu_notifier_invalidate_range_end(&range);
 	tlb_finish_mmu(&tlb);
@@ -2368,7 +2285,7 @@ EXPORT_SYMBOL(vmf_insert_mixed_mkwrite);
  */
 static int remap_pte_range(struct mm_struct *mm, pmd_t *pmd,
 			unsigned long addr, unsigned long end,
-			unsigned long pfn, pgprot_t prot, struct vm_area_struct *vma)
+			unsigned long pfn, pgprot_t prot)
 {
 	pte_t *pte, *mapped_pte;
 	spinlock_t *ptl;
@@ -2394,7 +2311,7 @@ static int remap_pte_range(struct mm_struct *mm, pmd_t *pmd,
 
 static inline int remap_pmd_range(struct mm_struct *mm, pud_t *pud,
 			unsigned long addr, unsigned long end,
-			unsigned long pfn, pgprot_t prot, struct vm_area_struct *vma)
+			unsigned long pfn, pgprot_t prot)
 {
 	pmd_t *pmd;
 	unsigned long next;
@@ -2408,7 +2325,7 @@ static inline int remap_pmd_range(struct mm_struct *mm, pud_t *pud,
 	do {
 		next = pmd_addr_end(addr, end);
 		err = remap_pte_range(mm, pmd, addr, next,
-				pfn + (addr >> PAGE_SHIFT), prot, vma);
+				pfn + (addr >> PAGE_SHIFT), prot);
 		if (err)
 			return err;
 	} while (pmd++, addr = next, addr != end);
@@ -2417,7 +2334,7 @@ static inline int remap_pmd_range(struct mm_struct *mm, pud_t *pud,
 
 static inline int remap_pud_range(struct mm_struct *mm, p4d_t *p4d,
 			unsigned long addr, unsigned long end,
-			unsigned long pfn, pgprot_t prot, struct vm_area_struct *vma)
+			unsigned long pfn, pgprot_t prot)
 {
 	pud_t *pud;
 	unsigned long next;
@@ -2430,7 +2347,7 @@ static inline int remap_pud_range(struct mm_struct *mm, p4d_t *p4d,
 	do {
 		next = pud_addr_end(addr, end);
 		err = remap_pmd_range(mm, pud, addr, next,
-				pfn + (addr >> PAGE_SHIFT), prot, vma);
+				pfn + (addr >> PAGE_SHIFT), prot);
 		if (err)
 			return err;
 	} while (pud++, addr = next, addr != end);
@@ -2439,7 +2356,7 @@ static inline int remap_pud_range(struct mm_struct *mm, p4d_t *p4d,
 
 static inline int remap_p4d_range(struct mm_struct *mm, pgd_t *pgd,
 			unsigned long addr, unsigned long end,
-			unsigned long pfn, pgprot_t prot, struct vm_area_struct *vma)
+			unsigned long pfn, pgprot_t prot)
 {
 	p4d_t *p4d;
 	unsigned long next;
@@ -2452,7 +2369,7 @@ static inline int remap_p4d_range(struct mm_struct *mm, pgd_t *pgd,
 	do {
 		next = p4d_addr_end(addr, end);
 		err = remap_pud_range(mm, p4d, addr, next,
-				pfn + (addr >> PAGE_SHIFT), prot, vma);
+				pfn + (addr >> PAGE_SHIFT), prot);
 		if (err)
 			return err;
 	} while (p4d++, addr = next, addr != end);
@@ -2508,7 +2425,7 @@ int remap_pfn_range_notrack(struct vm_area_struct *vma, unsigned long addr,
 	do {
 		next = pgd_addr_end(addr, end);
 		err = remap_p4d_range(mm, pgd, addr, next,
-				pfn + (addr >> PAGE_SHIFT), prot, vma);
+				pfn + (addr >> PAGE_SHIFT), prot);
 		if (err)
 			return err;
 	} while (pgd++, addr = next, addr != end);
@@ -4714,32 +4631,7 @@ static vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,
 	pgd_t *pgd;
 	p4d_t *p4d;
 	vm_fault_t ret;
-#ifdef CONFIG_HAS_VPK
-	struct vkey_map_struct *vkm = current->vkm;
-	struct vkey_map_struct *main_vkm = mm->main_vkm;
-	int vkey = mm_mprotect_vkey(vma, -1);
-	int target_pkey = -1;
-	int iter;
-	bool origin_pmd_none = false;
-
-	if (vkey) {
-		if (main_vkm) {
-			target_pkey = get_execute_only_pkey(current->mm);
-			spin_lock(&main_vkm->slock);
-			for (iter = 0; iter < arch_max_pkey() - 1; iter++)
-				if (main_vkm->pkey_vkey[iter] == vkey) {
-					target_pkey = mm_vkm_idx_to_pkey(iter);
-					break;
-				}
-			vma->vm_flags &= (~((unsigned long)arch_calc_vm_prot_bits(0, 15)));
-			vma->vm_flags |= (unsigned long)arch_calc_vm_prot_bits(0, target_pkey);
-			vma_set_page_prot(vma);
-			spin_unlock(&main_vkm->slock);
-		}
-	}
-#endif
 
-	/* The main vkm should check the pkey-vkey map if vkey is not zero. */
 	pgd = pgd_offset(mm, address);
 	p4d = p4d_alloc(mm, pgd, address);
 	if (!p4d)
@@ -4750,10 +4642,6 @@ static vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,
 		return VM_FAULT_OOM;
 retry_pud:
 	if (pud_none(*vmf.pud) && __transparent_hugepage_enabled(vma)) {
-#ifdef CONFIG_HAS_VPK
-		if (vkey)
-			printk(KERN_INFO "[%s] vkey meets huge pud, try prot anon, if file xok\n", __func__);
-#endif
 		ret = create_huge_pud(&vmf);
 		if (!(ret & VM_FAULT_FALLBACK))
 			return ret;
@@ -4764,10 +4652,7 @@ retry_pud:
 		if (pud_trans_huge(orig_pud) || pud_devmap(orig_pud)) {
 
 			/* NUMA case for anonymous PUDs would go here */
-#ifdef CONFIG_HAS_VPK
-			if (vkey)
-				printk(KERN_ERR "[%s] vkey meets huge pud, try prot anon, if file xok...", __func__);
-#endif
+
 			if (dirty && !pud_write(orig_pud)) {
 				ret = wp_huge_pud(&vmf, orig_pud);
 				if (!(ret & VM_FAULT_FALLBACK))
@@ -4787,16 +4672,7 @@ retry_pud:
 	if (pud_trans_unstable(vmf.pud))
 		goto retry_pud;
 
-#ifdef CONFIG_HAS_VPK
-	if (pmd_none(*vmf.pmd) || pmd_bad(*vmf.pmd))
-		origin_pmd_none = true;
-#endif
-
 	if (pmd_none(*vmf.pmd) && __transparent_hugepage_enabled(vma)) {
-#ifdef CONFIG_HAS_VPK
-		if (vkey)
-			printk(KERN_INFO "[%s] vkey meets huge pmd, try prot anon, if file xok...", __func__);
-#endif
 		ret = create_huge_pmd(&vmf);
 		if (!(ret & VM_FAULT_FALLBACK))
 			return ret;
@@ -4812,10 +4688,6 @@ retry_pud:
 			return 0;
 		}
 		if (pmd_trans_huge(vmf.orig_pmd) || pmd_devmap(vmf.orig_pmd)) {
-#ifdef CONFIG_HAS_VPK
-			if (vkey)
-				printk(KERN_INFO "[%s] vkey meets huge pmd, try prot anon, if file xok...", __func__);
-#endif
 			if (pmd_protnone(vmf.orig_pmd) && vma_is_accessible(vma))
 				return do_huge_pmd_numa_page(&vmf);
 
@@ -4830,47 +4702,7 @@ retry_pud:
 		}
 	}
 
-#ifdef CONFIG_HAS_VPK
-	ret = handle_pte_fault(&vmf);
-	if (unlikely(origin_pmd_none && vkey && main_vkm && !ret)) {
-		pmd_t *fault_pmd;
-		fault_pmd = pmd_offset(pud_offset(
-				p4d_offset(pgd_offset(mm, address), address), address), address);
-		vkm_pmd_populate(mm, fault_pmd, pmd_pgtable(*fault_pmd), target_pkey);
-	}
-	if (vkm && !ret) {
-		if (vkm->pgd != mm->pgd) {
-			target_pkey = -1;
-			if (vkey) {
-				target_pkey = get_execute_only_pkey(current->mm);
-				spin_lock(&vkm->slock);
-				for (iter = 0; iter < arch_max_pkey() - 1; iter++)
-					if (vkm->pkey_vkey[iter] == vkey) {
-						target_pkey = mm_vkm_idx_to_pkey(iter);
-						break;
-					}
-				spin_unlock(&vkm->slock);
-			}
-			ret = mm_vkm_mod_p4d_range(vma, vkm->pgd + pgd_index(address),
-							pgd, address, address, target_pkey, true, true);
-		} else if (mm_vkm_is_reserved_pk_fault(flags)) {	/* Intel only */
-			flush_cache_range(vma, address & PAGE_MASK, (address & PAGE_MASK) + PAGE_SIZE);
-			inc_tlb_flush_pending(mm);
-			if (unlikely(mm_vkm_mod_p4d_range(vma, pgd, NULL, 
-					address, address, target_pkey, false, true))) {
-				printk(KERN_ERR "[%s] fatal error detected for vkm, please kill the process\n", __func__);
-				flush_tlb_mm(vma->vm_mm);
-				dec_tlb_flush_pending(mm);
-				return VM_FAULT_OOM;
-			}
-			flush_tlb_vkm_page(address, vkm);
-			dec_tlb_flush_pending(mm);
-		}
-	}
-	return ret;
-#else
 	return handle_pte_fault(&vmf);
-#endif
 }
 
 /**
diff --git a/./mm/mempolicy.c b/../../../../linux/mm/mempolicy.c
index 628b5de86..69284d3b5 100644
--- a/./mm/mempolicy.c
+++ b/../../../../linux/mm/mempolicy.c
@@ -793,15 +793,9 @@ static int mbind_range(struct mm_struct *mm, unsigned long start,
 	pgoff_t pgoff;
 	unsigned long vmstart;
 	unsigned long vmend;
-	unsigned long vm_vkey;
 
 	vma = find_vma(mm, start);
 	VM_BUG_ON(!vma);
-#ifdef CONFIG_HAS_VPK
-	vm_vkey = mm_mprotect_vkey(vma, -1);
-#else
-	vm_vkey = 0;
-#endif
 
 	prev = vma->vm_prev;
 	if (start > vma->vm_start)
@@ -818,7 +812,7 @@ static int mbind_range(struct mm_struct *mm, unsigned long start,
 		pgoff = vma->vm_pgoff +
 			((vmstart - vma->vm_start) >> PAGE_SHIFT);
 		prev = vma_merge(mm, prev, vmstart, vmend, vma->vm_flags,
-				 vm_vkey, vma->anon_vma, vma->vm_file, pgoff,
+				 vma->anon_vma, vma->vm_file, pgoff,
 				 new_pol, vma->vm_userfaultfd_ctx,
 				 anon_vma_name(vma));
 		if (prev) {
diff --git a/./mm/mlock.c b/../../../../linux/mm/mlock.c
index 11e5471f3..25934e7db 100644
--- a/./mm/mlock.c
+++ b/../../../../linux/mm/mlock.c
@@ -24,7 +24,6 @@
 #include <linux/memcontrol.h>
 #include <linux/mm_inline.h>
 #include <linux/secretmem.h>
-#include <linux/vkeys.h>
 
 #include "internal.h"
 
@@ -503,13 +502,7 @@ static int mlock_fixup(struct vm_area_struct *vma, struct vm_area_struct **prev,
 	int ret = 0;
 	int lock = !!(newflags & VM_LOCKED);
 	vm_flags_t old_flags = vma->vm_flags;
-	unsigned long vm_vkey;
 
-#ifdef CONFIG_HAS_VPK
-	vm_vkey = mm_mprotect_vkey(vma, -1);
-#else
-	vm_vkey = 0;
-#endif
 	if (newflags == vma->vm_flags || (vma->vm_flags & VM_SPECIAL) ||
 	    is_vm_hugetlb_page(vma) || vma == get_gate_vma(current->mm) ||
 	    vma_is_dax(vma) || vma_is_secretmem(vma))
@@ -517,7 +510,7 @@ static int mlock_fixup(struct vm_area_struct *vma, struct vm_area_struct **prev,
 		goto out;
 
 	pgoff = vma->vm_pgoff + ((start - vma->vm_start) >> PAGE_SHIFT);
-	*prev = vma_merge(mm, *prev, start, end, newflags, vm_vkey, vma->anon_vma,
+	*prev = vma_merge(mm, *prev, start, end, newflags, vma->anon_vma,
 			  vma->vm_file, pgoff, vma_policy(vma),
 			  vma->vm_userfaultfd_ctx, anon_vma_name(vma));
 	if (*prev) {
diff --git a/./mm/mmap.c b/../../../../linux/mm/mmap.c
index 5e8f9571b..f61a15474 100644
--- a/./mm/mmap.c
+++ b/../../../../linux/mm/mmap.c
@@ -48,9 +48,6 @@
 #include <linux/pkeys.h>
 #include <linux/oom.h>
 #include <linux/sched/mm.h>
-#include <linux/vkey_map.h>
-#include <linux/vkeys.h>
-#include <linux/vmalloc.h>
 
 #include <linux/uaccess.h>
 #include <asm/cacheflush.h>
@@ -1033,7 +1030,6 @@ again:
  */
 static inline int is_mergeable_vma(struct vm_area_struct *vma,
 				struct file *file, unsigned long vm_flags,
-				unsigned long vm_vkey,
 				struct vm_userfaultfd_ctx vm_userfaultfd_ctx,
 				struct anon_vma_name *anon_name)
 {
@@ -1045,10 +1041,6 @@ static inline int is_mergeable_vma(struct vm_area_struct *vma,
 	 * the kernel to generate new VMAs when old one could be
 	 * extended instead.
 	 */
-#ifdef CONFIG_HAS_VPK
-	if (mm_mprotect_vkey(vma, -1) != vm_vkey)
-		return 0;
-#endif
 	if ((vma->vm_flags ^ vm_flags) & ~VM_SOFTDIRTY)
 		return 0;
 	if (vma->vm_file != file)
@@ -1089,13 +1081,12 @@ static inline int is_mergeable_anon_vma(struct anon_vma *anon_vma1,
  */
 static int
 can_vma_merge_before(struct vm_area_struct *vma, unsigned long vm_flags,
-			 unsigned long vm_vkey,
 		     struct anon_vma *anon_vma, struct file *file,
 		     pgoff_t vm_pgoff,
 		     struct vm_userfaultfd_ctx vm_userfaultfd_ctx,
 		     struct anon_vma_name *anon_name)
 {
-	if (is_mergeable_vma(vma, file, vm_flags, vm_vkey, vm_userfaultfd_ctx, anon_name) &&
+	if (is_mergeable_vma(vma, file, vm_flags, vm_userfaultfd_ctx, anon_name) &&
 	    is_mergeable_anon_vma(anon_vma, vma->anon_vma, vma)) {
 		if (vma->vm_pgoff == vm_pgoff)
 			return 1;
@@ -1112,13 +1103,12 @@ can_vma_merge_before(struct vm_area_struct *vma, unsigned long vm_flags,
  */
 static int
 can_vma_merge_after(struct vm_area_struct *vma, unsigned long vm_flags,
-		    unsigned long vm_vkey,
 		    struct anon_vma *anon_vma, struct file *file,
 		    pgoff_t vm_pgoff,
 		    struct vm_userfaultfd_ctx vm_userfaultfd_ctx,
 		    struct anon_vma_name *anon_name)
 {
-	if (is_mergeable_vma(vma, file, vm_flags, vm_vkey, vm_userfaultfd_ctx, anon_name) &&
+	if (is_mergeable_vma(vma, file, vm_flags, vm_userfaultfd_ctx, anon_name) &&
 	    is_mergeable_anon_vma(anon_vma, vma->anon_vma, vma)) {
 		pgoff_t vm_pglen;
 		vm_pglen = vma_pages(vma);
@@ -1174,7 +1164,6 @@ can_vma_merge_after(struct vm_area_struct *vma, unsigned long vm_flags,
 struct vm_area_struct *vma_merge(struct mm_struct *mm,
 			struct vm_area_struct *prev, unsigned long addr,
 			unsigned long end, unsigned long vm_flags,
-			unsigned long vm_vkey,
 			struct anon_vma *anon_vma, struct file *file,
 			pgoff_t pgoff, struct mempolicy *policy,
 			struct vm_userfaultfd_ctx vm_userfaultfd_ctx,
@@ -1206,7 +1195,7 @@ struct vm_area_struct *vma_merge(struct mm_struct *mm,
 	 */
 	if (prev && prev->vm_end == addr &&
 			mpol_equal(vma_policy(prev), policy) &&
-			can_vma_merge_after(prev, vm_flags, vm_vkey,
+			can_vma_merge_after(prev, vm_flags,
 					    anon_vma, file, pgoff,
 					    vm_userfaultfd_ctx, anon_name)) {
 		/*
@@ -1214,7 +1203,7 @@ struct vm_area_struct *vma_merge(struct mm_struct *mm,
 		 */
 		if (next && end == next->vm_start &&
 				mpol_equal(policy, vma_policy(next)) &&
-				can_vma_merge_before(next, vm_flags, vm_vkey,
+				can_vma_merge_before(next, vm_flags,
 						     anon_vma, file,
 						     pgoff+pglen,
 						     vm_userfaultfd_ctx, anon_name) &&
@@ -1238,7 +1227,7 @@ struct vm_area_struct *vma_merge(struct mm_struct *mm,
 	 */
 	if (next && end == next->vm_start &&
 			mpol_equal(policy, vma_policy(next)) &&
-			can_vma_merge_before(next, vm_flags, vm_vkey,
+			can_vma_merge_before(next, vm_flags,
 					     anon_vma, file, pgoff+pglen,
 					     vm_userfaultfd_ctx, anon_name)) {
 		if (prev && addr < prev->vm_end)	/* case 4 */
@@ -1771,7 +1760,7 @@ unsigned long mmap_region(struct file *file, unsigned long addr,
 	/*
 	 * Can we just expand an old mapping?
 	 */
-	vma = vma_merge(mm, prev, addr, addr + len, vm_flags, 0,
+	vma = vma_merge(mm, prev, addr, addr + len, vm_flags,
 			NULL, file, pgoff, NULL, NULL_VM_UFFD_CTX, NULL);
 	if (vma)
 		goto out;
@@ -1789,7 +1778,7 @@ unsigned long mmap_region(struct file *file, unsigned long addr,
 
 	vma->vm_start = addr;
 	vma->vm_end = addr + len;
-	vma->vm_flags = vm_flags;	/* By static analysis, only mmap will call this, whose vkey must be 0. */
+	vma->vm_flags = vm_flags;
 	vma->vm_page_prot = vm_get_page_prot(vm_flags);
 	vma->vm_pgoff = pgoff;
 
@@ -1820,7 +1809,7 @@ unsigned long mmap_region(struct file *file, unsigned long addr,
 		 * as we may succeed this time.
 		 */
 		if (unlikely(vm_flags != vma->vm_flags && prev)) {
-			merge = vma_merge(mm, prev, vma->vm_start, vma->vm_end, vma->vm_flags, 0,
+			merge = vma_merge(mm, prev, vma->vm_start, vma->vm_end, vma->vm_flags,
 				NULL, vma->vm_file, vma->vm_pgoff, NULL, NULL_VM_UFFD_CTX, NULL);
 			if (merge) {
 				/* ->mmap() can change vma->vm_file and fput the original file. So
@@ -2664,21 +2653,6 @@ static void unmap_region(struct mm_struct *mm,
 	lru_add_drain();
 	tlb_gather_mmu(&tlb, mm);
 	update_hiwater_rss(mm);
-#ifdef CONFIG_HAS_VPK
-	if (!list_empty(&mm->vkm_chain)) {
-		struct list_head *pos;
-		list_for_each(pos, &mm->vkm_chain) {
-			struct vkey_map_struct *vkm = list_entry(pos, struct vkey_map_struct, vkm_chain);
-			if (vkm->pgd == mm->pgd)
-				continue;
-			tlb.vkm = vkm;
-			unmap_vmas(&tlb, vma, start, end);
-			mm_vkm_free_page_table(&tlb, vma, vkm, prev ? prev->vm_end : FIRST_USER_ADDRESS,
-				 next ? next->vm_start : USER_PGTABLES_CEILING);
-		}
-	}
-	tlb.vkm = NULL;
-#endif
 	unmap_vmas(&tlb, vma, start, end);
 	free_pgtables(&tlb, vma, prev ? prev->vm_end : FIRST_USER_ADDRESS,
 				 next ? next->vm_start : USER_PGTABLES_CEILING);
@@ -2743,7 +2717,7 @@ int __split_vma(struct mm_struct *mm, struct vm_area_struct *vma,
 			return err;
 	}
 
-	new = vm_area_dup(vma, true);
+	new = vm_area_dup(vma);
 	if (!new)
 		return -ENOMEM;
 
@@ -2910,15 +2884,6 @@ int __do_munmap(struct mm_struct *mm, unsigned long start, size_t len,
 	if (!detach_vmas_to_be_unmapped(mm, vma, prev, end))
 		downgrade = false;
 
-#ifdef CONFIG_HAS_VPK
-	unmap_region(mm, vma, prev, start, end);
-
-	/* Fix up all other VM information */
-	remove_vma_list(mm, vma);
-
-	if (downgrade)
-		mmap_write_downgrade(mm);
-#else
 	if (downgrade)
 		mmap_write_downgrade(mm);
 
@@ -2926,7 +2891,6 @@ int __do_munmap(struct mm_struct *mm, unsigned long start, size_t len,
 
 	/* Fix up all other VM information */
 	remove_vma_list(mm, vma);
-#endif
 
 	return downgrade ? 1 : 0;
 }
@@ -3097,7 +3061,7 @@ static int do_brk_flags(unsigned long addr, unsigned long len, unsigned long fla
 		return -ENOMEM;
 
 	/* Can we just expand an old private anonymous mapping? */
-	vma = vma_merge(mm, prev, addr, addr + len, flags, 0,
+	vma = vma_merge(mm, prev, addr, addr + len, flags,
 			NULL, NULL, pgoff, NULL, NULL_VM_UFFD_CTX, NULL);
 	if (vma)
 		goto out;
@@ -3115,7 +3079,7 @@ static int do_brk_flags(unsigned long addr, unsigned long len, unsigned long fla
 	vma->vm_start = addr;
 	vma->vm_end = addr + len;
 	vma->vm_pgoff = pgoff;
-	vma->vm_flags = flags;	/* The new flag is eith 0 or VM_EXEC by static analysis. */
+	vma->vm_flags = flags;
 	vma->vm_page_prot = vm_get_page_prot(flags);
 	vma_link(mm, vma, prev, rb_link, rb_parent);
 out:
@@ -3209,12 +3173,6 @@ void exit_mmap(struct mm_struct *mm)
 	lru_add_drain();
 	flush_cache_mm(mm);
 	tlb_gather_mmu_fullmm(&tlb, mm);
-#ifdef CONFIG_HAS_VPK
-	/* Free the vkey map first to avoid double-free of pgd */
-	destroy_vkey(mm);
-	destroy_vkey_map(mm);
-	tlb.vkm = NULL;
-#endif
 	/* update_hiwater_rss(mm) here? but nobody should be looking */
 	/* Use -1 here to ensure all VMAs in the mm are unmapped */
 	unmap_vmas(&tlb, vma, 0, -1);
@@ -3229,14 +3187,6 @@ void exit_mmap(struct mm_struct *mm)
 		cond_resched();
 	}
 	mm->mmap = NULL;
-#ifdef CONFIG_HAS_VPK
-#ifdef CONFIG_HAS_VPK_USER_VKRU
-	if (mm->lvkru_kaddr) {
-		vunmap((void *)(mm->lvkru_kaddr));
-		mm->lvkru_kaddr = 0;
-	}
-#endif
-#endif
 	mmap_write_unlock(mm);
 	vm_unacct_memory(nr_accounted);
 }
@@ -3292,13 +3242,7 @@ struct vm_area_struct *copy_vma(struct vm_area_struct **vmap,
 	struct vm_area_struct *new_vma, *prev;
 	struct rb_node **rb_link, *rb_parent;
 	bool faulted_in_anon_vma = true;
-	unsigned long vm_vkey;
 
-#ifdef CONFIG_HAS_VPK
-	vm_vkey = mm_mprotect_vkey(vma, -1);
-#else
-	vm_vkey = 0;
-#endif
 	/*
 	 * If anonymous vma has not yet been faulted, update new pgoff
 	 * to match new location, to increase its chance of merging.
@@ -3310,7 +3254,7 @@ struct vm_area_struct *copy_vma(struct vm_area_struct **vmap,
 
 	if (find_vma_links(mm, addr, addr + len, &prev, &rb_link, &rb_parent))
 		return NULL;	/* should never get here */
-	new_vma = vma_merge(mm, prev, addr, addr + len, vma->vm_flags, vm_vkey,
+	new_vma = vma_merge(mm, prev, addr, addr + len, vma->vm_flags,
 			    vma->anon_vma, vma->vm_file, pgoff, vma_policy(vma),
 			    vma->vm_userfaultfd_ctx, anon_vma_name(vma));
 	if (new_vma) {
@@ -3336,7 +3280,7 @@ struct vm_area_struct *copy_vma(struct vm_area_struct **vmap,
 		}
 		*need_rmap_locks = (new_vma->vm_pgoff <= vma->vm_pgoff);
 	} else {
-		new_vma = vm_area_dup(vma, true);
+		new_vma = vm_area_dup(vma);
 		if (!new_vma)
 			goto out;
 		new_vma->vm_start = addr;
@@ -3496,7 +3440,6 @@ static struct vm_area_struct *__install_special_mapping(
 	int ret;
 	struct vm_area_struct *vma;
 
-	/* Never installs vkey related mapping by static analysis, no need to take care of vma->vkey_chain. */
 	vma = vm_area_alloc(mm);
 	if (unlikely(vma == NULL))
 		return ERR_PTR(-ENOMEM);
diff --git a/./mm/mmu_gather.c b/../../../../linux/mm/mmu_gather.c
index dd297ac96..afb7185ff 100644
--- a/./mm/mmu_gather.c
+++ b/../../../../linux/mm/mmu_gather.c
@@ -8,7 +8,6 @@
 #include <linux/rcupdate.h>
 #include <linux/smp.h>
 #include <linux/swap.h>
-#include <linux/vkey_map.h>
 
 #include <asm/pgalloc.h>
 #include <asm/tlb.h>
@@ -257,10 +256,6 @@ static void __tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm,
 	tlb->mm = mm;
 	tlb->fullmm = fullmm;
 
-#ifdef CONFIG_HAS_VPK
-	tlb->vkm = current->vkm;
-#endif
-
 #ifndef CONFIG_MMU_GATHER_NO_GATHER
 	tlb->need_flush_all = 0;
 	tlb->local.next = NULL;
diff --git a/./mm/mprotect.c b/../../../../linux/mm/mprotect.c
index e4d0a83f1..2887644fd 100644
--- a/./mm/mprotect.c
+++ b/../../../../linux/mm/mprotect.c
@@ -29,20 +29,15 @@
 #include <linux/uaccess.h>
 #include <linux/mm_inline.h>
 #include <linux/pgtable.h>
-#include <linux/vkeys.h>
-#include <linux/vkey_map.h>
 #include <asm/cacheflush.h>
 #include <asm/mmu_context.h>
 #include <asm/tlbflush.h>
-#include <asm/vkeys.h>
 
 #include "internal.h"
 
-extern bool try_to_free_pmd_page(pmd_t *pmd);
-
 static unsigned long change_pte_range(struct vm_area_struct *vma, pmd_t *pmd,
 		unsigned long addr, unsigned long end, pgprot_t newprot,
-		unsigned long cp_flags, bool is_vkm)
+		unsigned long cp_flags)
 {
 	pte_t *pte, oldpte;
 	spinlock_t *ptl;
@@ -226,7 +221,7 @@ static inline int pmd_none_or_clear_bad_unless_trans_huge(pmd_t *pmd)
 
 static inline unsigned long change_pmd_range(struct vm_area_struct *vma,
 		pud_t *pud, unsigned long addr, unsigned long end,
-		pgprot_t newprot, unsigned long cp_flags, bool is_vkm)
+		pgprot_t newprot, unsigned long cp_flags)
 {
 	pmd_t *pmd;
 	unsigned long next;
@@ -255,33 +250,16 @@ static inline unsigned long change_pmd_range(struct vm_area_struct *vma,
 			goto next;
 
 		/* invoke the mmu notifier if the pmd is populated */
-		if (!is_vkm) {	/* The subscription callback should do the operation to all vkms. */
-			if (!range.start) {
-				mmu_notifier_range_init(&range,
-					MMU_NOTIFY_PROTECTION_VMA, 0,
-					vma, vma->vm_mm, addr, end);
-				mmu_notifier_invalidate_range_start(&range);
-			}
+		if (!range.start) {
+			mmu_notifier_range_init(&range,
+				MMU_NOTIFY_PROTECTION_VMA, 0,
+				vma, vma->vm_mm, addr, end);
+			mmu_notifier_invalidate_range_start(&range);
 		}
 
-#ifdef CONFIG_HAS_VPK
-		/* Here, we can use the value of main_vkm because the xo-vkey is always mapped to xo-pkey. */
-		if (mm_mprotect_vkey(vma, -1))
-			vkm_pmd_populate(vma->vm_mm, pmd, pmd_pgtable(*pmd), -1);
-#endif
-
 		if (is_swap_pmd(*pmd) || pmd_trans_huge(*pmd) || pmd_devmap(*pmd)) {
 			if (next - addr != HPAGE_PMD_SIZE) {
-				if (is_vkm)	{	/* Just invalidate the pmd for later page fault */
-					spinlock_t *ptl;
-					bool ret;
-					ptl = pmd_lock(vma->vm_mm, pmd);
-					ret = try_to_free_pmd_page(pmd);
-					spin_unlock(ptl);
-					if (!ret)
-						printk(KERN_ERR "[%s] BUG: failed to free pmd...\n", __func__);
-				} else
-					__split_huge_pmd(vma, pmd, addr, false, NULL);
+				__split_huge_pmd(vma, pmd, addr, false, NULL);
 			} else {
 				int nr_ptes = change_huge_pmd(vma, pmd, addr,
 							      newprot, cp_flags);
@@ -299,14 +277,13 @@ static inline unsigned long change_pmd_range(struct vm_area_struct *vma,
 			/* fall through, the trans huge pmd just split */
 		}
 		this_pages = change_pte_range(vma, pmd, addr, next, newprot,
-					      cp_flags, is_vkm);
+					      cp_flags);
 		pages += this_pages;
 next:
-		if (!is_vkm)
-			cond_resched();
+		cond_resched();
 	} while (pmd++, addr = next, addr != end);
 
-	if (!is_vkm && range.start)
+	if (range.start)
 		mmu_notifier_invalidate_range_end(&range);
 
 	if (nr_huge_updates)
@@ -316,7 +293,7 @@ next:
 
 static inline unsigned long change_pud_range(struct vm_area_struct *vma,
 		p4d_t *p4d, unsigned long addr, unsigned long end,
-		pgprot_t newprot, unsigned long cp_flags, bool is_vkm)
+		pgprot_t newprot, unsigned long cp_flags)
 {
 	pud_t *pud;
 	unsigned long next;
@@ -328,7 +305,7 @@ static inline unsigned long change_pud_range(struct vm_area_struct *vma,
 		if (pud_none_or_clear_bad(pud))
 			continue;
 		pages += change_pmd_range(vma, pud, addr, next, newprot,
-					  cp_flags, is_vkm);
+					  cp_flags);
 	} while (pud++, addr = next, addr != end);
 
 	return pages;
@@ -336,7 +313,7 @@ static inline unsigned long change_pud_range(struct vm_area_struct *vma,
 
 static inline unsigned long change_p4d_range(struct vm_area_struct *vma,
 		pgd_t *pgd, unsigned long addr, unsigned long end,
-		pgprot_t newprot, unsigned long cp_flags, bool is_vkm)
+		pgprot_t newprot, unsigned long cp_flags)
 {
 	p4d_t *p4d;
 	unsigned long next;
@@ -348,7 +325,7 @@ static inline unsigned long change_p4d_range(struct vm_area_struct *vma,
 		if (p4d_none_or_clear_bad(p4d))
 			continue;
 		pages += change_pud_range(vma, p4d, addr, next, newprot,
-					  cp_flags, is_vkm);
+					  cp_flags);
 	} while (p4d++, addr = next, addr != end);
 
 	return pages;
@@ -363,49 +340,22 @@ static unsigned long change_protection_range(struct vm_area_struct *vma,
 	unsigned long next;
 	unsigned long start = addr;
 	unsigned long pages = 0;
-#ifdef CONFIG_HAS_VPK
-	struct list_head *pos;
-	struct vkey_map_struct *vkm;
-	unsigned long vkm_addr;
-#endif
 
 	BUG_ON(addr >= end);
-	flush_cache_range(vma, addr, end);	/* x86 does nothing */
-	inc_tlb_flush_pending(mm);
-#ifdef CONFIG_HAS_VPK
-	/* All PTEs must be present in the main vkm space, so no need to count pages. */
-	/* if (!list_empty(&mm->vkm_chain))
-		printk("[%s] begin changing PTEs of vkms [%lx, %lx)].\n", __func__, addr, end); */
-	list_for_each(pos, &mm->vkm_chain) {
-		vkm = list_entry(pos, struct vkey_map_struct, vkm_chain);
-		if (vkm->pgd == mm->pgd)
-			continue;
-		vkm_addr = addr;
-		pgd = vkm->pgd + pgd_index(vkm_addr);
-		do {
-			next = pgd_addr_end(vkm_addr, end);
-			if (pgd_none_or_clear_bad(pgd))
-				continue;
-			change_p4d_range(vma, pgd, vkm_addr, next, newprot,
-					  cp_flags, true);
-		} while (pgd++, vkm_addr = next, vkm_addr != end);
-		/* printk("[%s] a new vkm's PTEs are changed.\n", __func__); */
-	}
-	/* if (!list_empty(&mm->vkm_chain))
-		printk("[%s] end changing PTEs of vkms.\n", __func__); */
-#endif
 	pgd = pgd_offset(mm, addr);
+	flush_cache_range(vma, addr, end);
+	inc_tlb_flush_pending(mm);
 	do {
 		next = pgd_addr_end(addr, end);
 		if (pgd_none_or_clear_bad(pgd))
 			continue;
 		pages += change_p4d_range(vma, pgd, addr, next, newprot,
-					  cp_flags, false);
+					  cp_flags);
 	} while (pgd++, addr = next, addr != end);
 
 	/* Only flush the TLB if we actually modified any entries: */
 	if (pages)
-		flush_tlb_range(vma, start, end);	/* flush tlb ranges of all vkms */
+		flush_tlb_range(vma, start, end);
 	dec_tlb_flush_pending(mm);
 
 	return pages;
@@ -457,7 +407,7 @@ static const struct mm_walk_ops prot_none_walk_ops = {
 
 int
 mprotect_fixup(struct vm_area_struct *vma, struct vm_area_struct **pprev,
-	unsigned long start, unsigned long end, unsigned long newflags, unsigned long newvkey)
+	unsigned long start, unsigned long end, unsigned long newflags)
 {
 	struct mm_struct *mm = vma->vm_mm;
 	unsigned long oldflags = vma->vm_flags;
@@ -466,12 +416,8 @@ mprotect_fixup(struct vm_area_struct *vma, struct vm_area_struct **pprev,
 	pgoff_t pgoff;
 	int error;
 	int dirty_accountable = 0;
-	int oldvkey = 0;
-#ifdef CONFIG_HAS_VPK
-	oldvkey = mm_mprotect_vkey(vma, -1);
-#endif
 
-	if (newflags == oldflags && newvkey == oldvkey) {
+	if (newflags == oldflags) {
 		*pprev = vma;
 		return 0;
 	}
@@ -486,7 +432,6 @@ mprotect_fixup(struct vm_area_struct *vma, struct vm_area_struct **pprev,
 	    (newflags & VM_ACCESS_FLAGS) == 0) {
 		pgprot_t new_pgprot = vm_get_page_prot(newflags);
 
-		/* This has a real impact on PTEs, so vkeys does not affect new_pgprot. */
 		error = walk_page_range(current->mm, start, end,
 				&prot_none_walk_ops, &new_pgprot);
 		if (error)
@@ -517,7 +462,7 @@ mprotect_fixup(struct vm_area_struct *vma, struct vm_area_struct **pprev,
 	 * First try to merge with previous and/or next vma.
 	 */
 	pgoff = vma->vm_pgoff + ((start - vma->vm_start) >> PAGE_SHIFT);
-	*pprev = vma_merge(mm, *pprev, start, end, newflags, newvkey,
+	*pprev = vma_merge(mm, *pprev, start, end, newflags,
 			   vma->anon_vma, vma->vm_file, pgoff, vma_policy(vma),
 			   vma->vm_userfaultfd_ctx, anon_vma_name(vma));
 	if (*pprev) {
@@ -545,31 +490,13 @@ success:
 	 * vm_flags and vm_page_prot are protected by the mmap_lock
 	 * held in write mode.
 	 */
-#ifdef CONFIG_HAS_VPK
-	vma->vm_vkey = newvkey;
-#endif
 	vma->vm_flags = newflags;
 	dirty_accountable = vma_wants_writenotify(vma, vma->vm_page_prot);
 	vma_set_page_prot(vma);
 
-	/* Change protection bits and flush related TLB. */
 	change_protection(vma, start, end, vma->vm_page_prot,
 			  dirty_accountable ? MM_CP_DIRTY_ACCT : 0);
 
-	/* link the vma with the table if vkey is not -1 or 0 */
-#ifdef CONFIG_HAS_VPK
-	list_del_init(&vma->vkey_chain);
-	if (newvkey) {
-		struct vkey_kgd_struct *kgd = current->mm->vkey.kgd;
-		int kgd_oft = vkey_kgd_offset(newvkey);
-		int kte_oft = vkey_kte_offset(newvkey);
-		if (kgd && kgd->ktes[kgd_oft])
-			list_add(&vma->vkey_chain, &(kgd->ktes[kgd_oft]->vkey_vma_heads[kte_oft]));
-		else
-			printk(KERN_ERR "[%s] vkey table is not initialized before vkey_mprotect...\n", __func__);
-	}
-#endif
-
 	/*
 	 * Private VM_LOCKED VMA becoming writable: trigger COW to avoid major
 	 * fault on access.
@@ -593,7 +520,7 @@ fail:
  * pkey==-1 when doing a legacy mprotect()
  */
 static int do_mprotect_pkey(unsigned long start, size_t len,
-		unsigned long prot, int pkey, int vkey, bool has_locked)
+		unsigned long prot, int pkey)
 {
 	unsigned long nstart, end, tmp, reqprot;
 	struct vm_area_struct *vma, *prev;
@@ -621,21 +548,16 @@ static int do_mprotect_pkey(unsigned long start, size_t len,
 
 	reqprot = prot;
 
-	if (!has_locked && mmap_write_lock_killable(current->mm))
+	if (mmap_write_lock_killable(current->mm))
 		return -EINTR;
 
 	/*
-	 * If userspace did not allocate the pkey (or vkey), do not let
+	 * If userspace did not allocate the pkey, do not let
 	 * them use it here.
-	 * When the config does not support, this fuunction ends here.
 	 */
 	error = -EINVAL;
-	if ((vkey != -1) && !mm_vkey_is_allocated(current->mm, vkey))
+	if ((pkey != -1) && !mm_pkey_is_allocated(current->mm, pkey))
 		goto out;
-	if ((pkey != -1) && !mm_pkey_is_allocated(current->mm, pkey)) {
-		if (vkey == -1 && pkey != execute_only_pkey(current->mm))
-			goto out;
-	}
 
 	vma = find_vma(current->mm, start);
 	error = -ENOMEM;
@@ -669,7 +591,6 @@ static int do_mprotect_pkey(unsigned long start, size_t len,
 		unsigned long mask_off_old_flags;
 		unsigned long newflags;
 		int new_vma_pkey;
-		int new_vma_vkey;
 
 		/* Here we know that vma->vm_start <= nstart < vma->vm_end. */
 
@@ -685,20 +606,9 @@ static int do_mprotect_pkey(unsigned long start, size_t len,
 		mask_off_old_flags = VM_READ | VM_WRITE | VM_EXEC |
 					VM_FLAGS_CLEAR;
 
-		new_vma_vkey = mm_mprotect_vkey(vma, vkey);
 		new_vma_pkey = arch_override_mprotect_pkey(vma, prot, pkey);
 		newflags = calc_vm_prot_bits(prot, new_vma_pkey);
 		newflags |= (vma->vm_flags & ~mask_off_old_flags);
-		/* Here, we do not merge calc_vm_vkey_bits with calc_vm_prot_bits
-		 * for 2 reasons.
-		 * First, calc_vm_prot_bits(prot, pkey) is invoked, with
-		 * pkey a non-zero value only in mprotect.c and mmap.c. This means other
-		 * call sites want the pkey to be zero, which is default. Also, the default
-		 * vkey is also zero no matter calc_vm_vkey_bits(vkey) is invoked or not.
-		 * Second, mmap.c cannot really deal with user-assigned pkeys, only zero
-		 * or the XO-pkey might be the value.
-		 * Hence, calc_vm_vkey_bits(vkey) has to be called only in this function.
-		 */
 
 		/* newflags >> 4 shift VM_MAY% in place of VM_% */
 		if ((newflags & ~(newflags >> 4)) & VM_ACCESS_FLAGS) {
@@ -707,7 +617,6 @@ static int do_mprotect_pkey(unsigned long start, size_t len,
 		}
 
 		/* Allow architectures to sanity-check the new flags */
-		/* For X86, this always returns true, other archs are not affected by the vkey */
 		if (!arch_validate_flags(newflags)) {
 			error = -EINVAL;
 			goto out;
@@ -722,12 +631,12 @@ static int do_mprotect_pkey(unsigned long start, size_t len,
 			tmp = end;
 
 		if (vma->vm_ops && vma->vm_ops->mprotect) {
-			error = vma->vm_ops->mprotect(vma, nstart, tmp, newflags);	/* This is always NULL in this version of kernel. */
+			error = vma->vm_ops->mprotect(vma, nstart, tmp, newflags);
 			if (error)
 				goto out;
 		}
 
-		error = mprotect_fixup(vma, &prev, nstart, tmp, newflags, calc_vm_vkey_bits(new_vma_vkey));
+		error = mprotect_fixup(vma, &prev, nstart, tmp, newflags);
 		if (error)
 			goto out;
 
@@ -735,9 +644,8 @@ static int do_mprotect_pkey(unsigned long start, size_t len,
 
 		if (nstart < prev->vm_end)
 			nstart = prev->vm_end;
-		if (nstart >= end) {
+		if (nstart >= end)
 			goto out;
-		}
 
 		vma = prev->vm_next;
 		if (!vma || vma->vm_start != nstart) {
@@ -747,241 +655,14 @@ static int do_mprotect_pkey(unsigned long start, size_t len,
 		prot = reqprot;
 	}
 out:
-	if (!has_locked)
-		mmap_write_unlock(current->mm);
+	mmap_write_unlock(current->mm);
 	return error;
 }
 
-#ifdef CONFIG_HAS_VPK
-extern void switch_mm_fast(struct mm_struct *mm, struct task_struct *tsk);
-extern void vkey_print_error_message(unsigned long address);
-
-static inline vm_fault_t activate_vkey(int vkey)
-{
-	struct vkey_map_struct *orig_vkm, *new_vkm, *real_orig_vkm;
-	bool stay_still, alloc_new, vkm_single, already_mapped;
-	int evicted_vkey;
-	int xok;
-	int vkru_perm;
-	int i, j, idx;
-	int conflict_vkey, conflict_j;
-	int perm[MAX_ACTIVE_VKEYS];
-
-	i = j = 0;
-	xok = get_execute_only_pkey(current->mm);
-	idx = ((vkey - 1) / MAX_ACTIVE_VKEYS) % current->vkm_nas;
-	already_mapped = false;	/* might be triggered by cross pgd page faults */
-	if (!vkey)
-		return 0;
-
-	/* alloc metadata according to index */
-	if (!current->mapped_vkeys) {
-		current->vkm_arr = tsk_vkm_arr_alloc();
-		current->mvk_arr = tsk_mvk_arr_alloc();
-		if (!current->vkm_arr || !current->mvk_arr) {
-			printk(KERN_ERR "[%s] no memory for per thread mapped metadata\n", __func__);
-			return VM_FAULT_SIGSEGV;
-		}
-	}
-	if (!current->mvk_arr[idx]) {
-		current->mvk_arr[idx] = tsk_mvk_alloc();
-		if (!current->mvk_arr[idx]) {
-			printk(KERN_ERR "[%s] no memory for per thread mapped vkeys\n", __func__);
-			return VM_FAULT_SIGSEGV;
-		}
-	}
-	real_orig_vkm = current->vkm;
-	if (current->vkm != current->vkm_arr[idx])
-		current->mapped_vkeys->pkru = mm_vkm_pkru_get();
-	current->vkm = current->vkm_arr[idx];	/* might be NULL ptr */
-	current->mapped_vkeys = current->mvk_arr[idx];
-
-	conflict_vkey = current->vkm ? current->vkm->pkey_vkey[(vkey - 1) % MAX_ACTIVE_VKEYS + 1] : 0;
-	for (i = 0; i < MAX_ACTIVE_VKEYS; i++) {
-		if (!current->mapped_vkeys->ts[i]) {
-			current->mapped_vkeys->ts[i] = MAX_ACTIVE_VKEYS - 1;
-			j = i;
-		} else
-			current->mapped_vkeys->ts[i]--;
-		if (current->mapped_vkeys->map[i] == conflict_vkey)
-			conflict_j = i;
-		if (current->mapped_vkeys->map[i] == vkey && vkey) {
-			j = conflict_j = i;
-			already_mapped = true;
-			break;
-		}
-	}
-
-	if (current->mapped_vkeys->map[j]) {	/* try the same pkey */
-#ifdef CONFIG_HAS_VPK_USER_VKRU
-		if (vkey_get_vkru_permission(current, conflict_vkey) == VKEY_AD)	/* !(accessible or pinned) */
-#else
-		if (vkey_get_vkrk_permission(current->vkrk, conflict_vkey) == VKEY_AD)
-#endif
-			j = conflict_j;
-	}	/* lru evict */
-	evicted_vkey = current->mapped_vkeys->map[j];
-	current->mapped_vkeys->map[j] = vkey;
-
-	/* Try stage */
-	new_vkm = orig_vkm = current->vkm;
-#ifdef CONFIG_HAS_VPK_USER_VKRU
-	if (current->vkru)
-		vkru_perm = vkey_get_vkru_permission(current, vkey);
-	else
-		vkru_perm = VKEY_AD;
-#else
-	if (current->vkrk)
-		vkru_perm = vkey_get_vkrk_permission(current->vkrk, vkey);
-	else
-		vkru_perm = VKEY_AD;
-#endif
-	/*
-	 * If the thread overflows, then the corresponding vkm has only 1 thread and cannot
-	 * be the migration dest of any other thread. So, no lock is needed if the system
-	 * works fine.
-	 * Otherwise, more than one threads may modify the vkm. To avoid TOCTOU inconsistency
-	 * caused by race condition, we use mmap lock for slower but safer implementation.
-	 */
-	alloc_new = false;
-	if (!already_mapped) {
-		if (orig_vkm)
-			spin_lock(&orig_vkm->slock);
-		vkm_single = orig_vkm ? (orig_vkm->nr_thread == 1) : false;
-		if (!vkm_single) {
-			stay_still = false;
-			if (orig_vkm)	/* try self first, stay_still when succeed */
-				stay_still = mm_vkm_can_add(orig_vkm, current->mapped_vkeys->map, MAX_ACTIVE_VKEYS);
-			if (!stay_still) {	/* try existing vkms, !alloc_new when succeed */
-				for (i = 0; i < MAX_ACTIVE_VKEYS; i++)
-					perm[i] = VKEY_AD;
-				if (orig_vkm) {
-					mm_vkm_del(orig_vkm, current->mm, current->mapped_vkeys->map, MAX_ACTIVE_VKEYS, perm, evicted_vkey);
-					preempt_disable_notrace();
-					arch_cpumask_clear_vkm(smp_processor_id(), vkm_cpumask(orig_vkm), orig_vkm);
-					preempt_enable_no_resched_notrace();
-				}
-				perm[j] = vkru_perm;
-				alloc_new = true;
-				read_lock(&current->mm->vkey_lock);
-				if (!evicted_vkey) {
-					struct list_head *pos;
-					list_for_each(pos, &current->mm->vkm_chain) {
-						struct vkey_map_struct *entry_vkm;
-						bool try_entry = true;
-						entry_vkm = list_entry(pos, struct vkey_map_struct, vkm_chain);
-						for (i = 0; i < current->vkm_nas; i++) /* no need to lock */
-							if (entry_vkm == current->vkm_arr[i])
-								try_entry = false;
-						if (try_entry) {
-							if (!spin_trylock(&entry_vkm->slock))
-								continue;
-							if (mm_vkm_can_add(entry_vkm, current->mapped_vkeys->map, MAX_ACTIVE_VKEYS)) {
-								new_vkm = entry_vkm;
-								alloc_new = false;
-								break;
-							} else
-								spin_unlock(&entry_vkm->slock);
-						}
-					}
-				}
-				if (alloc_new) {
-					read_unlock(&current->mm->vkey_lock);
-					/* may cause extra vkm alloc but no harm done */
-					if (orig_vkm)
-						spin_unlock(&orig_vkm->slock);
-					new_vkm = mm_vkm_alloc_init(current->mm);
-					if (!new_vkm) {
-						current->mapped_vkeys->map[j] = evicted_vkey;
-						current->mapped_vkeys->ts[j] = -1;
-						for (j = 0; j < MAX_ACTIVE_VKEYS; j++)
-							current->mapped_vkeys->ts[j]++;
-						if (orig_vkm)
-							spin_unlock(&orig_vkm->slock);
-						printk(KERN_ERR "[%s] no memory for vkey space\n", __func__);
-						return VM_FAULT_SIGSEGV;
-					}
-				} else
-					read_unlock(&current->mm->vkey_lock);
-			}
-		}
-
-		/* Decide stage */
-		if (new_vkm == orig_vkm)
-			current->mapped_vkeys->pmap[j] = mm_vkm_add_vkey(new_vkm, vkey, evicted_vkey, xok, vkru_perm);	/* the mapped pkey */
-		else {
-			preempt_disable_notrace();
-			cpumask_set_cpu(smp_processor_id(), vkm_cpumask(new_vkm));
-			preempt_enable_no_resched_notrace();
-			mm_vkm_add(new_vkm, current->mapped_vkeys, MAX_ACTIVE_VKEYS, xok, perm);
-		}
-
-		current->vkm = current->vkm_arr[idx] = new_vkm;
-#ifdef CONFIG_HAS_VPK_USER_VKRU
-		preempt_disable_notrace();
-		copy_vktramp_map(smp_processor_id(), new_vkm, current->mapped_vkeys->map, MAX_ACTIVE_VKEYS);
-		preempt_enable_no_resched_notrace();
-#endif
-
-		if (orig_vkm && !alloc_new)
-			spin_unlock(&orig_vkm->slock);
-
-		if (new_vkm != orig_vkm) {
-			if (alloc_new) {
-				write_lock(&current->mm->vkey_lock);
-				if (current->mm->pgd == new_vkm->pgd)
-					current->mm->main_vkm = new_vkm;
-				list_add(&new_vkm->vkm_chain, &current->mm->vkm_chain);
-				write_unlock(&current->mm->vkey_lock);
-			} else if (new_vkm)
-				spin_unlock(&new_vkm->slock);
-		}
-	} else	/* besides save, restore is also needed */
-#ifdef CONFIG_HAS_VPK_USER_VKRU
-	{
-		u32 target_pkru = current->mapped_vkeys->pkru;
-		int perm_xok = mm_vkm_pkru_get() & 0x0000000c;
-		target_pkru &= 0xfffffff3;
-		target_pkru |= perm_xok;
-		wrpkru((target_pkru & (~(0x3 <<
-			(current->mapped_vkeys->pmap[j] << 1)))) |
-			(vkru_perm << (current->mapped_vkeys->pmap[j] << 1)));
-	}
-#else
-		set_domain((current->mapped_vkeys->pkru &
-			(~domain_mask(current->mapped_vkeys->pmap[j]))) |
-			domain_val(current->mapped_vkeys->pmap[j], vkru_perm));
-#endif
-
-	if (new_vkm != real_orig_vkm) {
-		/* eager context switch as exec */
-		mb();	/* local mfence instruction */
-		local_irq_disable();
-		if (!IS_ENABLED(CONFIG_ARCH_WANT_IRQS_OFF_ACTIVATE_MM))
-			local_irq_enable();
-#if defined(CONFIG_X86) || defined(CONFIG_X86_64)
-		paravirt_activate_mm(current->active_mm, current->active_mm);
-#endif
-		switch_mm(current->active_mm, current->active_mm, current);
-		if (IS_ENABLED(CONFIG_ARCH_WANT_IRQS_OFF_ACTIVATE_MM))
-			local_irq_enable();
-	}
-
-	// vkey_print_error_message(0);
-
-	return 0;
-}
-
-vm_fault_t do_vkey_activate(int vkey)
-{
-	return activate_vkey(vkey);
-}
-#endif
-
 SYSCALL_DEFINE3(mprotect, unsigned long, start, size_t, len,
 		unsigned long, prot)
 {
-	return do_mprotect_pkey(start, len, prot, -1, -1, false);
+	return do_mprotect_pkey(start, len, prot, -1);
 }
 
 #ifdef CONFIG_ARCH_HAS_PKEYS
@@ -989,7 +670,7 @@ SYSCALL_DEFINE3(mprotect, unsigned long, start, size_t, len,
 SYSCALL_DEFINE4(pkey_mprotect, unsigned long, start, size_t, len,
 		unsigned long, prot, int, pkey)
 {
-	return do_mprotect_pkey(start, len, prot, pkey, -1, false);
+	return do_mprotect_pkey(start, len, prot, pkey);
 }
 
 SYSCALL_DEFINE2(pkey_alloc, unsigned long, flags, unsigned long, init_val)
@@ -1038,305 +719,3 @@ SYSCALL_DEFINE1(pkey_free, int, pkey)
 }
 
 #endif /* CONFIG_ARCH_HAS_PKEYS */
-
-
-#ifdef CONFIG_HAS_VPK
-
-#ifdef CONFIG_HAS_VPK_USER_VKRU
-extern struct vkey_per_cpu_cl *vktramp;
-SYSCALL_DEFINE2(vkey_reg_lib, unsigned long, laddr, unsigned long, taddr)
-{
-	int ret;
-	struct vm_area_struct *vma;
-
-	mmap_write_lock(current->mm);
-	ret = -EINVAL;
-
-	if (current->mm->lvkru_uaddr || current->mm->vktramp_uaddr) {
-		printk(KERN_ERR "The trusted memory in userspace has been set and locked...\n");
-		/* This means an attacker, should we terminates gracefully? */
-		goto fail;
-	}
-
-	vma = find_vma(current->mm, laddr);
-	if (!vma) {
-		printk(KERN_ERR "The trusted libvkeys is not found...\n");
-		goto fail;
-	}
-	if (lvkru_mmap_lock(vma, laddr & PAGE_MASK)) {
-		printk(KERN_ERR "The libvkeys vkru failed map...\n");
-		goto fail;
-	}
-	current->mm->lvkru_uaddr = laddr;
-
-	vma = find_vma(current->mm, taddr);
-	if (!vma) {
-		printk(KERN_ERR "The trusted trampoline is not found...\n");
-		goto fail;
-	}
-	// FIXME: [VDom] check vm_flags
-	if (vktramp_mmap_lock(vma)) {
-		printk(KERN_ERR "The trusted trampoline failed map...\n");
-		goto fail;
-	}
-	current->mm->vktramp_uaddr = taddr;
-
-	for (vma = current->mm->mmap; vma; vma = vma->vm_next)
-		if ((vma->vm_flags & PROT_EXEC) &&
-				vma->vm_file && vma->vm_file->f_path.dentry &&
-				(strcmp(vma->vm_file->f_path.dentry->d_iname, "libvkeys.so") == 0 ||
-				strcmp(vma->vm_file->f_path.dentry->d_iname, "libvkeyss.so") == 0))
-			break;
-	if (!vma) {
-		printk(KERN_ERR "The trusted libvkeys code section is not found...\n");
-		goto fail;
-	}
-	current->mm->lvkey_code_vma = vma;
-
-	ret = 0;
-	goto out;
-fail:
-	current->mm->lvkru_kaddr = 0;
-	current->mm->lvkru_uaddr = 0;
-	current->mm->vktramp_uaddr = 0;
-	current->mm->lvkey_code_vma = NULL;
-out:
-	mmap_write_unlock(current->mm);
-	return ret;
-}
-
-SYSCALL_DEFINE2(vkey_reg_vkru, unsigned long, addr, unsigned int, nas)
-{
-	unsigned long *vktramp_map_base;
-	int i;
-
-	if (nas > 0 && nas <= MAX_ADDR_SPACE_PER_THREAD && current->vkm_nas <= nas)
-		current->vkm_nas = nas;
-	else
-		return -EINVAL;
-
-	if (current->vkru && addr != current->vkru) {
-		if (addr != 0)
-			printk(KERN_ERR "The vkru of the thread set and locked...\n");
-		/* If the addr is poisoned, it cannot be realloced */
-		current->vkru = 0;
-		preempt_disable_notrace();
-		vktramp[smp_processor_id()].vkru = 0;
-		preempt_enable_no_resched_notrace();
-		return -EINVAL;
-	}
-	current->vkru = addr;
-	preempt_disable_notrace();
-	vktramp[smp_processor_id()].vkru = addr;
-	vktramp_map_base = &vktramp[smp_processor_id()].map;
-	/* vkm lock should be outside this function */
-	for (i = 0; i < VPMAP_LONGS; i++)
-		*(vktramp_map_base + i) = 0;
-	preempt_enable_no_resched_notrace();
-	return 0;
-}
-
-SYSCALL_DEFINE2(vkey_wrvkrk, int, vkey, int, perm)
-{
-	return -EINVAL;
-}
-
-SYSCALL_DEFINE1(vkey_activate, int, vkey)
-{
-	int ret, i;
-	struct mapped_vkey_struct *current_mvk, *target_mvk;
-	int idx = ((vkey - 1) / MAX_ACTIVE_VKEYS) % current->vkm_nas;
-	int pkey_oft, perm, pkru;
-
-	current_mvk = current->mapped_vkeys;
-	if (current_mvk) {
-		target_mvk = current->mvk_arr[idx];
-		if (target_mvk && current_mvk != target_mvk) {
-			for (i = 0; i < MAX_ACTIVE_VKEYS; i++)
-				if (target_mvk->map[i] == vkey)
-					break;
-			if (i != MAX_ACTIVE_VKEYS) {
-				current->vkm = current->vkm_arr[idx];
-				current->mapped_vkeys = target_mvk;
-				current_mvk->pkru = mm_vkm_pkru_get();
-				perm = vkey_get_vkru_permission(current, vkey);
-				pkey_oft = target_mvk->pmap[i] << 1;
-				pkru = target_mvk->pkru & 0xfffffff3;
-				pkru &= (~(0x3 << pkey_oft));
-				pkru |= (perm << pkey_oft);
-				switch_mm_fast(current->mm, current);
-				wrpkru(pkru);
-				return 0;
-			}
-		}
-	}
-	mmap_read_lock(current->mm);
-	ret = do_vkey_activate(vkey);
-	mmap_read_unlock(current->mm);
-	return ret;
-}
-
-#else	/* Kernel VKRK */
-
-SYSCALL_DEFINE2(vkey_reg_lib, unsigned long, laddr, unsigned long, taddr)
-{
-	return -EINVAL;
-}
-
-SYSCALL_DEFINE2(vkey_reg_vkru, unsigned long, addr, unsigned int, nas)
-{
-	if (unlikely(!current->vkrk)) {
-		current->vkrk = tsk_vkrk_alloc();
-		if (!current->vkrk)
-			return -EINVAL;
-	}
-	if (nas > 0 && nas <= MAX_ADDR_SPACE_PER_THREAD && current->vkm_nas <= nas) {
-		current->vkm_nas = nas;
-		return 0;
-	}
-	return -EINVAL;
-}
-
-SYSCALL_DEFINE2(vkey_wrvkrk, int, vkey, int, perm)
-{
-	if (unlikely(!current->vkrk)) {
-		printk(KERN_ERR "[%s] Please allocate with VKRK before assigning", __func__);
-		return -EINVAL;
-	}
-
-	/* First, find mvk, then vkm mapping. */
-	if (vkey != ARCH_DEFAULT_VKEY) {
-		int i;
-		int idx = ((vkey - 1) / MAX_ACTIVE_VKEYS) % current->vkm_nas;
-		vkey_set_vkrk_permission(vkey, perm & 0x3);
-		if (current->mapped_vkeys && current->mvk_arr[idx]) {
-			vpmap_t *target_maps = current->mvk_arr[idx]->map;
-			vpmap_t *target_pmaps = current->mvk_arr[idx]->pmap;
-			struct vkey_map_struct *target_vkm = current->vkm_arr[idx];
-			for (i = 0; i < MAX_ACTIVE_VKEYS; i++)	/* Mapped vkeys, no data race */
-				if (target_maps[i] == vkey)
-					break;
-			if (i != MAX_ACTIVE_VKEYS) {
-				/* Vkm is never NULL, and no need to lock because other threads in vkm
-				* can never write the vkey pkey map used by current thread */
-				if (target_vkm != current->vkm) {
-					if (!(perm & VKEY_MASK)) {
-						current->vkm = target_vkm;
-						current->mapped_vkeys->pkru = mm_vkm_pkru_get();
-						current->mapped_vkeys = current->mvk_arr[idx];
-						switch_mm_fast(current->mm, current);
-						set_domain((current->mapped_vkeys->pkru &
-							(~domain_mask(target_pmaps[i]))) |
-							domain_val(target_pmaps[i], perm & 0x3));
-					}
-				} else
-					mm_vkm_pkru_set_bits(target_pmaps[i], perm & 0x3);
-				return 0;
-			}
-		}
-
-		if (!(perm & VKEY_MASK)) {
-			int ret;
-			mmap_read_lock(current->mm);
-			ret = do_vkey_activate(vkey);
-			mmap_read_unlock(current->mm);
-			return ret;
-		}
-	}
-
-	return -EINVAL;
-}
-
-SYSCALL_DEFINE1(vkey_activate, int, vkey)
-{
-	return -EINVAL;
-}
-
-#endif /* HAS_VPK_USER_VKRU */
-
-SYSCALL_DEFINE0(vkey_alloc)
-{
-	int vkey;
-	int ret;
-
-	mmap_write_lock(current->mm);
-	vkey = mm_vkey_alloc(current->mm);
-
-	ret = -ENOSPC;
-	if (vkey == -1)
-		goto out;
-
-	ret = vkey;
-	if (ret == 1)
-		do_vkey_activate(ret);
-out:
-	mmap_write_unlock(current->mm);
-	return ret;
-}
-
-SYSCALL_DEFINE1(vkey_free, int, vkey)
-{
-	int ret;
-
-	/* Pkey use after free should be avoided by the user */
-	mmap_write_lock(current->mm);
-	ret = mm_vkey_free(current->mm, vkey);
-	mmap_write_unlock(current->mm);
-
-	return ret;
-}
-
-SYSCALL_DEFINE4(vkey_mprotect, unsigned long, start, size_t, len,
-		unsigned long, prot, int, vkey)
-{
-	/* Map the area with vkey, disable both read and write (with the only execute-only pkey)
-	 * Let the following page fault to trigger the vkey->pkey mapping
-	 */
-	int ret;
-	int pkey;
-	mmap_write_lock(current->mm);
-#ifdef CONFIG_HAS_VPK_USER_VKRU
-	if (unlikely(!current->mm->lvkru_uaddr)) {
-		printk(KERN_ERR "Cannot find the trusted libvkeys in userspace...\n");
-		mmap_write_unlock(current->mm);
-		return -EINVAL;
-	}
-#endif
-	if (unlikely(execute_only_pkey(current->mm) != get_execute_only_pkey(current->mm))) {
-		printk(KERN_ERR "Cannot find the xok %d...\n", execute_only_pkey(current->mm));
-		mmap_write_unlock(current->mm);
-		return -EINVAL;
-	}
-
-	/* If the vkey is 0 (default), just set the pkey to 0 as well. */
-	pkey = execute_only_pkey(current->mm);
-	if (current->vkm) {
-		int i, j;
-		for (i = arch_max_pkey() - 2; i >= 0; i--)
-			if (current->vkm->pkey_vkey[i] == vkey) {
-				struct mapped_vkey_struct* mvk = current->mapped_vkeys;
-				if (mvk) {
-					for (j = 0; j < MAX_ACTIVE_VKEYS; j++)
-						if (vkey == mvk->map[j]) {
-							pkey = mm_vkm_idx_to_pkey(i);
-							break;
-						}
-				}
-				break;
-			}
-	}
-	if (unlikely(!vkey))
-		pkey = 0;
-
-	/* Just shift left the vkey to use the high bits of the vm_flags and,
-	 * make the real pkey to the execute-only one
-	 * to trigger later pkey-caused page fault.
-	 * If the XO-pkey is 3, then is the final vkey sent to do_mprotect_pkey().
-	 * [ vkey, 0011 ]
-	 */
-	ret = do_mprotect_pkey(start, len, prot, pkey, vkey, true);
-	mmap_write_unlock(current->mm);
-	return ret;
-}
-
-#endif /* CONFIG_HAS_VPK */
diff --git a/./mm/mremap.c b/../../../../linux/mm/mremap.c
index 9a2f18b79..002eec83e 100644
--- a/./mm/mremap.c
+++ b/../../../../linux/mm/mremap.c
@@ -23,7 +23,6 @@
 #include <linux/mmu_notifier.h>
 #include <linux/uaccess.h>
 #include <linux/userfaultfd_k.h>
-#include <linux/vkeys.h>
 
 #include <asm/cacheflush.h>
 #include <asm/tlb.h>
@@ -271,9 +270,6 @@ static bool move_normal_pmd(struct vm_area_struct *vma, unsigned long old_addr,
 
 	VM_BUG_ON(!pmd_none(*new_pmd));
 
-#ifdef CONFIG_HAS_VPK
-	if (!vkm_pmd_populate(mm, new_pmd, pmd_pgtable(pmd), -1))
-#endif
 	pmd_populate(mm, new_pmd, pmd_pgtable(pmd));
 	flush_tlb_range(vma, old_addr, old_addr + PMD_SIZE);
 	if (new_ptl != old_ptl)
diff --git a/./mm/nommu.c b/../../../../linux/mm/nommu.c
index 3bceb8495..55a9e48a7 100644
--- a/./mm/nommu.c
+++ b/../../../../linux/mm/nommu.c
@@ -1354,7 +1354,7 @@ int split_vma(struct mm_struct *mm, struct vm_area_struct *vma,
 	if (!region)
 		return -ENOMEM;
 
-	new = vm_area_dup(vma, false);
+	new = vm_area_dup(vma);
 	if (!new) {
 		kmem_cache_free(vm_region_jar, region);
 		return -ENOMEM;
diff --git a/./mm/oom_kill.c b/../../../../linux/mm/oom_kill.c
index a2da0ea91..832fb3303 100644
--- a/./mm/oom_kill.c
+++ b/../../../../linux/mm/oom_kill.c
@@ -516,10 +516,6 @@ bool __oom_reap_task_mm(struct mm_struct *mm)
 {
 	struct vm_area_struct *vma;
 	bool ret = true;
-#ifdef CONFIG_HAS_VPK
-	struct list_head *pos;
-	struct vkey_map_struct *vkm;
-#endif
 
 	/*
 	 * Tell all users of get_user/copy_from_user etc... that the content
@@ -556,18 +552,6 @@ bool __oom_reap_task_mm(struct mm_struct *mm)
 				ret = false;
 				continue;
 			}
-#ifdef CONFIG_HAS_VPK
-			if (!list_empty(&mm->vkm_chain)) {
-				list_for_each(pos, &mm->vkm_chain) {
-					vkm = list_entry(pos, struct vkey_map_struct, vkm_chain);
-					if (vkm->pgd == mm->pgd)
-						continue;
-					tlb.vkm = vkm;
-					unmap_page_range(&tlb, vma, range.start, range.end, NULL);
-				}
-			}
-			tlb.vkm = NULL;
-#endif
 			unmap_page_range(&tlb, vma, range.start, range.end, NULL);
 			mmu_notifier_invalidate_range_end(&range);
 			tlb_finish_mmu(&tlb);
diff --git a/./mm/pagewalk.c b/../../../../linux/mm/pagewalk.c
index eabc7d877..9b3db11a4 100644
--- a/./mm/pagewalk.c
+++ b/../../../../linux/mm/pagewalk.c
@@ -375,30 +375,8 @@ static int __walk_page_range(unsigned long start, unsigned long end,
 	if (vma && is_vm_hugetlb_page(vma)) {
 		if (ops->hugetlb_entry)
 			err = walk_hugetlb_range(start, end, walk);
-#ifdef CONFIG_HAS_VPK
-			if (walk->mm && !list_empty(&walk->mm->vkm_chain))
-				printk(KERN_ERR "[%s] synchronization of hugetlb in addrress spaces is not supported yet.", __func__);
-#endif
-	} else {
-#ifdef CONFIG_HAS_VPK
-		if (!walk->pgd && walk->mm && !list_empty(&walk->mm->vkm_chain)) {
-			struct list_head *pos;
-			struct mm_struct *mm;
-			mm = walk->mm;
-			list_for_each(pos, &mm->vkm_chain) {
-				struct vkey_map_struct *vkm = list_entry(pos, struct vkey_map_struct, vkm_chain);
-				if (vkm->pgd == mm->pgd)
-					continue;
-				walk->pgd = vkm->pgd;
-				err = walk_pgd_range(start, end, walk);
-				if (err)
-					return err;
-			}
-			walk->pgd = NULL;
-		}
-#endif
+	} else
 		err = walk_pgd_range(start, end, walk);
-	}
 
 	if (vma && ops->post_vma)
 		ops->post_vma(walk);
diff --git a/./mm/swap_state.c b/../../../../linux/mm/swap_state.c
index 31479050a..8d4104242 100644
--- a/./mm/swap_state.c
+++ b/../../../../linux/mm/swap_state.c
@@ -22,7 +22,6 @@
 #include <linux/swap_slots.h>
 #include <linux/huge_mm.h>
 #include <linux/shmem_fs.h>
-#include <linux/vkey_map.h>
 #include "internal.h"
 
 /*
diff --git a/../../../../linux/tools/perf/include/perf/perf_dlfilter.h b/../../../../linux/tools/perf/include/perf/perf_dlfilter.h
new file mode 100644
index 000000000..3eef03d66
--- /dev/null
+++ b/../../../../linux/tools/perf/include/perf/perf_dlfilter.h
@@ -0,0 +1,150 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * perf_dlfilter.h: API for perf --dlfilter shared object
+ * Copyright (c) 2021, Intel Corporation.
+ */
+#ifndef _LINUX_PERF_DLFILTER_H
+#define _LINUX_PERF_DLFILTER_H
+
+#include <linux/perf_event.h>
+#include <linux/types.h>
+
+/* Definitions for perf_dlfilter_sample flags */
+enum {
+	PERF_DLFILTER_FLAG_BRANCH	= 1ULL << 0,
+	PERF_DLFILTER_FLAG_CALL		= 1ULL << 1,
+	PERF_DLFILTER_FLAG_RETURN	= 1ULL << 2,
+	PERF_DLFILTER_FLAG_CONDITIONAL	= 1ULL << 3,
+	PERF_DLFILTER_FLAG_SYSCALLRET	= 1ULL << 4,
+	PERF_DLFILTER_FLAG_ASYNC	= 1ULL << 5,
+	PERF_DLFILTER_FLAG_INTERRUPT	= 1ULL << 6,
+	PERF_DLFILTER_FLAG_TX_ABORT	= 1ULL << 7,
+	PERF_DLFILTER_FLAG_TRACE_BEGIN	= 1ULL << 8,
+	PERF_DLFILTER_FLAG_TRACE_END	= 1ULL << 9,
+	PERF_DLFILTER_FLAG_IN_TX	= 1ULL << 10,
+	PERF_DLFILTER_FLAG_VMENTRY	= 1ULL << 11,
+	PERF_DLFILTER_FLAG_VMEXIT	= 1ULL << 12,
+};
+
+/*
+ * perf sample event information (as per perf script and <linux/perf_event.h>)
+ */
+struct perf_dlfilter_sample {
+	__u32 size; /* Size of this structure (for compatibility checking) */
+	__u16 ins_lat;		/* Refer PERF_SAMPLE_WEIGHT_TYPE in <linux/perf_event.h> */
+	__u16 p_stage_cyc;	/* Refer PERF_SAMPLE_WEIGHT_TYPE in <linux/perf_event.h> */
+	__u64 ip;
+	__s32 pid;
+	__s32 tid;
+	__u64 time;
+	__u64 addr;
+	__u64 id;
+	__u64 stream_id;
+	__u64 period;
+	__u64 weight;		/* Refer PERF_SAMPLE_WEIGHT_TYPE in <linux/perf_event.h> */
+	__u64 transaction;	/* Refer PERF_SAMPLE_TRANSACTION in <linux/perf_event.h> */
+	__u64 insn_cnt;	/* For instructions-per-cycle (IPC) */
+	__u64 cyc_cnt;		/* For instructions-per-cycle (IPC) */
+	__s32 cpu;
+	__u32 flags;		/* Refer PERF_DLFILTER_FLAG_* above */
+	__u64 data_src;		/* Refer PERF_SAMPLE_DATA_SRC in <linux/perf_event.h> */
+	__u64 phys_addr;	/* Refer PERF_SAMPLE_PHYS_ADDR in <linux/perf_event.h> */
+	__u64 data_page_size;	/* Refer PERF_SAMPLE_DATA_PAGE_SIZE in <linux/perf_event.h> */
+	__u64 code_page_size;	/* Refer PERF_SAMPLE_CODE_PAGE_SIZE in <linux/perf_event.h> */
+	__u64 cgroup;		/* Refer PERF_SAMPLE_CGROUP in <linux/perf_event.h> */
+	__u8  cpumode;		/* Refer CPUMODE_MASK etc in <linux/perf_event.h> */
+	__u8  addr_correlates_sym; /* True => resolve_addr() can be called */
+	__u16 misc;		/* Refer perf_event_header in <linux/perf_event.h> */
+	__u32 raw_size;		/* Refer PERF_SAMPLE_RAW in <linux/perf_event.h> */
+	const void *raw_data;	/* Refer PERF_SAMPLE_RAW in <linux/perf_event.h> */
+	__u64 brstack_nr;	/* Number of brstack entries */
+	const struct perf_branch_entry *brstack; /* Refer <linux/perf_event.h> */
+	__u64 raw_callchain_nr;	/* Number of raw_callchain entries */
+	const __u64 *raw_callchain; /* Refer <linux/perf_event.h> */
+	const char *event;
+};
+
+/*
+ * Address location (as per perf script)
+ */
+struct perf_dlfilter_al {
+	__u32 size; /* Size of this structure (for compatibility checking) */
+	__u32 symoff;
+	const char *sym;
+	__u64 addr; /* Mapped address (from dso) */
+	__u64 sym_start;
+	__u64 sym_end;
+	const char *dso;
+	__u8  sym_binding; /* STB_LOCAL, STB_GLOBAL or STB_WEAK, refer <elf.h> */
+	__u8  is_64_bit; /* Only valid if dso is not NULL */
+	__u8  is_kernel_ip; /* True if in kernel space */
+	__u32 buildid_size;
+	__u8 *buildid;
+	/* Below members are only populated by resolve_ip() */
+	__u8 filtered; /* True if this sample event will be filtered out */
+	const char *comm;
+};
+
+struct perf_dlfilter_fns {
+	/* Return information about ip */
+	const struct perf_dlfilter_al *(*resolve_ip)(void *ctx);
+	/* Return information about addr (if addr_correlates_sym) */
+	const struct perf_dlfilter_al *(*resolve_addr)(void *ctx);
+	/* Return arguments from --dlarg option */
+	char **(*args)(void *ctx, int *dlargc);
+	/*
+	 * Return information about address (al->size must be set before
+	 * calling). Returns 0 on success, -1 otherwise.
+	 */
+	__s32 (*resolve_address)(void *ctx, __u64 address, struct perf_dlfilter_al *al);
+	/* Return instruction bytes and length */
+	const __u8 *(*insn)(void *ctx, __u32 *length);
+	/* Return source file name and line number */
+	const char *(*srcline)(void *ctx, __u32 *line_number);
+	/* Return perf_event_attr, refer <linux/perf_event.h> */
+	struct perf_event_attr *(*attr)(void *ctx);
+	/* Read object code, return numbers of bytes read */
+	__s32 (*object_code)(void *ctx, __u64 ip, void *buf, __u32 len);
+	/* Reserved */
+	void *(*reserved[120])(void *);
+};
+
+/*
+ * If implemented, 'start' will be called at the beginning,
+ * before any calls to 'filter_event'. Return 0 to indicate success,
+ * or return a negative error code. '*data' can be assigned for use
+ * by other functions. 'ctx' is needed for calls to perf_dlfilter_fns,
+ * but most perf_dlfilter_fns are not valid when called from 'start'.
+ */
+int start(void **data, void *ctx);
+
+/*
+ * If implemented, 'stop' will be called at the end,
+ * after any calls to 'filter_event'. Return 0 to indicate success, or
+ * return a negative error code. 'data' is set by start(). 'ctx' is
+ * needed for calls to perf_dlfilter_fns, but most perf_dlfilter_fns
+ * are not valid when called from 'stop'.
+ */
+int stop(void *data, void *ctx);
+
+/*
+ * If implemented, 'filter_event' will be called for each sample
+ * event. Return 0 to keep the sample event, 1 to filter it out, or
+ * return a negative error code. 'data' is set by start(). 'ctx' is
+ * needed for calls to perf_dlfilter_fns.
+ */
+int filter_event(void *data, const struct perf_dlfilter_sample *sample, void *ctx);
+
+/*
+ * The same as 'filter_event' except it is called before internal
+ * filtering.
+ */
+int filter_event_early(void *data, const struct perf_dlfilter_sample *sample, void *ctx);
+
+/*
+ * If implemented, return a one-line description of the filter, and optionally
+ * a longer description.
+ */
+const char *filter_description(const char **long_description);
+
+#endif
diff --git a/./patch.diff b/../../../../linux/tools/testing/selftests/tc-testing/plugins/__init__.py
similarity index 100%
rename from ./patch.diff
rename to ../../../../linux/tools/testing/selftests/tc-testing/plugins/__init__.py
index 5e4ee643e..e69de29bb 100644
